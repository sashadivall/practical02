{
    "0": {
        "file": "Extended Notes - Cornell B+ Trees.pdf",
        "page": 0,
        "chunk": "B+ Trees The idea we saw earlier of putting multiple set (list, hash table) elements together into large chunks that exploit locality can also be applied to trees. Binary search trees are not good for locality because a given node of the binary tree probably occupies only a fraction of any cache line. B+ trees are a way to get better locality by putting multiple elements into each tree node. B+ trees were originally invented for storing data structures on disk, where locality is even more crucial than with memory. Accessing a disk location takes about 5ms = 5,000,000ns. Therefore, if you are storing a tree on disk, you want to make sure that a given disk read is as effective as possible. B+ trees have a high branching factor, much larger than 2, which ensures that few disk reads are needed to navigate to the place where data is stored. B+ trees may also useful for in-memory data structures because these days main memory is almost as slow relative to the processor as disk drives were to main memory when B+ trees were first introduced! A B+ tree of order m is a search tree in which each nonleaf node has up to m children. The actual elements of the collection are stored in the leaves of the tree, and the nonleaf nodes contain only keys. Each leaf stores some number of elements; the maximum number may be greater or (typically) less than m. The data structure satisfies several invariants: 1.\u200b Every path from the root to a leaf has the same length 2.\u200b If a node has n children, it contains n\u22121 keys. 3.\u200b Every node (except the root) is at least half full 4.\u200b The elements stored in a given subtree all have keys that are between"
    },
    "1": {
        "file": "Extended Notes - Cornell B+ Trees.pdf",
        "page": 0,
        "chunk": "invariants: 1.\u200b Every path from the root to a leaf has the same length 2.\u200b If a node has n children, it contains n\u22121 keys. 3.\u200b Every node (except the root) is at least half full 4.\u200b The elements stored in a given subtree all have keys that are between the keys in the parent node on either side of the subtree pointer. (This generalizes the BST invariant.) 5.\u200b The root has at least two children if it is not a leaf. For example, the following is an order-5 B+ tree (m=5) where the leaves have enough space to store up to 3 data records:"
    },
    "2": {
        "file": "Extended Notes - Cornell B+ Trees.pdf",
        "page": 1,
        "chunk": "Because the height of the tree is uniformly the same and every node is at least half full, we are guaranteed that the asymptotic performance is O(lg n) where n is the size of the collection. The real win is in the constant factors, of course. We can choose m so that the pointers to the m children plus the m\u22121 elements fill out a cache line at the highest level of the memory hierarchy where we can expect to get cache hits. For example, if we are accessing a large disk database then our \"cache lines\" are memory blocks of the size that is read from disk. Lookup in a B+ tree is straightforward. Given a node to start from, we use a simple linear or binary search to find whether the desired element is in the node, or if not, which child pointer to follow from the current node. Insertion and deletion from a B+ tree are more complicated; in fact, they are notoriously difficult to implement correctly. For insertion, we first find the appropriate leaf node into which the inserted element falls (assuming it is not already in the tree). If there is already room in the node, the new element can be inserted simply. Otherwise the current leaf is already full and must be split into two leaves, one of which acquires the new element. The parent is then updated to contain a new key and child pointer. If the parent is already full, the process ripples upwards, eventually possibly reaching the root. If the root is split into two, then a new root is created with just two children, increasing the height of the tree by one. For example, here is the effect of a series of insertions. The first insertion (13) merely affects a"
    },
    "3": {
        "file": "Extended Notes - Cornell B+ Trees.pdf",
        "page": 1,
        "chunk": "ripples upwards, eventually possibly reaching the root. If the root is split into two, then a new root is created with just two children, increasing the height of the tree by one. For example, here is the effect of a series of insertions. The first insertion (13) merely affects a leaf. The second insertion (14) overflows the leaf and adds a key to an internal node. The third insertion propagates all the way to the root."
    },
    "4": {
        "file": "Extended Notes - Cornell B+ Trees.pdf",
        "page": 2,
        "chunk": "Deletion works in the opposite way: the element is removed from the leaf. If the leaf becomes empty, a key is removed from the parent node. If that breaks invariant 3, the keys of the parent node and its immediate right (or left) sibling are reapportioned among them so that invariant 3 is satisfied. If this is not possible, the parent node can be combined with that sibling, removing a key another level up in the tree and possible causing a ripple all the way to the root. If the root has just two children, and they are combined, then the root is deleted and the new combined node becomes the root of the tree, reducing the height of the tree by one. Further reading: Aho, Hopcroft, and Ullman, Data Structures and Algorithms, Chapter 11."
    },
    "5": {
        "file": "Extended Notes - Fontenot B+ Tree Walkthrough.pdf",
        "page": 0,
        "chunk": "Example B+ Tree: m = 4 Step 1 - Insert: 42, 21, 63, 89 -\u200b Initially, the first node is a leaf node AND root node. -\u200b 21, 42, \u2026 represent keys of some set of K:V pairs -\u200b Leaf nodes store keys and data, although data not shown -\u200b Inserting another key will cause the node to split. Step 2 - Insert: 35 -\u200b Leaf node needs to split to accommodate 35. New leaf node allocated to the right of existing node -\u200b 5/2 values stay in original node; remaining values moved to new node -\u200b Smallest value from new leaf node (42) is copied up to the parent, which needs to be created in this case. It will be an internal node. Step 3 - Insert: 10, 27, 96 -\u200b The insert process starts at the root node. The keys of the root node are searched to find out which child node we need to descend to. -\u200b EX: 10. Since 10 < 42, we follow the pointer to the left of 42 -\u200b Note - none of these new values cause a node to split"
    },
    "6": {
        "file": "Extended Notes - Fontenot B+ Tree Walkthrough.pdf",
        "page": 1,
        "chunk": "Step 4 - Insert 30 -\u200b Starting at root, we descend to the left-most child (we\u2019ll call curr). -\u200b curr is a leaf node. Thus, we insert 30 into curr. -\u200b BUT curr is full. So we have to split. -\u200b Create a new node to the right of curr, temporarily called newNode. -\u200b Insert newNode into the doubly linked list of leaf nodes. -\u200b re-distribute the keys -\u200b copy the smallest key (27 in this case) from newNode to parent; rearrange keys and pointers in parent node. -\u200b Parent of newNode is also root. So, nothing else to do Observation: The root node is full. -\u200b The next insertion that splits a leaf will cause the root to split, and thus the tree will get 1 level deeper."
    },
    "7": {
        "file": "Extended Notes - Fontenot B+ Tree Walkthrough.pdf",
        "page": 2,
        "chunk": "Step 4 - Insert 37 -\u200b When splitting an internal node, we move the middle element to the parent (instead of copying it). -\u200b In this particular tree, that means we have to create a new internal node which is also now the root."
    },
    "8": {
        "file": "Extended Notes - Moving Beyond the Relational Model Slide Deck.pdf",
        "page": 0,
        "chunk": "Benefits of Relational Model: -\u200b (Mostly) Standard Data Model and Query Language -\u200b The relational model is widely adopted across industries, providing a structured way to store and manage data in tables with well-defined relationships. -\u200b SQL (Structured Query Language) is the standard query language for relational databases, making it easier for developers, analysts, and data scientists to work with data across different database systems. -\u200b Most relational databases follow common conventions, allowing for easier migration between different database management systems (e.g., MySQL, PostgreSQL, SQL Server). -\u200b ACID Compliance (Atomicity, Consistency, Isolation, Durability) - Ensures reliable transactions by enforcing four key properties: -\u200b Atomicity: A transaction is either fully completed or fully rolled back\u2014no partial updates (aka transaction is treated as an atomic unit - it is fully executed or no parts of it are executed) -\u200b Consistency: The database remains in a valid state before and after a transaction (aka a transaction takes a database from one consistent state to another consistent state consistent state - all data meets integrity constraints) -\u200b Isolation: Transactions execute independently without interfering with each other -\u200b Two transactions T1 and T2 are being executed at the same time but cannot affect each other -\u200b If both T1 and T2 are reading the data - no problem -\u200b If T1 is reading the same data that T2 may be writing, can result in -\u200b Dirty Read: a transaction T1 is able to read a row that has been modified by another transaction T2 that hasn\u2019t yet executed a COMMIT -\u200b Non-repeatable Read: two queries in a single transaction T1 execute a SELECT but get different values because another transaction T2 has changed data and COMMITTED -\u200b Phantom Reads: when a transaction T1 is running and another transaction T2 adds or deletes rows from the"
    },
    "9": {
        "file": "Extended Notes - Moving Beyond the Relational Model Slide Deck.pdf",
        "page": 0,
        "chunk": "hasn\u2019t yet executed a COMMIT -\u200b Non-repeatable Read: two queries in a single transaction T1 execute a SELECT but get different values because another transaction T2 has changed data and COMMITTED -\u200b Phantom Reads: when a transaction T1 is running and another transaction T2 adds or deletes rows from the set T1 is using -\u200b Durability: Once a transaction is committed, its changes are permanently stored, even in the case of a system failure. -\u200b Once a transaction is completed and committed successfully, its changes are permanent. -\u200b Even in the event of a system failure, committed transactions are preserved -\u200b Works Well with Highly Structured Data: -\u200b Best suited for scenarios where data can be organized into predefined schemas with fixed relationships, such as financial records, inventory management, and customer databases."
    },
    "10": {
        "file": "Extended Notes - Moving Beyond the Relational Model Slide Deck.pdf",
        "page": 1,
        "chunk": "-\u200b Ensures data integrity through constraints like primary keys, foreign keys, and unique constraints. -\u200b Enables efficient joins and aggregations, which are crucial for analytical and transactional workloads. -\u200b Can Handle Large Amounts of Data -\u200b Optimized indexing, partitioning, and query optimization techniques allow relational databases to scale efficiently. -\u200b Many enterprise-grade relational databases support horizontal and vertical scaling, handling millions or even billions of rows. -\u200b Used in large-scale applications like banking systems, airline reservation systems, and enterprise resource planning (ERP) software. -\u200b Well Understood, Lots of Tooling, Lots of Experience -\u200b Relational databases have been around for decades, leading to a wealth of knowledge, best practices, and mature ecosystem support. -\u200b A large number of tools exist for database administration, performance tuning, data migration, and backup management. -\u200b Many database professionals, from developers to DBAs, have deep experience in relational databases, making hiring and knowledge transfer easier for organizations. Relational Database Performance -\u200b Many techniques help relational database management systems (RDBMS) optimize efficiency, ensuring fast queries, reduced latency, and scalable performance. 1.\u200b Indexing -\u200b Indexes improve query performance by allowing the database to locate data quickly rather than scanning entire tables. -\u200b Common types of indexes: -\u200b B-Tree Indexes: Used for range and equality searches. -\u200b Hash Indexes: Ideal for exact-match lookups. -\u200b Bitmap Indexes: Efficient for low-cardinality columns (e.g., gender, Boolean values). -\u200b Proper indexing speeds up read operations but can slightly slow down writes due to index maintenance. 2.\u200b Directly Controlling Storage -\u200b RDBMSs manage how data is physically stored on disk, optimizing storage structures for performance. -\u200b Techniques include: -\u200b Data compression to reduce disk I/O. -\u200b Page-level optimizations for fast retrieval. -\u200b Efficient allocation of memory buffers and disk space."
    },
    "11": {
        "file": "Extended Notes - Moving Beyond the Relational Model Slide Deck.pdf",
        "page": 1,
        "chunk": "data is physically stored on disk, optimizing storage structures for performance. -\u200b Techniques include: -\u200b Data compression to reduce disk I/O. -\u200b Page-level optimizations for fast retrieval. -\u200b Efficient allocation of memory buffers and disk space."
    },
    "12": {
        "file": "Extended Notes - Moving Beyond the Relational Model Slide Deck.pdf",
        "page": 2,
        "chunk": "3.\u200b Column-Oriented Storage vs. Row-Oriented Storage -\u200b Row-Oriented Storage: Stores entire rows together; efficient for transactional (OLTP) workloads where frequent inserts, updates, and deletes occur. -\u200b Column-Oriented Storage: Stores data by columns instead of rows, improving performance for analytical (OLAP) queries that aggregate large amounts of data. Reduces disk I/O by only retrieving necessary columns. 4.\u200b Query Optimization -\u200b RDBMSs analyze SQL queries to find the most efficient execution plan. -\u200b Techniques include: Using cost-based optimization to determine the lowest-cost query plan; Reordering joins to minimize processing time; Pushdown filtering to reduce the amount of processed data. 5.\u200b Caching/Prefetching -\u200b Caching: Frequently accessed query results are stored in memory to reduce redundant computations. -\u200b Prefetching: Anticipates future queries and loads relevant data into memory ahead of time. -\u200b Reduces disk I/O bottlenecks and speeds up query execution. 6.\u200b Materialized Views -\u200b A materialized view is a precomputed result of a query stored as a table. -\u200b Unlike regular views, which recompute results each time they are queried, materialized views store results persistently. -\u200b Commonly used in reporting and analytics to speed up expensive aggregations and joins. 7.\u200b Precompiled Stored Procedures -\u200b Stored procedures are precompiled SQL queries that execute faster because they don\u2019t need to be re-parsed every time. -\u200b Benefits: Reduces query execution time, Improves security by limiting direct access to raw tables, Allows complex logic to be executed at the database level. 8.\u200b Data Replication and Partitioning -\u200b Replication: Copies data across multiple database instances to improve availability and fault tolerance -\u200b Common types: Master-slave replication (one primary, multiple read replicas); Multi-master replication (multiple writable nodes). -\u200b Partitioning: Splits large tables into smaller, more manageable chunks to improve query performance. -\u200b Horizontal partitioning: Divides rows across different tables (e.g., by region, time range). -\u200b Vertical partitioning: Stores specific"
    },
    "13": {
        "file": "Extended Notes - Moving Beyond the Relational Model Slide Deck.pdf",
        "page": 2,
        "chunk": "and fault tolerance -\u200b Common types: Master-slave replication (one primary, multiple read replicas); Multi-master replication (multiple writable nodes). -\u200b Partitioning: Splits large tables into smaller, more manageable chunks to improve query performance. -\u200b Horizontal partitioning: Divides rows across different tables (e.g., by region, time range). -\u200b Vertical partitioning: Stores specific columns in separate tables to optimize read performance. Transaction Processing -\u200b Transaction: a sequence of one or more of the CRUD operations performed as a single, logical unit of work -\u200b Either the entire sequence succeeds (COMMIT)"
    },
    "14": {
        "file": "Extended Notes - Moving Beyond the Relational Model Slide Deck.pdf",
        "page": 3,
        "chunk": "-\u200b OR the entire sequence fails (ROLLBACK or ABORT) -\u200b Help ensure: Data Integrity, Error Recovery, Concurrency Control, Reliable Data Storage, Simplified Error Handling Transaction Processing What is a Transaction? -\u200b A transaction is a sequence of one or more CRUD operations (Create, Read, Update, Delete) performed as a single, logical unit of work. Transactions ensure data consistency and reliability in database systems. -\u200b All operations must either complete successfully together (COMMIT) or fail together (ROLLBACK or ABORT). -\u200b If any part of the transaction encounters an error, the entire sequence is reversed to maintain data integrity. How Transactions Work 1.\u200b Begin Transaction \u2013 The system starts a new transaction. 2.\u200b Perform CRUD Operations \u2013 Multiple database operations occur (e.g., inserting a record, updating a balance). 3.\u200b Validation & Checks \u2013 The system ensures constraints and consistency rules are met. 4.\u200b Commit or Rollback -\u200b COMMIT: If all operations succeed, changes are saved permanently. -\u200b ROLLBACK (ABORT): If any operation fails, all changes are undone, ensuring the database remains in a consistent state. Why Are Transactions Important? Transactions play a crucial role in maintaining a reliable and robust database system by ensuring: 1.\u200b Data Integrity: Transactions maintain database correctness by ensuring that only valid and complete changes are saved. a.\u200b Prevents issues such as half-completed updates or lost data during failures. 2.\u200b Error Recovery: If a system crashes or a query fails, the database can roll back to a stable state. a.\u200b Ensures that no partial or inconsistent data is left behind. 3.\u200b Concurrency Control: Multiple users can safely perform operations on the database at the same time. a.\u200b Prevents issues like lost updates, dirty reads, and inconsistent data retrieval. 4.\u200b Reliable Data Storage: Transactions ensure that once data is committed, it remains stored even in the event of a"
    },
    "15": {
        "file": "Extended Notes - Moving Beyond the Relational Model Slide Deck.pdf",
        "page": 3,
        "chunk": "left behind. 3.\u200b Concurrency Control: Multiple users can safely perform operations on the database at the same time. a.\u200b Prevents issues like lost updates, dirty reads, and inconsistent data retrieval. 4.\u200b Reliable Data Storage: Transactions ensure that once data is committed, it remains stored even in the event of a power failure or system crash. a.\u200b Guarantees Durability, one of the key ACID properties. 5.\u200b Simplified Error Handling: Developers can focus on application logic without worrying about data corruption. a.\u200b If an error occurs, rolling back ensures a clean database state."
    },
    "16": {
        "file": "Extended Notes - Moving Beyond the Relational Model Slide Deck.pdf",
        "page": 4,
        "chunk": "Example Transaction - Transfer $$ DELIMITER // CREATE PROCEDURE transfer( IN sender_id INT, IN receiver_id INT, IN amount DECIMAL(10,2) ) BEGIN DECLARE rollback_message VARCHAR(255) \u200b DEFAULT 'Transaction rolled back: Insufficient funds'; DECLARE commit_message VARCHAR(255) DEFAULT 'Transaction committed successfully'; -- Start the transaction START TRANSACTION; -- Attempt to debit money from account 1 UPDATE accounts SET balance = balance - amount WHERE account_id = sender_id; -- Attempt to credit money to account 2 UPDATE accounts SET balance = balance + amount WHERE account_id = receiver_id; -- Check if there are sufficient funds in account 1 -- Simulate a condition where there are insufficient funds IF (SELECT balance FROM accounts WHERE account_id = sender_id) < 0 THEN -- Roll back the transaction if there are insufficient funds ROLLBACK; SIGNAL SQLSTATE '45000' \u200b -- 45000 is unhandled, user-defined error SET MESSAGE_TEXT = rollback_message; ELSE -- Log the transactions if there are sufficient funds INSERT INTO transactions (account_id, amount, transaction_type) \u200b \u200b VALUES (sender_id, -amount, 'WITHDRAWAL'); INSERT INTO transactions (account_id, amount, transaction_type) VALUES (receiver_id, amount, 'DEPOSIT'); -- Commit the transaction COMMIT; SELECT commit_message AS 'Result'; END IF; END // DELIMITER ;"
    },
    "17": {
        "file": "Extended Notes - Moving Beyond the Relational Model Slide Deck.pdf",
        "page": 5,
        "chunk": "Relational Database Problems While relational databases are powerful and widely used, they may not be the best choice for every use case. Some challenges and limitations include: 1.\u200b Schemas Evolve Over Time -\u200b Rigid Schema Structure: -\u200b Relational databases require predefined schemas, meaning tables and relationships must be designed before data is stored. -\u200b As applications grow and change, modifying the schema (e.g., adding new columns, changing relationships) can be cumbersome and may require schema migrations, which can be slow and risky in large databases. -\u200b Alternative Solutions: -\u200b NoSQL databases (e.g., MongoDB, Cassandra) provide schema flexibility, allowing fields to be added dynamically without disrupting existing data. 2.\u200b Not All Applications Need Full ACID Compliance -\u200b ACID compliance ensures strong consistency, but at a cost: -\u200b Transactions in relational databases follow Atomicity, Consistency, Isolation, and Durability, which can introduce performance overhead, especially in distributed systems. -\u200b Some applications prioritize availability and speed over strong consistency. -\u200b Alternative Solutions: -\u200b Eventual consistency models in NoSQL databases (e.g., Amazon DynamoDB, Apache Cassandra) are often preferred for applications where absolute consistency is not critical (e.g., social media feeds, recommendation engines). 3.\u200b Joins Can Be Expensive -\u200b Performance Bottlenecks: -\u200b SQL queries involving multiple joins can become computationally expensive, especially on large datasets. -\u200b Each join operation requires scanning and matching rows from multiple tables, which can lead to high CPU and memory usage. -\u200b Alternative Solutions: -\u200b Denormalization (storing related data together) is sometimes preferred in NoSQL databases to avoid expensive join operations. -\u200b Columnar databases (e.g., Apache Parquet, ClickHouse) optimize analytical workloads by reducing the need for joins. 4.\u200b A Lot of Data Is Semi-Structured or Unstructured (JSON, XML, etc.) -\u200b Relational databases are designed for structured data: -\u200b They work best when data fits neatly into rows and columns. -\u200b However,"
    },
    "18": {
        "file": "Extended Notes - Moving Beyond the Relational Model Slide Deck.pdf",
        "page": 5,
        "chunk": "-\u200b Columnar databases (e.g., Apache Parquet, ClickHouse) optimize analytical workloads by reducing the need for joins. 4.\u200b A Lot of Data Is Semi-Structured or Unstructured (JSON, XML, etc.) -\u200b Relational databases are designed for structured data: -\u200b They work best when data fits neatly into rows and columns. -\u200b However, modern applications generate semi-structured (JSON, XML) or unstructured (text, images, videos) data that doesn\u2019t always fit a tabular format. -\u200b Alternative Solutions: -\u200b NoSQL document stores (e.g., MongoDB, Firebase) allow for flexible JSON-like storage. -\u200b Search engines like Elasticsearch and OpenSearch are optimized for handling unstructured text and logs. 5.\u200b Horizontal Scaling Presents Challenges -\u200b Scaling relational databases across multiple servers is complex: -\u200b Relational databases traditionally scale vertically (by adding more CPU, RAM, or storage to a single server)."
    },
    "19": {
        "file": "Extended Notes - Moving Beyond the Relational Model Slide Deck.pdf",
        "page": 6,
        "chunk": "-\u200b Scaling horizontally (distributing data across multiple servers) requires techniques like sharding, replication, and distributed transactions, which add complexity. -\u200b Alternative Solutions: -\u200b NoSQL databases like Cassandra and DynamoDB are natively designed for horizontal scaling, handling massive amounts of data across distributed nodes with ease. 6.\u200b Some Applications Require More Performance (Real-Time, Low Latency Systems) -\u200b Relational databases can be too slow for real-time applications: -\u200b High-throughput, low-latency applications (e.g., stock trading platforms, real-time analytics, gaming leaderboards) need sub-millisecond response times. -\u200b RDBMS architectures, with their strict consistency and transaction management, can introduce delays. -\u200b Alternative Solutions: -\u200b In-memory databases (e.g., Redis, Memcached) provide ultra-fast key-value lookups. -\u200b Time-series databases (e.g., InfluxDB, TimescaleDB) are optimized for time-based data analysis. Scalability \u2013 Up or Out? Conventional Wisdom: Scale Vertically (Up) Until High Availability Demands Horizontal Scaling (Out) -\u200b Vertical Scaling (Scaling Up): -\u200b Definition: Scaling up refers to adding more resources (CPU, RAM, storage) to a single server to handle increased workload. -\u200b Why it\u2019s common: -\u200b It\u2019s simple and relatively cost-effective in the short term. -\u200b No architecture changes are needed; you just upgrade the existing server hardware. -\u200b Often sufficient for many small to medium-sized applications. -\u200b Limitations: -\u200b Physical limits: At some point, hardware upgrades become prohibitively expensive or ineffective. You can\u2019t keep adding more CPUs, RAM, or storage indefinitely. -\u200b Single point of failure: With a vertically scaled system, if the server goes down, the entire application can become unavailable. This poses availability and redundancy challenges, especially in mission-critical systems. -\u200b Cost efficiency: The larger the system, the more expensive it is to upgrade. There's a diminishing return on investment with each incremental upgrade. -\u200b Horizontal Scaling (Scaling Out): -\u200b Definition: Scaling out means adding more servers or nodes to distribute the load, effectively splitting the workload"
    },
    "20": {
        "file": "Extended Notes - Moving Beyond the Relational Model Slide Deck.pdf",
        "page": 6,
        "chunk": "in mission-critical systems. -\u200b Cost efficiency: The larger the system, the more expensive it is to upgrade. There's a diminishing return on investment with each incremental upgrade. -\u200b Horizontal Scaling (Scaling Out): -\u200b Definition: Scaling out means adding more servers or nodes to distribute the load, effectively splitting the workload across multiple machines. -\u200b Why it\u2019s challenging: -\u200b Requires distributed computing models (e.g., sharding, replication), which can introduce complexity in managing consistency, coordination, and fault tolerance. -\u200b Managing a large number of machines introduces complexity in system design (network communication, load balancing, etc.), and data consistency must be carefully handled."
    },
    "21": {
        "file": "Extended Notes - Moving Beyond the Relational Model Slide Deck.pdf",
        "page": 7,
        "chunk": "-\u200b Why it\u2019s necessary: -\u200b For high-traffic applications or applications requiring high availability and fault tolerance, scaling vertically alone eventually becomes insufficient. -\u200b High-demand apps (e.g., social media platforms, e-commerce, cloud services) often need to serve millions or billions of users, requiring distributed systems that can horizontally scale. -\u200b Why Scaling Up (Vertical Scaling) is Often Easier -\u200b Simple Upgrades: -\u200b It\u2019s often easier to just increase the resources of an existing machine than to architect a distributed system. -\u200b No need to redesign the application: Vertical scaling doesn\u2019t require significant changes in the system architecture or application code. -\u200b Less Complex Infrastructure: -\u200b No need to worry about network communication, data sharding, or distributed transactions. -\u200b Fewer Management Overheads: -\u200b Fewer machines mean fewer maintenance tasks, backups, and monitoring systems. -\u200b Practical and Financial Limits to Scaling Up -\u200b Physical Hardware Constraints: -\u200b Eventually, you\u2019ll hit hardware limits. For example, there\u2019s a limit to the number of CPUs, RAM, and storage you can add to a single server. -\u200b As machines grow more powerful, the cost of upgrading them grows exponentially. High-end servers can be very expensive compared to commodity servers. -\u200b Single Point of Failure: -\u200b If your vertically scaled system crashes, everything crashes. There's no inherent redundancy unless you add complex failover mechanisms. -\u200b High availability and fault tolerance are limited unless you implement replication or clustering, which pushes the system toward horizontal scaling. Modern Systems That Make Horizontal Scaling Less Problematic -\u200b Distributed Databases: -\u200b Modern distributed databases (e.g., Cassandra, Google Spanner) are designed to scale horizontally with built-in mechanisms for data partitioning (sharding), replication, and eventual consistency. -\u200b They handle much of the complexity of scaling, making horizontal scaling much more accessible for modern systems. -\u200b Cloud Computing and Serverless Architectures: -\u200b Cloud platforms (e.g.,"
    },
    "22": {
        "file": "Extended Notes - Moving Beyond the Relational Model Slide Deck.pdf",
        "page": 7,
        "chunk": "distributed databases (e.g., Cassandra, Google Spanner) are designed to scale horizontally with built-in mechanisms for data partitioning (sharding), replication, and eventual consistency. -\u200b They handle much of the complexity of scaling, making horizontal scaling much more accessible for modern systems. -\u200b Cloud Computing and Serverless Architectures: -\u200b Cloud platforms (e.g., AWS, Google Cloud, Azure) allow dynamic horizontal scaling, where resources (such as virtual machines or containers) can be added automatically based on demand. -\u200b Serverless frameworks (e.g., AWS Lambda, Azure Functions) enable scaling on-demand without worrying about managing infrastructure. -\u200b These platforms provide auto-scaling capabilities that automatically spin up new resources as traffic increases and scale them down when demand drops."
    },
    "23": {
        "file": "Extended Notes - Moving Beyond the Relational Model Slide Deck.pdf",
        "page": 8,
        "chunk": "-\u200b Microservices: -\u200b Microservices architectures decompose applications into smaller, more manageable components, which can be scaled independently. -\u200b This allows individual services to be scaled out independently, making it easier to handle varying levels of load across different parts of an application. -\u200b Containerization (Docker, Kubernetes): -\u200b Containers allow for easier deployment and scaling of applications across distributed environments. -\u200b Kubernetes, for example, simplifies the process of managing containers and automatically scaling services as needed. So What? Distributed Data when Scaling Out What is a Distributed System? -\u200b A distributed system is a network of independent computers that work together to provide a unified service, appearing to users as a single system. As Andrew Tannenbaum puts it: -\u200b \u201cA distributed system is a collection of independent computers that appear to its users as one computer.\u201d -\u200b When scaling out in a distributed system, multiple machines (or nodes) handle different pieces of the workload, allowing the system to scale horizontally. This enables systems to handle larger amounts of data and more requests concurrently, but it also introduces a unique set of challenges. Characteristics of Distributed Systems Distributed systems have several key characteristics that differentiate them from traditional, monolithic systems: 1.\u200b Computers Operate Concurrently: -\u200b Concurrency means that multiple processes or threads can be executed in parallel across different machines. -\u200b Each machine in a distributed system may be working on different tasks at the same time, contributing to the overall processing power of the system. -\u200b This concurrent processing allows distributed systems to handle a higher volume of requests and tasks efficiently. For example, one node may be processing user requests, another may be updating a database, while a third node handles background jobs. -\u200b Challenges: -\u200b Synchronization between machines can be tricky, especially when they must share or access common"
    },
    "24": {
        "file": "Extended Notes - Moving Beyond the Relational Model Slide Deck.pdf",
        "page": 8,
        "chunk": "systems to handle a higher volume of requests and tasks efficiently. For example, one node may be processing user requests, another may be updating a database, while a third node handles background jobs. -\u200b Challenges: -\u200b Synchronization between machines can be tricky, especially when they must share or access common data. -\u200b This requires effective communication mechanisms and often a distributed consensus to ensure that all parts of the system are working in harmony. 2.\u200b Computers Fail Independently -\u200b Independent Failures: In a distributed system, each machine can fail independently of the others. This means that even if one node goes down, the others can continue to function. -\u200b This fault tolerance is one of the primary benefits of distributed systems, as failures do not bring down the entire system. -\u200b Challenges: -\u200b Fault detection and recovery become complex when systems are distributed. How do we know a machine has failed, and how do we ensure it recovers or that its tasks are redistributed to healthy machines?"
    },
    "25": {
        "file": "Extended Notes - Moving Beyond the Relational Model Slide Deck.pdf",
        "page": 9,
        "chunk": "-\u200b Data consistency and availability can be impacted by node failures, which must be handled carefully through techniques like replication and partitioning. -\u200b Example: If a server in a database cluster fails, replicas of the data stored on other machines can still serve requests, but the system must know which copies are most up-to-date and reliable. 3.\u200b No Shared Global Clock -\u200b No Global Clock means there is no single time source or synchronized clock across all machines in a distributed system. Each machine operates based on its local time, leading to discrepancies across the system. -\u200b Without a shared clock, it\u2019s impossible to guarantee that all events in the system happen in a perfectly synchronized manner. -\u200b This has important implications for tasks like ordering operations (e.g., in transaction processing or event logs) and event coordination across nodes. -\u200b Challenges: -\u200b Clock skew: Differences in time across nodes can cause issues with event ordering. For example, determining the exact sequence of operations in a distributed database might be difficult. -\u200b Distributed systems often use techniques like logical clocks (e.g., Lamport timestamps) to maintain an order of events across machines without relying on synchronized physical clocks. Challenges in Scaling Out with Distributed Data While distributed systems offer scalability and fault tolerance, they come with inherent challenges: 1.\u200b Data Partitioning (Sharding) -\u200b Sharding is the process of dividing a large dataset into smaller, more manageable parts (shards), which can be distributed across multiple machines. -\u200b Challenges: -\u200b Deciding on the best strategy for partitioning data. For example, which data should go on which machine, and how do we ensure balanced distribution of the load? -\u200b Handling cross-shard queries can be complex and inefficient. If a query needs data from multiple shards, it must be coordinated and consolidated efficiently. 2.\u200b Data Replication"
    },
    "26": {
        "file": "Extended Notes - Moving Beyond the Relational Model Slide Deck.pdf",
        "page": 9,
        "chunk": "strategy for partitioning data. For example, which data should go on which machine, and how do we ensure balanced distribution of the load? -\u200b Handling cross-shard queries can be complex and inefficient. If a query needs data from multiple shards, it must be coordinated and consolidated efficiently. 2.\u200b Data Replication -\u200b Replication involves creating multiple copies of data on different machines to ensure high availability and fault tolerance. -\u200b Challenges: -\u200b Managing consistency across replicas: How do we make sure all replicas are up-to-date after a change is made? -\u200b Handling replication lag: When updates are made to one replica, it might take time for the changes to propagate to others, potentially leading to inconsistent reads. -\u200b Distributed databases often employ eventual consistency or use techniques like Quorum-based replication to strike a balance between consistency and availability. 3.\u200b Communication and Latency -\u200b In a distributed system, nodes must communicate over a network, which can introduce latency and potential bottlenecks. -\u200b Challenges:"
    },
    "27": {
        "file": "Extended Notes - Moving Beyond the Relational Model Slide Deck.pdf",
        "page": 10,
        "chunk": "-\u200b Ensuring low-latency communication between nodes while scaling out to a large number of machines. -\u200b Handling network partitions, where communication between certain nodes is temporarily broken. This requires sophisticated strategies for maintaining availability and consistency during these partitions (e.g., CAP theorem considerations). 4.\u200b Consistency and Consensus -\u200b Achieving consistency in a distributed system can be difficult, especially when multiple nodes are involved. -\u200b Challenges: -\u200b Distributed consensus protocols (e.g., Paxos, Raft) are often needed to agree on the state of the system across nodes. These protocols can be complex and costly in terms of performance but are crucial for maintaining consistency. -\u200b CAP theorem: It\u2019s impossible to guarantee consistency, availability, and partition tolerance all at once in a distributed system, leading to trade-offs. Distributed Data Stores What Are Distributed Data Stores? -\u200b A distributed data store is a system where data is stored across multiple machines or nodes rather than on a single server. The goal is to achieve high availability, scalability, and fault tolerance by spreading data across different locations. -\u200b Replication: A key feature of distributed data stores is data replication, where each block of data is typically replicated across multiple nodes to ensure redundancy and fault tolerance. For example, a database might store the same data on N nodes, ensuring that even if one or more nodes fail, the data can still be accessed from the other nodes. -\u200b Sharding: Data is partitioned into smaller, more manageable pieces (called shards) that are distributed across nodes. Each shard contains a subset of the overall data, helping to improve performance and scalability. Types of Distributed Databases Distributed databases can either be relational or non-relational (NoSQL), with both types supporting distribution, replication, and sharding in different ways. 1.\u200b Relational Distributed Databases: Some traditional relational databases have evolved to support"
    },
    "28": {
        "file": "Extended Notes - Moving Beyond the Relational Model Slide Deck.pdf",
        "page": 10,
        "chunk": "shard contains a subset of the overall data, helping to improve performance and scalability. Types of Distributed Databases Distributed databases can either be relational or non-relational (NoSQL), with both types supporting distribution, replication, and sharding in different ways. 1.\u200b Relational Distributed Databases: Some traditional relational databases have evolved to support distributed configurations: -\u200b MySQL and PostgreSQL are examples of relational databases that offer replication (the process of maintaining copies of the same data on different nodes) and sharding (splitting data into subsets that reside on different machines). -\u200b These systems typically require additional tools or setups (like MySQL Cluster, PgSQL's logical replication, or PostgreSQL partitioning) to scale horizontally and achieve distribution across multiple nodes. -\u200b Replication ensures data availability and durability in case of failures. -\u200b Sharding helps distribute the load across multiple nodes to improve scalability, though managing distributed transactions and maintaining consistency becomes more complex in these cases. 2.\u200b NoSQL Distributed Databases: Many NoSQL databases were designed with distributed systems in mind, providing more inherent support for sharding and replication: -\u200b Cassandra, MongoDB, and Couchbase are some examples of distributed NoSQL systems that automatically handle sharding and replication."
    },
    "29": {
        "file": "Extended Notes - Moving Beyond the Relational Model Slide Deck.pdf",
        "page": 11,
        "chunk": "-\u200b NoSQL databases often favor eventual consistency over strict ACID compliance, meaning that while data is eventually consistent across all nodes, it may not be immediately consistent after updates. This allows for more flexibility in distributed configurations but requires careful attention to potential inconsistencies. -\u200b Cassandra uses a peer-to-peer model and supports replication and partitioning, with the ability to scale horizontally across many nodes. -\u200b MongoDB offers replication and sharding out of the box, making it suitable for horizontally scalable applications. 3.\u200b Newer Players -\u200b CockroachDB: -\u200b A newer distributed relational database, CockroachDB was designed to provide horizontal scalability and high availability with strong consistency (supporting the ACID properties of transactions). It automatically handles sharding and replication, and its architecture is similar to that of Google Spanner, focusing on global distribution. -\u200b It\u2019s a distributed SQL database that offers strong consistency, allowing users to scale applications globally while maintaining the guarantees typically associated with relational databases. Key Characteristics of Distributed Data Stores Replication: Replication ensures that data is available even when some nodes or machines fail. Typically, each data block (or row) is replicated across multiple nodes (often N nodes, with N being a configurable number). -\u200b Synchronous replication ensures that data is written to all nodes before acknowledging the write. -\u200b Asynchronous replication writes data to one node first and later synchronizes it to other replicas. This is often faster but can lead to eventual consistency. -\u200b Challenges: Managing consistency between replicas can be complex. There is always a trade-off between performance and consistency, particularly when nodes are located in different regions (latency issues can arise). Sharding: Sharding involves splitting data into smaller chunks called shards, each of which resides on a separate node. This partitioning is usually done based on a shard key (e.g., user ID or region)"
    },
    "30": {
        "file": "Extended Notes - Moving Beyond the Relational Model Slide Deck.pdf",
        "page": 11,
        "chunk": "trade-off between performance and consistency, particularly when nodes are located in different regions (latency issues can arise). Sharding: Sharding involves splitting data into smaller chunks called shards, each of which resides on a separate node. This partitioning is usually done based on a shard key (e.g., user ID or region) to ensure that data is distributed evenly across nodes. -\u200b Advantages: Sharding allows for horizontal scalability, meaning you can add more nodes to the system to distribute the load as demand increases. -\u200b Challenges: -\u200b Managing cross-shard queries can be complex and potentially slower. If data is spread across different shards, queries that need data from multiple shards may incur additional overhead. -\u200b Rebalancing shards across nodes can be difficult, especially when data grows unevenly, requiring manual intervention or complex algorithms to ensure even distribution. Network Partitioning is Inevitable One of the most important principles in distributed systems is that network partitioning is inevitable, meaning that at some point, the network may experience temporary failures, leading to split-brain situations where some nodes can no longer communicate with others."
    },
    "31": {
        "file": "Extended Notes - Moving Beyond the Relational Model Slide Deck.pdf",
        "page": 12,
        "chunk": "-\u200b What is Network Partitioning? -\u200b Network partitioning occurs when there\u2019s a disruption in the communication between parts of the system, resulting in some nodes being cut off from others. -\u200b For example, if a failure occurs in the network between two data centers, the systems on each side of the partition may be isolated and unable to communicate with each other. -\u200b System Needs to Be Partition Tolerant -\u200b Partition Tolerance refers to the ability of the system to continue functioning even when parts of the network are unavailable. This is crucial in distributed systems because network failures are inevitable and cannot always be prevented. -\u200b According to the CAP Theorem (states that it is impossible for a distributed data store to simultaneously provide more than two out of the following three guarantees), a distributed system can only guarantee two of the following three properties: -\u200b Consistency: Every read operation sees the most recent write. -\u200b Every user of the DB has an identical view of the data at any given instant -\u200b Consistency + Availability: System always responds with the latest data and every request gets a response, but may not be able to deal with network issues -\u200b Availability: Every request to the system gets a response (either success or failure). -\u200b In the event of a failure, the database remains operational -\u200b Availability + Partition Tolerance: System always sends are responds based on distributed store, but may not be the absolute latest data. -\u200b Partition Tolerance: The system continues to operate even if there\u2019s a network partition. -\u200b The database can maintain operations in the event of the network\u2019s failing between two segments of the distributed system -\u200b Consistency + Partition Tolerance: If system responds with data from a distributed store, it is always the latest,"
    },
    "32": {
        "file": "Extended Notes - Moving Beyond the Relational Model Slide Deck.pdf",
        "page": 12,
        "chunk": "The system continues to operate even if there\u2019s a network partition. -\u200b The database can maintain operations in the event of the network\u2019s failing between two segments of the distributed system -\u200b Consistency + Partition Tolerance: If system responds with data from a distributed store, it is always the latest, else data request is dropped. -\u200b What CAP is really saying: If you cannot limit the number of faults, requests can be directed to any server, and you insist on serving every request, then you cannot possibly be consistent (But it is interpreted as: You must always give up something: consistency, availability, or tolerance to failure) -\u200b In practice, most distributed systems opt for Partition Tolerance and Availability, as network failures are common. This often leads to eventual consistency (e.g., Cassandra, MongoDB)."
    },
    "33": {
        "file": "AVL Trees.pdf",
        "page": 0,
        "chunk": "AVL Tree \u25cf\u200b Problems with BST: The order in which elements are inserted affects the height of the tree. If elements are inserted in either ascending or descending order, then the tree is completely imbalanced. The more imbalanced a tree is, the more comparisons we have to make. Therefore, our goal is to create a BST with minimum height. \u25cb\u200b This can be achieved by a complete BST \u25a0\u200b A complete BST is a BST where every level is full except possibly the last one. \u25cb\u200b This ensures at most log(n) comparisons. \u25cb\u200b However, it is cumbersome to build such a tree from a list of numbers \u25cb\u200b Here come AVL Trees! \u25cf\u200b An AVL tree is an approximately balanced BST tree \u25cb\u200b Maintains a balance factor in each node \u25cb\u200b The balance property of a given node states that the maximum difference between the height of the left subtree and the height of the right subtree is 1. \u25cb\u200b Each time a node is inserted, the balance property is checked. If it is not satisfied, the tree must be rotated. \u25cb\u200b There are four cases that warrant rotation \u25a0\u200b Case 1: Left-Left Insertion \u25cf\u200b If Z is the node of imbalance, left-left rotation is caused when the inserted node falls in the left subtree of the left child of Z \u25cf\u200b Identify the unbalanced node (let\u2019s call it Z). \u25cf\u200b Let Y be the left child of Z, and X be the left child of Y. \u25cf\u200b Perform a right rotation: \u25cb\u200b Make Y the new root. \u25cb\u200b Move Z to be the right child of Y. \u25cb\u200b Assign Y's right child as Z's left child. \u25a0\u200b Case 2: Left-Right Insertion \u25cf\u200b Occurs when you insert the new node into the right subtree of the left child of Z"
    },
    "34": {
        "file": "AVL Trees.pdf",
        "page": 0,
        "chunk": "right rotation: \u25cb\u200b Make Y the new root. \u25cb\u200b Move Z to be the right child of Y. \u25cb\u200b Assign Y's right child as Z's left child. \u25a0\u200b Case 2: Left-Right Insertion \u25cf\u200b Occurs when you insert the new node into the right subtree of the left child of Z \u25cf\u200b Identify the unbalanced node (Z). \u25cf\u200b Look at Z's left child (Y), which has a right child (X). \u25cf\u200b Step 1: Move X up and Y down \u25cb\u200b X takes Y's position. \u25cb\u200b Y becomes X's left child. \u25cb\u200b If X had a left subtree, it becomes Y's right subtree. \u25cf\u200b Step 2: Move X up and Z down"
    },
    "35": {
        "file": "AVL Trees.pdf",
        "page": 1,
        "chunk": "\u25cb\u200b X takes Z's position. \u25cb\u200b Z becomes X's right child. \u25cb\u200b If X had a right subtree, it becomes Z's left subtree. \u25a0\u200b Case 3: Right-Left Insertion \u25cf\u200b Occurs when you insert the new node into the left subtree of the right child of Z \u25cf\u200b Identify the unbalanced node (Z). \u25cf\u200b Look at Z's right child (Y), which has a left child (X). \u25cf\u200b Step 1: Move X up and Y down \u25cb\u200b X takes Y's position. \u25cb\u200b Y becomes X's right child. \u25cb\u200b If X had a right subtree, it becomes Y's left subtree. \u25cf\u200b Step 2: Move X up and Z down \u25cb\u200b X takes Z's position. \u25cb\u200b Z becomes X's left child. \u25cb\u200b If X had a left subtree, it becomes Z's right subtree. \u25a0\u200b Case 4: Right-Right Insertion \u25cf\u200b Occurs when the new node is inserted into the right subtree of the right child of Z \u25cf\u200b Identify the unbalanced node (Z). \u25cf\u200b Let Y be the right child of Z, and X be the right child of Y. \u25cf\u200b Perform a left rotation: \u25cb\u200b Make Y the new root. \u25cb\u200b Move Z to be the left child of Y. \u25cb\u200b Assign Y's left child as Z's right child."
    },
    "36": {
        "file": "DS4300 Notes.pdf",
        "page": 0,
        "chunk": "Binary Search Tree (BST) \u2022 A BST is a binary tree where: o The left subtree contains nodes with values less than the parent. o The right subtree contains nodes with values greater than the parent. \u2022 The order in which values are inserted affects the shape of the tree. AVL Tree (Adelson-Velsky and Landis Tree) \u2022 A self-balancing BST that maintains the AVL balance property: o The balance factor of a node is calculated as: \u2022 Balance Factor = height(left subtree) - height(right subtree) o A node is balanced if the absolute value of its balance factor is \u2264 1. \u2022 AVL trees ensure a balanced height, keeping operations efficient. AVL Tree Rotations (Rebalancing) An insertion can cause an imbalance in one of the following four ways. In each case, alpha (\u03b1) is the first unbalanced node. Left-Left (LL) Case \u2022 Occurs when a node is inserted into the left subtree of the left child of \u03b1. \u2022 Structure before imbalance: \u03b1 / y / z \u2022 Fix: Single Right Rotation around \u03b1. o Make y the new root. o Move \u03b1 to the right of y. Left-Right (LR) Case \u2022 Occurs when a node is inserted into the right subtree of the left child of \u03b1. \u2022 Structure before imbalance: \u03b1 / y \\ z \u2022 Fix: Double Rotation (Left Rotation + Right Rotation) o First, Left Rotate around y, converting it into an LL case. o Then, Right Rotate around \u03b1, making z the new root of the affected subtree. Right-Left (RL) Case \u2022 Occurs when a node is inserted into the left subtree of the right child of \u03b1. \u2022 Structure before imbalance: \u03b1 \\ y / z"
    },
    "37": {
        "file": "DS4300 Notes.pdf",
        "page": 0,
        "chunk": "the affected subtree. Right-Left (RL) Case \u2022 Occurs when a node is inserted into the left subtree of the right child of \u03b1. \u2022 Structure before imbalance: \u03b1 \\ y / z"
    },
    "38": {
        "file": "DS4300 Notes.pdf",
        "page": 1,
        "chunk": "\u2022 Fix: Double Rotation (Right Rotation + Left Rotation) o First, Right Rotate around y, converting it into an RR case. o Then, Left Rotate around \u03b1, making z the new root of the affected subtree. Right-Right (RR) Case \u2022 Occurs when a node is inserted into the right subtree of the right child of \u03b1. \u2022 Structure before imbalance: \u03b1 \\ y \\ z \u2022 Fix: Single Left Rotation around \u03b1. o Make y the new root. o Move \u03b1 to the left of y. Key Notes on AVL Rotations \u2022 LL and RR cases require a single rotation. \u2022 LR and RL cases require a double rotation. \u2022 The case type refers to the position of the inserted node relative to the unbalanced node (\u03b1). \u2022 AVL trees do not stand for anything specific \u2013 they are named after the inventors Adelson-Velsky and Landis. Time Complexity of AVL Tree Operations \u2022 Search: O(log n) \u2022 Insertion: O(log n) (due to rebalancing) \u2022 Deletion: O(log n) (may require rotations) Hash Tables Definition \u2022 A hash table (also called a hash map) is a data structure that stores key-value pairs. \u2022 It uses a hash function to map keys to indices in an array. \u2022 Provides fast insert, delete, and lookup operations. \u2022 Dispersion: spread of hash values (we want good dispersion) Key Operations & Time Complexity \u2022 Insertion: O(1) on average, O(n) in the worst case (due to collisions). \u2022 Search: O(1) on average, O(n) in the worst case. \u2022 Deletion: O(1) on average, O(n) in the worst case. Hash Function \u2022 A hash function converts a key into an index. \u2022 It should: o Be fast to compute. o Distribute keys uniformly across the table (create dispersion). o Minimize collisions (different keys mapping to the same index). o"
    },
    "39": {
        "file": "DS4300 Notes.pdf",
        "page": 1,
        "chunk": "\u2022 Deletion: O(1) on average, O(n) in the worst case. Hash Function \u2022 A hash function converts a key into an index. \u2022 It should: o Be fast to compute. o Distribute keys uniformly across the table (create dispersion). o Minimize collisions (different keys mapping to the same index). o Should include modulo division with the table size to ensure it fits within the bounds Collisions"
    },
    "40": {
        "file": "DS4300 Notes.pdf",
        "page": 2,
        "chunk": "\u2022 A collision occurs when two keys produce the same index. \u2022 Collision Resolution Strategies: 1. Chaining (Separate Chaining) \u2022 Each index in the array stores a linked list (or another data structure) of key-value pairs. \u2022 When a collision occurs, the new key-value pair is added to the list at that index. \u2022 Pros: Simple, handles many collisions well. \u2022 Cons: Increased memory usage due to linked lists. 2. Open Addressing \u2022 Instead of using a linked list, all elements stay within the array itself. \u2022 If a collision occurs, the algorithm searches for an open slot. \u2022 Types of Open Addressing: o Linear Probing: If a slot is occupied, check the next slot (index + 1, wrap around if necessary). o Quadratic Probing: Check slots in a quadratic sequence (index + 1\u00b2, index + 2\u00b2, etc.). o Double Hashing: Use a second hash function to determine the step size for probing. \u2022 Pros: More cache-efficient, no extra memory for linked lists. \u2022 Cons: Can lead to clustering (many elements in the same region), reducing efficiency. Load Factor (\u03b1) \u2022 Load Factor = (number of elements) / (size of hash table). \u2022 Determines when to resize the table (typically when \u03b1 > 0.7). \u2022 Higher load factor \u2192 More collisions. \u2022 Lower load factor \u2192 More wasted space. Resizing & Rehashing \u2022 When the load factor exceeds a threshold (e.g., 0.7), the table is resized: 1. A new array (typically twice as large) is allocated. 2. Each key is rehashed into the new array. 3. The old array is discarded. \u2022 Rehashing is costly (O(n)), but reduces future collisions. Advantages of Hash Tables \u2022 Fast lookups, insertions, and deletions (O(1) average case). \u2022 Efficient memory usage for large datasets. \u2022 Used in many applications (caching, databases, sets, dictionaries). Disadvantages"
    },
    "41": {
        "file": "DS4300 Notes.pdf",
        "page": 2,
        "chunk": "rehashed into the new array. 3. The old array is discarded. \u2022 Rehashing is costly (O(n)), but reduces future collisions. Advantages of Hash Tables \u2022 Fast lookups, insertions, and deletions (O(1) average case). \u2022 Efficient memory usage for large datasets. \u2022 Used in many applications (caching, databases, sets, dictionaries). Disadvantages of Hash Tables \u2022 Worst-case O(n) time complexity if too many collisions occur. \u2022 Requires a good hash function to avoid inefficiency. \u2022 Rehashing is expensive when resizing. B+ Trees \u2022 Optimized for disk-based indexing, reducing disk access operations. \u2022 A B+ Tree is an m-way tree of order m, meaning: o Each node can have at most m keys. o Each node (except leaves) can have at most m+1 children. \u2022 Internal nodes only store keys and pointers to children (no data). \u2022 Leaf nodes store all data records and are linked as a doubly linked list for fast range queries. B+ Tree Properties \u2022 All nodes (except root) must be at least half full (at least \u2308m/2\u2309 keys)."
    },
    "42": {
        "file": "DS4300 Notes.pdf",
        "page": 3,
        "chunk": "\u2022 Root node does not need to be half full (can have fewer keys initially). \u2022 Insertion always happens at the leaf level. \u2022 Leaf nodes are connected as a doubly linked list, enabling efficient range queries. \u2022 Keys are always kept sorted within nodes. \u2022 Splitting nodes: o If a leaf node splits, the smallest key in the right half is copied up to the parent. o If an internal node splits, the middle key is moved up to the parent. B+ Tree Insertions Case 1: Inserting Without Splitting If the target leaf has space, insert the new key while keeping the order. Before inserting 25: Leaf Level: | 10 20 30 40 | After inserting 25: Leaf Level: | 10 20 25 30 40 | Case 2: Leaf Node Split If a leaf node is full, it must split into two. \u2022 The smallest key in the right half is copied to the parent. \u2022 The parent may also need to split if it becomes full. Before inserting 35 (Leaf Node Full): [ 20 40 ] / | \\ [10] [20 30] [40 50] After inserting 35 (Leaf Split): [ 20 30 40 ] / | \\ [10] [20 25] [30 35] [40 50] Case 3: Internal Node Split If an internal node is full after an insertion, it splits, and the middle key moves up. Before inserting 45 (Internal Node Full): [ 30 ] / \\ [10 20] [30 40 50] After inserting 45 (Internal Split):"
    },
    "43": {
        "file": "DS4300 Notes.pdf",
        "page": 4,
        "chunk": "[ 30 40 ] / | \\ [10 20] [30 35] [40 45 50] B+ Tree Deletions \u2022 Deletion happens at the leaf level. \u2022 If a node has too few keys, it may borrow from a sibling. \u2022 If borrowing is not possible, the node merges with a sibling, and the parent key is deleted. \u2022 The root is the only node allowed to have fewer than \u2308m/2\u2309 keys. Advantages of B+ Trees \u2022 Better disk efficiency: Nodes are designed to fit in memory pages. \u2022 Fast range queries: Leaves are linked, enabling fast sequential access. \u2022 More keys per node: Fewer levels compared to a binary search tree. \u2022 Efficient insertions & deletions: Keeps balance with minimal restructuring. AWS (Amazon Web Services) \u2022 Leading cloud platform with 200+ services for computing, storage, databases, networking, AI, and more. \u2022 Globally available through a network of regions (geographic areas) and availability zones (isolated data centers within regions). \u2022 Uses a pay-as-you-use model, meaning you only pay for the resources you consume. o Cost-effective compared to traditional on-premise data centers, but costs can scale quickly if not managed properly. AWS History & Growth \u2022 Launched in 2006 with only S3 (Simple Storage Service) and EC2 (Elastic Compute Cloud). \u2022 By 2010, AWS expanded with services like SimpleDB, EBS, RDS, DynamoDB, CloudWatch, CloudFront, and more. \u2022 Early adoption incentives: Amazon ran competitions with large prizes to encourage developers to use AWS. \u2022 Continuous innovation has led to 200+ services spanning operations, development, analytics, security, and AI. AWS Global Infrastructure \u2022 Regions: Geographic areas containing multiple data centers. \u2022 Availability Zones (AZs): Isolated data centers within a region, offering redundancy and fault tolerance. \u2022 POP (Points of Presence): Locations for Content Delivery Networks (CDN) to serve users faster (CloudFront uses these). Cloud Computing"
    },
    "44": {
        "file": "DS4300 Notes.pdf",
        "page": 4,
        "chunk": "analytics, security, and AI. AWS Global Infrastructure \u2022 Regions: Geographic areas containing multiple data centers. \u2022 Availability Zones (AZs): Isolated data centers within a region, offering redundancy and fault tolerance. \u2022 POP (Points of Presence): Locations for Content Delivery Networks (CDN) to serve users faster (CloudFront uses these). Cloud Computing Models"
    },
    "45": {
        "file": "DS4300 Notes.pdf",
        "page": 5,
        "chunk": "AWS supports the three main cloud service models: 1. Infrastructure as a Service (IaaS) o Provides virtualized computing resources (e.g., EC2, VPC, EBS). o User has the most control over configurations, networking, and OS. 2. Platform as a Service (PaaS) o Provides a managed platform for deploying applications (e.g., AWS Elastic Beanstalk, RDS). o AWS handles infrastructure, users focus on app development. 3. Software as a Service (SaaS) o Fully managed services where AWS handles everything (e.g., AWS Lambda, Amazon RDS, AI Services). o Users just interact with the software without worrying about infrastructure. AWS Shared Responsibility Model AWS operates under a shared security model, meaning security responsibilities are divided: AWS Responsibilities (Security OF the Cloud) \u2022 Protects physical infrastructure (data centers, power, networking). \u2022 Manages host operating systems & hypervisors. \u2022 Secures AWS-managed services (e.g., S3, RDS). Customer Responsibilities (Security IN the Cloud) \u2022 Controls data, encryption, and access management. \u2022 Manages Identity & Access Management (IAM) roles and policies. \u2022 Configures network security (e.g., security groups, VPN, firewalls). \u2022 Ensures compliance with governance policies. Core AWS Services Compute Services AWS provides multiple compute options: 1. VM-Based (Virtual Machines) o EC2 (Elastic Compute Cloud) \u2013 Virtual machines (VMs) with various instance types for different workloads. 2. Container-Based (Managed Containers) o ECS (Elastic Container Service) \u2013 Managed container orchestration for Docker. o EKS (Elastic Kubernetes Service) \u2013 Managed Kubernetes. o ECR (Elastic Container Registry) \u2013 Container image storage. o Fargate \u2013 Serverless container execution (no VM management). 3. Serverless Compute o AWS Lambda \u2013 Event-driven, serverless functions that scale automatically. Storage Services AWS offers various storage solutions based on needs: 1. Object Storage o Amazon S3 (Simple Storage Service) \u2013 Scalable, high-durability storage. \u2022 Supports time-limited upload links for secure file sharing."
    },
    "46": {
        "file": "DS4300 Notes.pdf",
        "page": 5,
        "chunk": "o AWS Lambda \u2013 Event-driven, serverless functions that scale automatically. Storage Services AWS offers various storage solutions based on needs: 1. Object Storage o Amazon S3 (Simple Storage Service) \u2013 Scalable, high-durability storage. \u2022 Supports time-limited upload links for secure file sharing."
    },
    "47": {
        "file": "DS4300 Notes.pdf",
        "page": 6,
        "chunk": "\u2022 Maximum file size: 5 TB. o Amazon Glacier \u2013 Low-cost archival storage. 2. Block Storage o Amazon EBS (Elastic Block Store) \u2013 Attachable disk storage for EC2 instances. 3. File Storage o Amazon EFS (Elastic File System) \u2013 Scalable, managed file storage for multiple EC2 instances. o Amazon File Cache \u2013 Cache storage for high-speed data access. 4. Backup & Recovery o AWS Backup \u2013 Centralized backup management. Database Services AWS provides both SQL and NoSQL database services: 1. Relational Databases (SQL-based) o Amazon RDS (Relational Database Service) \u2013 Managed SQL databases (MySQL, PostgreSQL, MariaDB, SQL Server, Oracle). o Amazon Aurora \u2013 MySQL/PostgreSQL-compatible, high-performance relational database. 2. NoSQL Databases o Amazon DynamoDB \u2013 Managed key-value database (similar to Redis). o Amazon DocumentDB \u2013 Managed NoSQL document database (MongoDB- compatible). o Amazon Neptune \u2013 Graph database service for complex relationships. 3. In-Memory Databases o Amazon ElastiCache \u2013 Managed Redis/Memcached caching for fast data retrieval. o Amazon MemoryDB \u2013 Fully managed Redis-compatible database. Networking & CDN \u2022 Amazon VPC (Virtual Private Cloud) \u2013 Isolated network for AWS resources. \u2022 AWS CloudFront \u2013 Amazon\u2019s Content Delivery Network (CDN) (competes with Cloudflare). Analytics Services AWS provides data analytics tools for big data processing, real-time streaming, and business intelligence: \u2022 Amazon Athena \u2013 SQL-based queries on S3 data. \u2022 Amazon EMR (Elastic MapReduce) \u2013 Managed Hadoop & Spark for big data. \u2022 AWS Glue \u2013 Serverless ETL (Extract, Transform, Load) service. \u2022 Amazon Redshift \u2013 Cloud-based data warehouse. \u2022 Amazon Kinesis \u2013 Real-time data streaming service. \u2022 Amazon QuickSight \u2013 Business intelligence & visualization tool. Machine Learning (ML) & AI Services"
    },
    "48": {
        "file": "DS4300 Notes.pdf",
        "page": 6,
        "chunk": "streaming service. \u2022 Amazon QuickSight \u2013 Business intelligence & visualization tool. Machine Learning (ML) & AI Services"
    },
    "49": {
        "file": "DS4300 Notes.pdf",
        "page": 7,
        "chunk": "AWS provides managed AI/ML services: 1. Amazon SageMaker \u2013 End-to-end machine learning platform for building, training, and deploying ML models. 2. AWS AI Services \u2013 Pre-built AI models: o Amazon Comprehend \u2013 Natural language processing (NLP). o Amazon Rekognition \u2013 Image and facial recognition. o Amazon Textract \u2013 Extracts text from scanned documents. o Amazon Translate \u2013 Language translation service. Why Use AWS? \u2022 Scalability \u2013 Auto-scaling and load balancing allow seamless growth. \u2022 Cost-Effectiveness \u2013 Pay only for what you use. \u2022 Security \u2013 AWS follows the highest security standards. \u2022 Flexibility \u2013 Supports multiple architectures (VMs, containers, serverless). \u2022 Global Reach \u2013 Available across the world with high availability. Final Thoughts AWS is the most widely adopted cloud platform due to its scalability, flexibility, and rich set of services. Understanding core AWS services, pricing, security, and deployment models is essential for cloud computing professionals."
    },
    "50": {
        "file": "04 - Data Replication.pdf",
        "page": 0,
        "chunk": "DS 4300 Replicating Data Mark Fontenot, PhD Northeastern University Some material used with permission from Dr. Rachlin, with thanks!"
    },
    "51": {
        "file": "04 - Data Replication.pdf",
        "page": 1,
        "chunk": "Distributing Data - Bene\ufb01ts 2 - Scalability / High throughput: Data volume or Read/Write load grows beyond the capacity of a single machine - Fault Tolerance / High Availability: Your application needs to continue working even if one or more machines goes down. - Latency: When you have users in different parts of the world you want to give them fast performance too"
    },
    "52": {
        "file": "04 - Data Replication.pdf",
        "page": 2,
        "chunk": "Distributed Data - Challenges - Consistency: Updates must be propagated across the network. - Application Complexity: Responsibility for reading and writing data in a distributed environment often falls to the application. 3"
    },
    "53": {
        "file": "04 - Data Replication.pdf",
        "page": 3,
        "chunk": "Vertical Scaling - Shared Memory Architectures - Geographically Centralized server - Some fault tolerance (via hot-swappable components) 4"
    },
    "54": {
        "file": "04 - Data Replication.pdf",
        "page": 4,
        "chunk": "Vertical Scaling - Shared Disk Architectures - Machines are connected via a fast network - Contention and the overhead of locking limit scalability (high-write volumes) \u2026 BUT ok for Data Warehouse applications (high read volumes) 5"
    },
    "55": {
        "file": "04 - Data Replication.pdf",
        "page": 5,
        "chunk": "AWS EC2 Pricing - Oct 2024 6 > $78,000/month https://aws.amazon.com/ec2/pricing/on-demand/"
    },
    "56": {
        "file": "04 - Data Replication.pdf",
        "page": 6,
        "chunk": "Horizontal Scaling - Shared Nothing Architectures \u25cfEach node has its own CPU, memory, and disk \u25cfCoordination via application layer using conventional network \u25cfGeographically distributed \u25cfCommodity hardware 7"
    },
    "57": {
        "file": "04 - Data Replication.pdf",
        "page": 7,
        "chunk": "Data - Replication vs Partitioning 8 Replicates have same data as Main Partitions have a subset of the data"
    },
    "58": {
        "file": "04 - Data Replication.pdf",
        "page": 8,
        "chunk": "Replication 9"
    },
    "59": {
        "file": "04 - Data Replication.pdf",
        "page": 9,
        "chunk": "Common Strategies for Replication - Single leader model - Multiple leader model - Leaderless model Distributed databases usually adopt one of these strategies. 10"
    },
    "60": {
        "file": "04 - Data Replication.pdf",
        "page": 10,
        "chunk": "Leader-Based Replication - All writes from clients go to the leader - Leader sends replication info to the followers - Followers process the instructions from the leader - Clients can read from either the leader or followers 11"
    },
    "61": {
        "file": "04 - Data Replication.pdf",
        "page": 11,
        "chunk": "Leader-Based Replication 12 This write could NOT be sent to one of the followers\u2026 only the leader."
    },
    "62": {
        "file": "04 - Data Replication.pdf",
        "page": 12,
        "chunk": "Leader-Based Replication - Very Common Strategy Relational: \u25cf MySQL, \u25cf Oracle, \u25cf SQL Server, \u25cf PostgreSQL NoSQL: \u25cf MongoDB, \u25cf RethinkDB (realtime web apps), \u25cf Espresso (LinkedIn) Messaging Brokers: Kafka, RabbitMQ 13"
    },
    "63": {
        "file": "04 - Data Replication.pdf",
        "page": 13,
        "chunk": "How Is Replication Info Transmitted to Followers? 14 Replication Method Description Statement-based Send INSERT, UPDATE, DELETEs to replica. Simple but error-prone due to non-deterministic functions like now(), trigger side-effects, and dif\ufb01culty in handling concurrent transactions. Write-ahead Log (WAL) A byte-level speci\ufb01c log of every change to the database. Leader and all followers must implement the same storage engine and makes upgrades dif\ufb01cult. Logical (row-based) Log For relational DBs: Inserted rows, modi\ufb01ed rows (before and after), deleted rows. A transaction log will identify all the rows that changed in each transaction and how they changed. Logical logs are decoupled from the storage engine and easier to parse. Trigger-based Changes are logged to a separate table whenever a trigger \ufb01res in response to an insert, update, or delete. Flexible because you can have application speci\ufb01c replication, but also more error prone."
    },
    "64": {
        "file": "04 - Data Replication.pdf",
        "page": 14,
        "chunk": "Synchronous vs Asynchronous Replication Synchronous: Leader waits for a response from the follower Asynchronous: Leader doesn\u2019t wait for con\ufb01rmation. 15 Synchronous: Asynchronous:"
    },
    "65": {
        "file": "04 - Data Replication.pdf",
        "page": 15,
        "chunk": "What Happens When the Leader Fails? Challenges: How do we pick a new Leader Node? \u25cfConsensus strategy \u2013 perhaps based on who has the most updates? \u25cfUse a controller node to appoint new leader? AND\u2026 how do we con\ufb01gure clients to start writing to the new leader? 16"
    },
    "66": {
        "file": "04 - Data Replication.pdf",
        "page": 16,
        "chunk": "What Happens When the Leader Fails? More Challenges: \u25cf If asynchronous replication is used, new leader may not have all the writes How do we recover the lost writes? Or do we simply discard? \u25cf After (if?) the old leader recovers, how do we avoid having multiple leaders receiving con\ufb02icting data? (Split brain: no way to resolve con\ufb02icting requests. \u25cf Leader failure detection. Optimal timeout is tricky. 17"
    },
    "67": {
        "file": "04 - Data Replication.pdf",
        "page": 17,
        "chunk": "Replication Lag refers to the time it takes for writes on the leader to be re\ufb02ected on all of the followers. \u25cfSynchronous replication: Replication lag causes writes to be slower and the system to be more brittle as num followers increases. \u25cfAsynchronous replication: We maintain availability but at the cost of delayed or eventual consistency. This delay is called the inconsistency window. Replication Lag 18"
    },
    "68": {
        "file": "04 - Data Replication.pdf",
        "page": 18,
        "chunk": "Read-after-Write Consistency Scenario - you\u2019re adding a comment to a Reddit post\u2026 after you click Submit and are back at the main post, your comment should show up for you. - Less important for other users to see your comment as immediately. 19"
    },
    "69": {
        "file": "04 - Data Replication.pdf",
        "page": 19,
        "chunk": "Implementing Read-After-Write Consistency Method 1: Modi\ufb01able data (from the client\u2019s perspective) is always read from the leader. 20"
    },
    "70": {
        "file": "04 - Data Replication.pdf",
        "page": 20,
        "chunk": "Implementing Read-After-Write Consistency Method 2: Dynamically switch to reading from leader for \u201crecently updated\u201d data. - For example, have a policy that all requests within one minute of last update come from leader. 21"
    },
    "71": {
        "file": "04 - Data Replication.pdf",
        "page": 21,
        "chunk": "But\u2026 This Can Create Its Own Challenges 22 We created followers so they would be proximal to users. BUT\u2026 now we have to route requests to distant leaders when reading modi\ufb01able data?? :("
    },
    "72": {
        "file": "04 - Data Replication.pdf",
        "page": 22,
        "chunk": "Monotonic Read Consistency Monotonic read anomalies: occur when a user reads values out of order from multiple followers. Monotonic read consistency: ensures that when a user makes multiple reads, they will not read older data after previously reading newer data. 23"
    },
    "73": {
        "file": "04 - Data Replication.pdf",
        "page": 23,
        "chunk": "Consistent Pre\ufb01x Reads Reading data out of order can occur if different partitions replicate data at different rates. There is no global write consistency. Consistent Pre\ufb01x Read Guarantee - ensures that if a sequence of writes happens in a certain order, anyone reading those writes will see them appear in the same order. 24 A B How far into the future can you see, Ms. B? About 10 seconds usually, Mr A."
    },
    "74": {
        "file": "04 - Data Replication.pdf",
        "page": 24,
        "chunk": "?? 25"
    },
    "75": {
        "file": "09 - Introduction to Graph Data Model.pdf",
        "page": 0,
        "chunk": "DS 4300 Introduction to the Graph Data Model Mark Fontenot, PhD Northeastern University Material referenced from Graph Algorithms - Practical Examples in Apache Spark and Neo4j by Needham and Hodler (O\u2019Reilly Press, 2019)"
    },
    "76": {
        "file": "09 - Introduction to Graph Data Model.pdf",
        "page": 1,
        "chunk": "What is a Graph Database - Data model based on the graph data structure - Composed of nodes and edges - edges connect nodes - each is uniquely identi\ufb01ed - each can contain properties (e.g. name, occupation, etc) - supports queries based on graph-oriented operations - traversals - shortest path - lots of others 2"
    },
    "77": {
        "file": "09 - Introduction to Graph Data Model.pdf",
        "page": 2,
        "chunk": "Where do Graphs Show up? - Social Networks - yes\u2026 things like Instagram, - but also\u2026 modeling social interactions in \ufb01elds like psychology and sociology - The Web - it is just a big graph of \u201cpages\u201d (nodes) connected by hyperlinks (edges) - Chemical and biological data - systems biology, genetics, etc. - interaction relationships in chemistry 3"
    },
    "78": {
        "file": "09 - Introduction to Graph Data Model.pdf",
        "page": 3,
        "chunk": "Basics of Graphs and Graph Theory 4"
    },
    "79": {
        "file": "09 - Introduction to Graph Data Model.pdf",
        "page": 4,
        "chunk": "What is a graph? Labeled Property Graph - Composed of a set of node (vertex) objects and relationship (edge) objects - Labels are used to mark a node as part of a group - Properties are attributes (think KV pairs) and can exist on nodes and relationships - Nodes with no associated relationships are OK. Edges not connected to nodes are not permitted. 5"
    },
    "80": {
        "file": "09 - Introduction to Graph Data Model.pdf",
        "page": 5,
        "chunk": "Example 2 Labels: - person - car 4 relationship types: - Drives - Owns - Lives_with - Married_to Properties 6"
    },
    "81": {
        "file": "09 - Introduction to Graph Data Model.pdf",
        "page": 6,
        "chunk": "Paths A path is an ordered sequence of nodes connected by edges in which no nodes or edges are repeated. 7 1 2 3 6 5 4 Ex: 1 \u2192 2 \u2192 6 \u2192 5 Not a path: 1 \u2192 2 \u2192 6 \u2192 2 \u2192 3"
    },
    "82": {
        "file": "09 - Introduction to Graph Data Model.pdf",
        "page": 7,
        "chunk": "Flavors of Graphs Connected (vs. Disconnected) \u2013 there is a path between any two nodes in the graph Weighted (vs. Unweighted) \u2013 edge has a weight property (important for some algorithms) Directed (vs. Undirected) \u2013 relationships (edges) de\ufb01ne a start and end node Acyclic (vs. Cyclic) \u2013 Graph contains no cycles 8"
    },
    "83": {
        "file": "09 - Introduction to Graph Data Model.pdf",
        "page": 8,
        "chunk": "Connected vs. Disconnected 9"
    },
    "84": {
        "file": "09 - Introduction to Graph Data Model.pdf",
        "page": 9,
        "chunk": "Weighted vs. Unweighted 10"
    },
    "85": {
        "file": "09 - Introduction to Graph Data Model.pdf",
        "page": 10,
        "chunk": "Directed vs. Undirected 11"
    },
    "86": {
        "file": "09 - Introduction to Graph Data Model.pdf",
        "page": 11,
        "chunk": "Cyclic vs Acyclic 12"
    },
    "87": {
        "file": "09 - Introduction to Graph Data Model.pdf",
        "page": 12,
        "chunk": "Sparse vs. Dense 13"
    },
    "88": {
        "file": "09 - Introduction to Graph Data Model.pdf",
        "page": 13,
        "chunk": "Trees 14"
    },
    "89": {
        "file": "09 - Introduction to Graph Data Model.pdf",
        "page": 14,
        "chunk": "Types of Graph Algorithms - Path\ufb01nding - Path\ufb01nding - \ufb01nding the shortest path between two nodes, if one exists, is probably the most common operation - \u201cshortest\u201d means fewest edges or lowest weight - Average Shortest Path can be used to monitor ef\ufb01ciency and resiliency of networks. - Minimum spanning tree, cycle detection, max/min \ufb02ow\u2026 are other types of path\ufb01nding 15"
    },
    "90": {
        "file": "09 - Introduction to Graph Data Model.pdf",
        "page": 15,
        "chunk": "BFS vs DFS 16"
    },
    "91": {
        "file": "09 - Introduction to Graph Data Model.pdf",
        "page": 16,
        "chunk": "Shortest Path 17"
    },
    "92": {
        "file": "09 - Introduction to Graph Data Model.pdf",
        "page": 17,
        "chunk": "Types of Graph Algorithms - Centrality & Community Detection - Centrality - determining which nodes are \u201cmore important\u201d in a network compared to other nodes - EX: Social Network In\ufb02uencers? - Community Detection - evaluate clustering or partitioning of nodes of a graph and tendency to strengthen or break apart 18"
    },
    "93": {
        "file": "09 - Introduction to Graph Data Model.pdf",
        "page": 18,
        "chunk": "Centrality 19"
    },
    "94": {
        "file": "09 - Introduction to Graph Data Model.pdf",
        "page": 19,
        "chunk": "Some Famous Graph Algorithms - Dijkstra\u2019s Algorithm - single-source shortest path algo for positively weighted graphs - A* Algorithm - Similar to Dijkstra\u2019s with added feature of using a heuristic to guide traversal - PageRank - measures the importance of each node within a graph based on the number of incoming relationships and the importance of the nodes from those incoming relationships 20"
    },
    "95": {
        "file": "09 - Introduction to Graph Data Model.pdf",
        "page": 20,
        "chunk": "Neo4j - A Graph Database System that supports both transactional and analytical processing of graph-based data - Relatively new class of no-sql DBs - Considered schema optional (one can be imposed) - Supports various types of indexing - ACID compliant - Supports distributed computing - Similar: Microsoft CosmoDB, Amazon Neptune 21"
    },
    "96": {
        "file": "09 - Introduction to Graph Data Model.pdf",
        "page": 21,
        "chunk": "?? 22"
    },
    "97": {
        "file": "02 - Foundations.pdf",
        "page": 0,
        "chunk": "DS 4300 Large Scale Information Storage and Retrieval Foundations Mark Fontenot, PhD Northeastern University"
    },
    "98": {
        "file": "02 - Foundations.pdf",
        "page": 1,
        "chunk": "Searching \u25cfSearching is the most common operation performed by a database system \u25cfIn SQL, the SELECT statement is arguably the most versatile / complex. \u25cfBaseline for ef\ufb01ciency is Linear Search \u25cb Start at the beginning of a list and proceed element by element until: \u25a0 You \ufb01nd what you\u2019re looking for \u25a0 You get to the last element and haven\u2019t found it 2"
    },
    "99": {
        "file": "02 - Foundations.pdf",
        "page": 2,
        "chunk": "Searching \u25cfRecord - A collection of values for attributes of a single entity instance; a row of a table \u25cfCollection - a set of records of the same entity type; a table \u25cb Trivially, stored in some sequential order like a list \u25cfSearch Key - A value for an attribute from the entity type \u25cb Could be >= 1 attribute 3"
    },
    "100": {
        "file": "02 - Foundations.pdf",
        "page": 3,
        "chunk": "Lists of Records \u25cfIf each record takes up x bytes of memory, then for n records, we need n*x bytes of memory. \u25cfContiguously Allocated List \u25cb All n*x bytes are allocated as a single \u201cchunk\u201d of memory \u25cfLinked List \u25cb Each record needs x bytes + additional space for 1 or 2 memory addresses \u25cb Individual records are linked together in a type of chain using memory addresses 4"
    },
    "101": {
        "file": "02 - Foundations.pdf",
        "page": 4,
        "chunk": "Contiguous vs Linked 5 6 Records Contiguously Allocated - Array front back 6 Records Linked by memory addresses - Linked List Extra storage for a memory address"
    },
    "102": {
        "file": "02 - Foundations.pdf",
        "page": 5,
        "chunk": "Pros and Cons \u25cfArrays are faster for random access, but slow for inserting anywhere but the end \u25cfLinked Lists are faster for inserting anywhere in the list, but slower for random access 6 Insert after 2nd record records: records: 5 records had to be moved to make space Insert after 2nd record"
    },
    "103": {
        "file": "02 - Foundations.pdf",
        "page": 6,
        "chunk": "Observations: - Arrays - fast for random access - slow for random insertions - Linked Lists - slow for random access - fast for random insertions 7"
    },
    "104": {
        "file": "02 - Foundations.pdf",
        "page": 7,
        "chunk": "Binary Search \u25cf Input: array of values in sorted order, target value \u25cf Output: the location (index) of where target is located or some value indicating target was not found def binary_search(arr, target) left, right = 0, len(arr) - 1 while left <= right: mid = (left + right) // 2 if arr[mid] == target: return mid elif arr[mid] < target: left = mid + 1 else: right = mid - 1 return -1 8 A C G M P R Z target = A mid Since target < arr[mid], we reset right to mid - 1. left right A C G M P R Z target = A mid left right"
    },
    "105": {
        "file": "02 - Foundations.pdf",
        "page": 8,
        "chunk": "Time Complexity \u25cfLinear Search \u25cb Best case: target is found at the \ufb01rst element; only 1 comparison \u25cb Worst case: target is not in the array; n comparisons \u25cb Therefore, in the worst case, linear search is O(n) time complexity. \u25cfBinary Search \u25cb Best case: target is found at mid; 1 comparison (inside the loop) \u25cb Worst case: target is not in the array; log2 n comparisons \u25cb Therefore, in the worst case, binary search is O(log2n) time complexity. 9"
    },
    "106": {
        "file": "02 - Foundations.pdf",
        "page": 9,
        "chunk": "Back to Database Searching \u25cf Assume data is stored on disk by column id\u2019s value \u25cf Searching for a speci\ufb01c id = fast. \u25cf But what if we want to search for a speci\ufb01c specialVal? \u25cb Only option is linear scan of that column \u25cf Can\u2019t store data on disk sorted by both id and specialVal (at the same time) \u25cb data would have to be duplicated \u2192 space inef\ufb01cient 10"
    },
    "107": {
        "file": "02 - Foundations.pdf",
        "page": 10,
        "chunk": "Back to Database Searching \u25cf Assume data is stored on disk by column id\u2019s value \u25cf Searching for a speci\ufb01c id = fast. \u25cf But what if we want to search for a speci\ufb01c specialVal? \u25cb Only option is linear scan of that column \u25cf Can\u2019t store data on disk sorted by both id and specialVal (at the same time) \u25cb data would have to be duplicated \u2192 space inef\ufb01cient 11 We need an external data structure to support faster searching by specialVal than a linear scan."
    },
    "108": {
        "file": "02 - Foundations.pdf",
        "page": 11,
        "chunk": "What do we have in our arsenal? 1) An array of tuples (specialVal, rowNumber) sorted by specialVal a) We could use Binary Search to quickly locate a particular specialVal and \ufb01nd its corresponding row in the table b) But, every insert into the table would be like inserting into a sorted array - slow\u2026 2) A linked list of tuples (specialVal, rowNumber) sorted by specialVal a) searching for a specialVal would be slow - linear scan required b) But inserting into the table would theoretically be quick to also add to the list. 12"
    },
    "109": {
        "file": "02 - Foundations.pdf",
        "page": 12,
        "chunk": "Something with Fast Insert and Fast Search? - Binary Search Tree - a binary tree where every node in the left subtree is less than its parent and every node in the right subtree is greater than its parent. 13 Image from: https://courses.grainger.illinois.edu/cs225/sp2019/notes/bst/"
    },
    "110": {
        "file": "02 - Foundations.pdf",
        "page": 13,
        "chunk": "To the Board! 14"
    },
    "111": {
        "file": "10 - Neo4j.pdf",
        "page": 0,
        "chunk": "DS 4300 Neo4j Mark Fontenot, PhD Northeastern University Material referenced from Graph Algorithms - Practical Examples in Apache Spark and Neo4j by Needham and Hodler (O\u2019Reilly Press, 2019)"
    },
    "112": {
        "file": "10 - Neo4j.pdf",
        "page": 1,
        "chunk": "Neo4j - A Graph Database System that supports both transactional and analytical processing of graph-based data - Relatively new class of no-sql DBs - Considered schema optional (one can be imposed) - Supports various types of indexing - ACID compliant - Supports distributed computing - Similar: Microsoft CosmoDB, Amazon Neptune 2"
    },
    "113": {
        "file": "10 - Neo4j.pdf",
        "page": 2,
        "chunk": "Neo4j - Query Language and Plugins - Cypher - Neo4j\u2019s graph query language created in 2011 - Goal: SQL-equivalent language for graph databases - Provides a visual way of matching patterns and relationships (nodes)-[:CONNECT_TO]->(otherNodes) - APOC Plugin - Awesome Procedures on Cypher - Add-on library that provides hundreds of procedures and functions - Graph Data Science Plugin - provides ef\ufb01cient implementations of common graph algorithms (like the ones we talked about yesterday) 3"
    },
    "114": {
        "file": "10 - Neo4j.pdf",
        "page": 3,
        "chunk": "Neo4j in Docker Compose 4"
    },
    "115": {
        "file": "10 - Neo4j.pdf",
        "page": 4,
        "chunk": "Docker Compose 5 \u25cfSupports multi-container management. \u25cfSet-up is declarative - using YAML docker-compose.yaml \ufb01le \u25cb services \u25cb volumes \u25cb networks, etc. \u25cf1 command can be used to start, stop, or scale a number of services at one time. \u25cfProvides a consistent method for producing an identical environment (no more \u201cwell\u2026 it works on my machine!) \u25cfInteraction is mostly via command line"
    },
    "116": {
        "file": "10 - Neo4j.pdf",
        "page": 5,
        "chunk": "docker-compose.yaml 6 services: neo4j: container_name: neo4j image: neo4j:latest ports: - 7474:7474 - 7687:7687 environment: - NEO4J_AUTH=neo4j/${NEO4J_PASSWORD} - NEO4J_apoc_export_file_enabled=true - NEO4J_apoc_import_file_enabled=true - NEO4J_apoc_import_file_use__neo4j__config=true - NEO4J_PLUGINS=[\"apoc\", \"graph-data-science\"] volumes: - ./neo4j_db/data:/data - ./neo4j_db/logs:/logs - ./neo4j_db/import:/var/lib/neo4j/import - ./neo4j_db/plugins:/plugins Never put \u201csecrets\u201d in a docker compose \ufb01le. Use .env \ufb01les."
    },
    "117": {
        "file": "10 - Neo4j.pdf",
        "page": 6,
        "chunk": ".env Files - .env \ufb01les - stores a collection of environment variables - good way to keep environment variables for different platforms separate - .env.local - .env.dev - .env.prod 7 NEO4J_PASSWORD=abc123!!! .env file"
    },
    "118": {
        "file": "10 - Neo4j.pdf",
        "page": 7,
        "chunk": "Docker Compose Commands \u25cfTo test if you have Docker CLI properly installed, run: docker --version \u25cfMajor Docker Commands \u25cb docker compose up \u25cb docker compose up -d \u25cb docker compose down \u25cb docker compose start \u25cb docker compose stop \u25cb docker compose build \u25cb docker compose build --no-cache 8"
    },
    "119": {
        "file": "10 - Neo4j.pdf",
        "page": 8,
        "chunk": "localhost:7474 9"
    },
    "120": {
        "file": "10 - Neo4j.pdf",
        "page": 9,
        "chunk": "Neo4j Browser 10 https://neo4j.com/docs/browser-manual/current/visual-tour/ localhost:7474 Then login."
    },
    "121": {
        "file": "10 - Neo4j.pdf",
        "page": 10,
        "chunk": "Inserting Data by Creating Nodes CREATE (:User {name: \"Alice\", birthPlace: \"Paris\"}) CREATE (:User {name: \"Bob\", birthPlace: \"London\"}) CREATE (:User {name: \"Carol\", birthPlace: \"London\"}) CREATE (:User {name: \"Dave\", birthPlace: \"London\"}) CREATE (:User {name: \"Eve\", birthPlace: \"Rome\"}) 11"
    },
    "122": {
        "file": "10 - Neo4j.pdf",
        "page": 11,
        "chunk": "Adding an Edge with No Variable Names CREATE (:User {name: \"Alice\", birthPlace: \"Paris\"}) CREATE (:User {name: \"Bob\", birthPlace: \"London\"}) MATCH (alice:User {name:\u201dAlice\u201d}) MATCH (bob:User {name: \u201cBob\u201d}) CREATE (alice)-[:KNOWS {since: \u201c2022-12-01\u201d}]->(bob) 12 Note: Relationships are directed in neo4j."
    },
    "123": {
        "file": "10 - Neo4j.pdf",
        "page": 12,
        "chunk": "Matching Which users were born in London? MATCH (usr:User {birthPlace: \u201cLondon\u201d}) RETURN usr.name, usr.birthPlace 13"
    },
    "124": {
        "file": "10 - Neo4j.pdf",
        "page": 13,
        "chunk": "Download Dataset and Move to Import Folder Clone this repo: https://github.com/PacktPublishing/Graph-Data-Science-with-Neo4j In Chapter02/data of data repo, unzip the net\ufb02ix.zip \ufb01le Copy net\ufb02ix_titles.csv into the following folder where you put your docker compose \ufb01le neo4j_db/neo4j_db/import 14"
    },
    "125": {
        "file": "10 - Neo4j.pdf",
        "page": 14,
        "chunk": "Importing Data 15"
    },
    "126": {
        "file": "10 - Neo4j.pdf",
        "page": 15,
        "chunk": "Basic Data Importing LOAD CSV WITH HEADERS FROM 'file:///netflix_titles.csv' AS line CREATE(:Movie { id: line.show_id, title: line.title, releaseYear: line.release_year } ) 16 Type the following into the Cypher Editor in Neo4j Browser"
    },
    "127": {
        "file": "10 - Neo4j.pdf",
        "page": 16,
        "chunk": "Loading CSVs - General Syntax LOAD CSV [WITH HEADERS] FROM 'file:///file_in_import_folder.csv' AS line [FIELDTERMINATOR ','] // do stuffs with 'line' 17"
    },
    "128": {
        "file": "10 - Neo4j.pdf",
        "page": 17,
        "chunk": "Importing with Directors this Time LOAD CSV WITH HEADERS FROM 'file:///netflix_titles.csv' AS line WITH split(line.director, \",\") as directors_list UNWIND directors_list AS director_name CREATE (:Person {name: trim(director_name)}) But this generates duplicate Person nodes (a director can direct more than 1 movie) 18"
    },
    "129": {
        "file": "10 - Neo4j.pdf",
        "page": 18,
        "chunk": "Importing with Directors Merged MATCH (p:Person) DELETE p LOAD CSV WITH HEADERS FROM 'file:///netflix_titles.csv' AS line WITH split(line.director, \",\") as directors_list UNWIND directors_list AS director_name MERGE (:Person {name: director_name}) 19"
    },
    "130": {
        "file": "10 - Neo4j.pdf",
        "page": 19,
        "chunk": "Adding Edges LOAD CSV WITH HEADERS FROM 'file:///netflix_titles.csv' AS line MATCH (m:Movie {id: line.show_id}) WITH m, split(line.director, \",\") as directors_list UNWIND directors_list AS director_name MATCH (p:Person {name: director_name}) CREATE (p)-[:DIRECTED]->(m) 20"
    },
    "131": {
        "file": "10 - Neo4j.pdf",
        "page": 20,
        "chunk": "Gut Check Let\u2019s check the movie titled Ray: MATCH (m:Movie {title: \"Ray\"})<-[:DIRECTED]-(p:Person) RETURN m, p 21"
    },
    "132": {
        "file": "10 - Neo4j.pdf",
        "page": 21,
        "chunk": "?? 22"
    },
    "133": {
        "file": "BST Trees.pdf",
        "page": 0,
        "chunk": "Binary Search Trees \u25cf\u200b Insertion - A new key is always inserted at the leaf by maintaining the property of the binary search tree. We start searching for a key from the root until we hit a leaf node. Once a leaf node is found, the new node is added as a child of the leaf node. The below steps are followed while we try to insert a node into a binary search tree: \u25cb\u200b Initialize the current node (say, currNode or node) with root node \u25cb\u200b Compare the key with the current node. \u25cb\u200b Move left if the key is less than or equal to the current node value. \u25cb\u200b Move right if the key is greater than the current node value. \u25cb\u200b Repeat steps 2 and 3 until you reach a leaf node. \u25cb\u200b Attach the new key as a left or right child based on the comparison with the leaf node\u2019s value. \u25cf\u200b Traversal \u25cb\u200b Preorder Traversal \u25a0\u200b The root node of the subtree is visited first. \u25a0\u200b Then the left subtree is traversed. \u25a0\u200b At last, the right subtree is traversed. \u25cb\u200b Postorder Traversal \u25a0\u200b The root node of the subtree is visited first. \u25a0\u200b Then the left subtree is traversed. \u25a0\u200b At last, the right subtree is traversed. \u25cb\u200b Inorder Traversal \u25a0\u200b The left subtree is traversed first \u25a0\u200b Then the root node for that subtree is traversed \u25a0\u200b Finally, the right subtree is traversed \u25cb\u200b Level Order Traversal \u25a0\u200b Given the root of a binary tree, return the level order traversal of its nodes' values. (i.e., from left to right, level by level)."
    },
    "134": {
        "file": "BST Trees.pdf",
        "page": 0,
        "chunk": "binary tree, return the level order traversal of its nodes' values. (i.e., from left to right, level by level)."
    },
    "135": {
        "file": "Extended Notes - Foundations Slide Deck.pdf",
        "page": 0,
        "chunk": "Searching: -\u200b Searching is the most common operation performed by a database system -\u200b In SQL, the SELECT statement is arguably the most versatile / complex (they can be recursive and there can even be select statements in select statements) General Vocab: -\u200b Record: a collection of values for attributes of a single entity instance; a row of a table -\u200b Collection: a set of records of the same entity type; a table (trivially stored in some sequential order like a list) -\u200b Search Key: a value for an attribute from the entity type (could be >= 1 attribute) id specVal 1 55 2 87 3 50 4 108 -\u200b Assume data is stored on disk by column id\u2019s value, searching for a specific id is fast -\u200b But searching for specific specialVal since data is unsorted, the only option is linear scan the column -\u200b Can\u2019t store data on disk sorted by both id and specialVal at the same time so the data would have to be duplicated and there\u2019d be inefficient space -\u200b Therefore we need an external data structure to support faster searching by specialVal than a linear search -\u200b What to do? -\u200b An array of tuples (specialVal, rowNumber) sorted by specialVal -\u200b We could use Binary Search to quickly locate a particular specialVal and find its corresponding row in the table -\u200b But, every insert into the table would be like inserting into a sorted array - slow\u2026 -\u200b OR A linked list of tuples (specialVal, rowNumber) sorted by specialVal -\u200b searching for a specialVal would be slow - linear scan required -\u200b But inserting into the table would theoretically be quick to also add to the list\u2026 INSTEAD USE BINARY SEARCH TREE Linear Search: -\u200b Baseline for efficiency where you start at the beginning"
    },
    "136": {
        "file": "Extended Notes - Foundations Slide Deck.pdf",
        "page": 0,
        "chunk": "(specialVal, rowNumber) sorted by specialVal -\u200b searching for a specialVal would be slow - linear scan required -\u200b But inserting into the table would theoretically be quick to also add to the list\u2026 INSTEAD USE BINARY SEARCH TREE Linear Search: -\u200b Baseline for efficiency where you start at the beginning of a list and proceed element element by element until you either find what you\u2019re looking for or get to the last element and haven\u2019t found it aka O(n) -\u200b If each record takes up x bytes of memory, then for n records, we need n*x bytes of memory -\u200b There are 2 different Data Structures for Linear Search: 1.\u200b Contiguously Allocated List (aka Array): all n*x are allocated as a single \u201cchunk\u201d of memory"
    },
    "137": {
        "file": "Extended Notes - Foundations Slide Deck.pdf",
        "page": 1,
        "chunk": "-\u200b Pro: Faster for random access -\u200b Con: Slow for inserting anywhere but the end 2.\u200b Linked List: each record needs x bytes and additional space for 1 or 2 memory addresses - individual records are linked together in a type of chain using memory addresses -\u200b Pro: Faster for inserting anywhere in the list -\u200b Con: Slower for random access -\u200b Best Case: target is found at the first element where only 1 comparison is needed -\u200b Worst Case: target is not in the array; n comparisons - O(n) time complexity is for worst case Binary Search: -\u200b Input: array of values in sorted order, target value -\u200b Output: location (index) of where target is located or some value indicating target was not found -\u200b Best Case: target is found at mid; 1 comparison (inside the loop) -\u200b Worst Case: target is not in the array; log2n comparisons - O(log2n) time complexity is for worse case Binary Search Tree: -\u200b A binary tree where every node in the left subtree is less than its parent and every node in the right subtree is greater than its parent"
    },
    "138": {
        "file": "Extended Notes - NoSQL Intro + KV DBs.pdf",
        "page": 0,
        "chunk": "Distributed DBs and ACID - Pessimistic Concurrency: ACID Transactions -\u200b ACID is a set of properties that guarantee reliable processing of database transactions: -\u200b Atomicity: Each transaction is treated as a single, indivisible unit. Either all operations in the transaction succeed, or none of them do. -\u200b Consistency: Transactions take the database from one valid state to another, preserving the integrity constraints. -\u200b Isolation: Transactions are isolated from each other, ensuring that concurrently executing transactions do not interfere. -\u200b Durability: Once a transaction is committed, it\u2019s permanent and will survive system crashes. Pessimistic Concurrency Control -\u200b Pessimistic concurrency control is a strategy for managing concurrent access to database resources in a way that prevents conflicts. -\u200b The key idea behind pessimistic concurrency is that conflicts between transactions are likely to happen, so the system assumes that if something can go wrong, it probably will. As a result, it proactively prevents conflicts from happening during the execution of transactions. -\u200b In other words, it operates on the assumption that other transactions will interfere, so it uses techniques to lock resources to avoid conflicts. How Does Pessimistic Concurrency Work? -\u200b Locking Resources: -\u200b To avoid concurrent transactions from interfering with each other, the database will lock resources (e.g., rows, tables, or even entire databases) until the transaction is complete. -\u200b This ensures that no other transaction can modify the same data at the same time. -\u200b There are two types of locks used: 1.\u200b Read Locks: Prevent other transactions from writing to the resource but allow them to read it. 2.\u200b Write Locks: Prevent other transactions from reading or writing to the resource until the lock is released. -\u200b This creates a serialized execution of transactions, ensuring consistency and isolation but at the cost of performance and concurrency. Write Lock Analogy Analogy:"
    },
    "139": {
        "file": "Extended Notes - NoSQL Intro + KV DBs.pdf",
        "page": 0,
        "chunk": "the resource but allow them to read it. 2.\u200b Write Locks: Prevent other transactions from reading or writing to the resource until the lock is released. -\u200b This creates a serialized execution of transactions, ensuring consistency and isolation but at the cost of performance and concurrency. Write Lock Analogy Analogy: Borrowing a Book from a Library -\u200b Imagine you want to borrow a book from the library. When you take the book, no one else can borrow it until you return it. -\u200b This is similar to a write lock: when a transaction locks a resource (e.g., a row in a database), no other transaction can access it (either for reading or writing) until the transaction is completed and the lock is released."
    },
    "140": {
        "file": "Extended Notes - NoSQL Intro + KV DBs.pdf",
        "page": 1,
        "chunk": "-\u200b Example: -\u200b Transaction A locks \"Book A\" (read/write lock). -\u200b Transaction B wants to read \"Book A\" but cannot until Transaction A finishes and releases the lock. Why Pessimistic Concurrency? -\u200b Data Safety: -\u200b The focus of pessimistic concurrency is on data safety and ensuring that transactions are executed correctly without interference. -\u200b By using locks, the system prevents conflicts and ensures that only one transaction can alter data at any given time, preventing inconsistencies or corruption. -\u200b Use Cases: -\u200b Pessimistic concurrency control is ideal for environments where conflicts are highly likely or when data integrity is critical (e.g., financial systems, inventory systems, or applications with high-value transactions). Challenges of Pessimistic Concurrency -\u200b Deadlock: Since transactions hold locks, they can sometimes end up in a situation where two transactions are waiting on each other to release a resource, leading to a deadlock. The system must have a mechanism for detecting and resolving deadlocks, often by aborting one of the transactions to break the cycle. -\u200b Performance Impact: Locking can significantly reduce throughput and increase response times, especially when there are many concurrent transactions. Since each transaction has to wait for others to release locks, this leads to inefficiencies in highly concurrent environments. -\u200b Additionally, the more data that is locked, the greater the chance of contention between transactions, further slowing down performance. Optimistic Concurrency: Overview -\u200b In optimistic concurrency control, transactions do not acquire locks on data when reading or writing. Instead, the system assumes that conflicts will be rare, and thus transactions proceed under the assumption that no other transaction will interfere with their work. -\u200b Optimistic because it works under the assumption that conflicts between transactions are unlikely to happen, and even if they do, it\u2019s not a problem. This approach minimizes contention for resources and"
    },
    "141": {
        "file": "Extended Notes - NoSQL Intro + KV DBs.pdf",
        "page": 1,
        "chunk": "be rare, and thus transactions proceed under the assumption that no other transaction will interfere with their work. -\u200b Optimistic because it works under the assumption that conflicts between transactions are unlikely to happen, and even if they do, it\u2019s not a problem. This approach minimizes contention for resources and allows for more concurrent transactions. How Does Optimistic Concurrency Work? 1.\u200b Reading Data: -\u200b A transaction reads the data it needs to work with and also fetches a timestamp or version number associated with the data. -\u200b These are used to track when the data was last modified and are attached to each row in the database. 2.\u200b Making Changes:"
    },
    "142": {
        "file": "Extended Notes - NoSQL Intro + KV DBs.pdf",
        "page": 2,
        "chunk": "-\u200b The transaction proceeds with making changes to the data without locking it, assuming no one else is modifying the same data at the same time. -\u200b During this phase, other transactions can read and write to the same data without interference (because no locks are in place). 3.\u200b Checking for Conflicts: -\u200b Before committing the transaction, it checks if any of the data it worked with has been modified by another transaction since it was first read. This is done by comparing the timestamp or version number that was initially retrieved with the current state of the data. -\u200b If the data has been modified (i.e., if the timestamp/version number doesn\u2019t match), a conflict is detected, and the transaction may be rolled back and retried. 4.\u200b Commit: -\u200b If no conflicts are found, the transaction proceeds to commit the changes, and the new timestamp/version number is updated in the database. Why Is It Optimistic? -\u200b Optimism comes from the assumption that conflicts are rare and that most transactions can proceed without interference. Rather than preemptively locking resources (as in pessimistic concurrency), the system relies on validation at commit time to detect conflicts. -\u200b The idea is that, by not locking data during the transaction, you can achieve higher throughput and better concurrency, especially in systems with lower conflict rates. Types of Systems Where Optimistic Concurrency Works Well 1.\u200b Low Conflict Systems -\u200b Backups: Systems performing operations like backups where few transactions occur concurrently. -\u200b Analytical Databases: In environments where data is mostly read-heavy and transactions involve more querying than updating, conflicts are less likely, and optimistic concurrency works effectively. -\u200b In these systems, conflicts are rare, and when they do occur, it\u2019s often acceptable to roll back and retry the transaction. -\u200b Benefits: -\u200b Higher throughput: Without the need"
    },
    "143": {
        "file": "Extended Notes - NoSQL Intro + KV DBs.pdf",
        "page": 2,
        "chunk": "data is mostly read-heavy and transactions involve more querying than updating, conflicts are less likely, and optimistic concurrency works effectively. -\u200b In these systems, conflicts are rare, and when they do occur, it\u2019s often acceptable to roll back and retry the transaction. -\u200b Benefits: -\u200b Higher throughput: Without the need for locks, more transactions can run concurrently, improving the performance of read-heavy systems. -\u200b Less contention: Since no locks are being placed, there is no waiting for locks to be released, so overall system performance can improve. 2.\u200b Read-heavy Systems -\u200b In read-heavy systems, most operations involve retrieving data rather than modifying it. Optimistic concurrency is a good fit for these environments because the likelihood of data contention (two transactions trying to modify the same data at the same time) is minimal."
    },
    "144": {
        "file": "Extended Notes - NoSQL Intro + KV DBs.pdf",
        "page": 3,
        "chunk": "-\u200b Example: A news website where users are reading articles, but only a few are submitting comments or editing content. Most transactions are reads, so the likelihood of conflicts is low, and optimistic concurrency can ensure that the system scales efficiently. Handling Conflicts in Optimistic Concurrency -\u200b If two transactions try to modify the same data at the same time, optimistic concurrency detects this at the commit phase. -\u200b If a conflict is detected (i.e., the data has been modified by another transaction after it was read), the transaction is rolled back and the process is retried. -\u200b This rollback and retry mechanism means the system can handle conflicts without locking data, but it introduces an overhead because transactions need to be retried if conflicts are detected. High Conflict Systems -\u200b Challenges: -\u200b In high conflict systems, where many transactions are competing for the same data, the rollback and retry approach becomes less efficient. -\u200b If conflicts happen frequently, the system might end up spending a lot of time rolling back and retrying transactions, which can reduce performance. -\u200b In such cases, pessimistic concurrency (using locks to prevent conflicts before they happen) may be preferable, even though it might limit concurrency. -\u200b High conflict systems may require a more predictable and controlled approach to ensure consistency. Comparison with Pessimistic Concurrency -\u200b Pessimistic Concurrency (locks data to prevent conflicts before they happen) is more appropriate for high-conflict systems where data contention is frequent, and ensuring consistency is paramount. -\u200b Optimistic Concurrency (checks for conflicts at commit time) is more suitable for low-conflict systems, where the overhead of managing locks is unnecessary and higher concurrency is desirable. No SQL: Origin of the Term \"NoSQL\" -\u200b The term \"NoSQL\" was first used in 1998 by Carlo Strozzi to describe his lightweight, open-source relational"
    },
    "145": {
        "file": "Extended Notes - NoSQL Intro + KV DBs.pdf",
        "page": 3,
        "chunk": "(checks for conflicts at commit time) is more suitable for low-conflict systems, where the overhead of managing locks is unnecessary and higher concurrency is desirable. No SQL: Origin of the Term \"NoSQL\" -\u200b The term \"NoSQL\" was first used in 1998 by Carlo Strozzi to describe his lightweight, open-source relational database system that did not use SQL for querying. -\u200b However, the modern understanding of NoSQL databases is quite different from Strozzi\u2019s original use of the term. Modern Meaning: \"Not Only SQL\" -\u200b Today, NoSQL is generally understood to mean \"Not Only SQL\", rather than strictly \"No SQL\" at all."
    },
    "146": {
        "file": "Extended Notes - NoSQL Intro + KV DBs.pdf",
        "page": 4,
        "chunk": "-\u200b While NoSQL databases often do not follow the traditional relational database model, some still allow for SQL-like querying or structured querying mechanisms. Is NoSQL Always Non-Relational? -\u200b NoSQL databases are often thought of as non-relational databases because they do not use the traditional table-based structure with strict schemas like relational databases (RDBMS). -\u200b Instead, they allow for more flexible data models, including: -\u200b Key-Value Stores (e.g., Redis, DynamoDB) -\u200b Document Stores (e.g., MongoDB, CouchDB) -\u200b Column-Family Stores (e.g., Apache Cassandra, HBase) -\u200b Graph Databases (e.g., Neo4j, ArangoDB) Why NoSQL? -\u200b NoSQL databases were originally developed in response to the need for handling large-scale, web-based, and unstructured data that relational databases struggled with. -\u200b The rise of Big Data, social media, and cloud computing created new challenges that required scalable, flexible, and high-performance database solutions that could handle: -\u200b Massive amounts of unstructured or semi-structured data (e.g., JSON, XML, multimedia content). -\u200b High-speed reads and writes (e.g., caching systems, real-time analytics). -\u200b Distributed and horizontally scalable architectures (e.g., global applications, cloud-based services). Key Characteristics of NoSQL Databases -\u200b Schema Flexibility: Unlike relational databases, NoSQL databases do not require predefined schemas. This makes it easier to evolve data models over time. -\u200b Horizontal Scalability: Many NoSQL databases are designed to scale horizontally (adding more machines) rather than vertically (adding more power to a single machine). -\u200b High Availability & Partition Tolerance: NoSQL databases often favor availability and partition tolerance over strict consistency, as per the CAP theorem. -\u200b Optimized for Specific Use Cases: Rather than a one-size-fits-all approach, NoSQL databases specialize in different types of workloads (e.g., key-value stores for caching, document stores for flexible data representation, graph databases for connected data)."
    },
    "147": {
        "file": "Extended Notes - NoSQL Intro + KV DBs.pdf",
        "page": 4,
        "chunk": "Use Cases: Rather than a one-size-fits-all approach, NoSQL databases specialize in different types of workloads (e.g., key-value stores for caching, document stores for flexible data representation, graph databases for connected data)."
    },
    "148": {
        "file": "Extended Notes - NoSQL Intro + KV DBs.pdf",
        "page": 5,
        "chunk": "CAP Theorem Review -\u200b The CAP theorem, proposed by Eric Brewer in 2000, states that a distributed database system can only guarantee two out of three of the following properties at any given time: 1.\u200b Consistency (C) \u2013 Every user of the database has an identical view of the data at any given instant. -\u200b All nodes return the most recent version of the data. -\u200b No stale or conflicting versions exist. -\u200b If a write occurs, all subsequent reads must reflect that change immediately. 2.\u200b Availability (A) \u2013 The database remains operational even in the event of failures. -\u200b Every request always receives a response (though it may not be the latest data). -\u200b The system does not go down even if some parts fail. 3.\u200b Partition Tolerance (P) \u2013 The database can continue to function even if network failures create partitions that temporarily prevent some nodes from communicating. -\u200b Even if messages between nodes are delayed or lost, the system remains operational. -\u200b Network partitions are inevitable in distributed systems, so databases must decide how to handle them. -\u200b Trade-Offs: Choosing Two Out of Three -\u200b Since it is impossible to achieve all three properties simultaneously, distributed databases must prioritize two based on their use case: 1.\u200b Consistency + Availability (CA) \u2192 No Partition Tolerance -\u200b The system always returns the most up-to-date data and never serves stale reads. -\u200b It remains available under normal conditions but cannot function correctly if the network is partitioned (i.e., when nodes cannot communicate). -\u200b If a partition occurs, the system may refuse requests or enter a failure mode until connectivity is restored."
    },
    "149": {
        "file": "Extended Notes - NoSQL Intro + KV DBs.pdf",
        "page": 5,
        "chunk": "cannot communicate). -\u200b If a partition occurs, the system may refuse requests or enter a failure mode until connectivity is restored."
    },
    "150": {
        "file": "Extended Notes - NoSQL Intro + KV DBs.pdf",
        "page": 6,
        "chunk": "-\u200b Example: Traditional relational databases (e.g., PostgreSQL, MySQL running on a single server). -\u200b A single-node database maintains consistency and availability but fails under network partitions. 2.\u200b Consistency + Partition Tolerance (CP) \u2192 Reduced Availability -\u200b The system always returns the most up-to-date data, even in the event of network failures. -\u200b However, if a partition occurs, some requests may be dropped or the system may become unavailable in order to maintain consistency. -\u200b This ensures data integrity at the cost of availability. -\u200b Example: HBase, Google Bigtable, MongoDB (with strong consistency settings) -\u200b Many banking systems prioritize CP because ensuring correct balances is more important than availability. 3.\u200b Availability + Partition Tolerance (AP) \u2192 Eventual Consistency -\u200b The system remains operational and responsive even when network partitions occur. -\u200b However, users may see slightly stale data due to eventual consistency. -\u200b Over time, updates propagate, and all nodes eventually reach a consistent state. -\u200b Example: DynamoDB, Cassandra, CouchDB, Riak -\u200b Content delivery networks (CDNs) prioritize availability and partition tolerance so that users always get content, even if it\u2019s not the absolute latest version. ACID Alternative for Distrib Systems - BASE -\u200b In distributed systems, strict ACID (Atomicity, Consistency, Isolation, Durability) guarantees can be difficult to maintain due to scalability constraints and the realities of network partitions (per the CAP theorem). To address this, many modern distributed databases adopt a more flexible approach known as BASE: -\u200b BASE is an alternative consistency model designed to prioritize availability and scalability over strict consistency. It consists of three key principles: 1.\u200b Basically Available: -\u200b The system guarantees availability (per the CAP theorem). -\u200b However, responses might indicate that data is incomplete, temporarily inconsistent, or even in a failure state due to ongoing updates. -\u200b The system \"works most of the time\", but"
    },
    "151": {
        "file": "Extended Notes - NoSQL Intro + KV DBs.pdf",
        "page": 6,
        "chunk": "strict consistency. It consists of three key principles: 1.\u200b Basically Available: -\u200b The system guarantees availability (per the CAP theorem). -\u200b However, responses might indicate that data is incomplete, temporarily inconsistent, or even in a failure state due to ongoing updates. -\u200b The system \"works most of the time\", but data accuracy is not always immediate. 2.\u200b Soft State: -\u200b The system's state can change over time, even without additional input. -\u200b This occurs due to eventual consistency mechanisms, such as background synchronization or replication processes. -\u200b Unlike ACID databases, BASE does not require immediate consistency across all replicas."
    },
    "152": {
        "file": "Extended Notes - NoSQL Intro + KV DBs.pdf",
        "page": 7,
        "chunk": "3.\u200b Eventual Consistency: -\u200b While data updates may not be immediately reflected across all nodes, the system guarantees that given enough time and no new updates, all replicas will converge to the same state. -\u200b All writes will eventually propagate across the distributed system, ensuring consistency in the long run. -\u200b This is a fundamental property of AP (Availability + Partition Tolerance) systems from the CAP theorem. -\u200b Examples of BASE-Oriented Databases -\u200b NoSQL Databases (e.g., Apache Cassandra, DynamoDB, Riak) -\u200b Key-Value Stores (e.g., Redis, Amazon S3) -\u200b Document Stores (e.g., MongoDB, CouchDB) -\u200b Eventually Consistent Storage Systems (e.g., Amazon SimpleDB, Cosmos DB) -\u200b When to Choose BASE over ACID - BASE is ideal for applications where: -\u200b High availability and scalability are more important than strict consistency -\u200b Data inconsistency is acceptable for short periods (e.g., social media feeds, recommendation systems) -\u200b The system can tolerate eventual consistency delays (e.g., analytics platforms, logging systems) -\u200b Horizontal scaling is needed to support massive workloads (e.g., global applications with distributed users) -\u200b Example: Imagine a social media platform like Twitter. If a user posts a tweet, their followers might not all see it immediately due to replication lag. However, within seconds or minutes, all servers will eventually update and reflect the post. This trade-off allows for high availability and global scalability, even if it means minor delays in data consistency. Key-Value Stores (3 Key Principles) -\u200b A key-value store is a non-relational database that follows a simple key = value structure. Each piece of data is stored as a unique key (identifier) paired with its corresponding value, making these databases highly efficient and scalable. 1.\u200b Simplicity -\u200b Minimalist Data Model: The key-value model is extremely simple compared to traditional relational databases (RDBMS), where data is stored in structured tables with predefined"
    },
    "153": {
        "file": "Extended Notes - NoSQL Intro + KV DBs.pdf",
        "page": 7,
        "chunk": "Each piece of data is stored as a unique key (identifier) paired with its corresponding value, making these databases highly efficient and scalable. 1.\u200b Simplicity -\u200b Minimalist Data Model: The key-value model is extremely simple compared to traditional relational databases (RDBMS), where data is stored in structured tables with predefined schemas. -\u200b Flat Data Structure: Unlike SQL databases that require tables, rows, and columns, key-value stores use a flat structure, making CRUD (Create, Read, Update, Delete) operations straightforward. -\u200b Schema-less: There are no strict rules on data format, allowing flexible data storage (e.g., JSON, XML, text, binary, etc.)."
    },
    "154": {
        "file": "Extended Notes - NoSQL Intro + KV DBs.pdf",
        "page": 8,
        "chunk": "-\u200b Ideal Use Cases: Caching (storing frequently accessed data for quick retrieval), Session Management (storing user sessions in web apps), Configuration Storage (storing app settings and feature flags) 2.\u200b Speed -\u200b Optimized for Fast Lookups: -\u200b Key-value stores use hash tables or similar data structures under the hood. -\u200b This allows them to retrieve a value by its key in O(1) time complexity \u2192 extremely fast. -\u200b Unlike relational databases that require complex indexing and query optimization, key-value stores provide instant access to values. -\u200b In-Memory Performance: -\u200b Many key-value stores (e.g., Redis, Memcached) run entirely in memory, making reads and writes blazing fast. -\u200b Eliminates the overhead of disk-based storage systems. -\u200b No Complex Queries or Joins: -\u200b Key-value stores do not support SQL-like queries, foreign keys, or joins. -\u200b Why? Because these operations slow down performance, making the system less efficient for its primary use cases. -\u200b If querying and relationships are needed, other NoSQL models (e.g., document stores or column-family stores) are a better fit. -\u200b Example: Redis Cache - Retrieving a cached webpage or API response from Redis can be 10\u2013100x faster than querying a relational database -\u200b Ideal Use Cases: Real-time applications (e.g., leaderboards, message queues), Caching API responses (e.g., storing computed results from expensive operations), Rate limiting (e.g., tracking API usage quotas) 3.\u200b Scalability -\u200b Designed for Horizontal Scaling: -\u200b Unlike relational databases, which scale vertically (adding more CPU/RAM to a single server), key-value stores scale horizontally (by distributing data across multiple nodes). -\u200b This makes them ideal for handling large-scale workloads with millions of concurrent users. -\u200b Eventual Consistency: -\u200b In a distributed key-value store, replicas of data exist across multiple servers. -\u200b The system guarantees eventual consistency, meaning that all nodes will converge to the same value over time, but not necessarily"
    },
    "155": {
        "file": "Extended Notes - NoSQL Intro + KV DBs.pdf",
        "page": 8,
        "chunk": "This makes them ideal for handling large-scale workloads with millions of concurrent users. -\u200b Eventual Consistency: -\u200b In a distributed key-value store, replicas of data exist across multiple servers. -\u200b The system guarantees eventual consistency, meaning that all nodes will converge to the same value over time, but not necessarily instantly. -\u200b Some systems allow users to trade off consistency for higher availability (per the CAP theorem) -\u200b Partitioning (Sharding) -\u200b Data can be easily partitioned across multiple servers by hashing the key (e.g., using consistent hashing)."
    },
    "156": {
        "file": "Extended Notes - NoSQL Intro + KV DBs.pdf",
        "page": 9,
        "chunk": "-\u200b This ensures that reads and writes remain fast and balanced across nodes. -\u200b Example: Amazon DynamoDB - Uses partitioning and replication to distribute billions of key-value pairs across a global infrastructure -\u200b Ideal Use Cases: Distributed caching layers, Global-scale applications, Internet of Things (IoT) data storage KV DS Use Cases - Data Science 1.\u200b EDA & Experimentation Results Store -\u200b Store intermediate results from data preprocessing, exploratory data analysis (EDA), or feature engineering. -\u200b Allows for quick lookups of previous experiment results without needing to recompute expensive transformations. -\u200b Useful for A/B testing: store experiment metadata and results without polluting the production database. -\u200b Benefits: Reduces computation time by caching results, Provides an easy way to resume interrupted experiments, Eliminates unnecessary writes to a structured RDBMS 2.\u200b Feature Store: -\u200b Store frequently accessed ML features for low-latency retrieval during model training and inference. -\u200b Supports real-time feature lookups, which is crucial for applications like fraud detection and recommendation systems. -\u200b Benefits: Faster access to features without recomputation, Reduces latency in model training & serving, Simplifies feature versioning 3.\u200b Model Monitoring & Performance Tracking -\u200b Store key performance metrics for deployed models. -\u200b Track real-time accuracy, precision, recall, drift detection, etc. -\u200b Enables real-time alerting if model performance degrades. -\u200b Enables real-time monitoring and alerting, Fast retrieval of historical model performance, Supports model version tracking KV DS Use Cases - Software Engineering 1.\u200b Storing Session Information -\u200b Everything about a user's current session (e.g., authentication, preferences, browsing history) can be stored in a single PUT operation and retrieved instantly. -\u200b Ideal for stateless web applications, where user sessions must persist across multiple requests. -\u200b Benefits: Fast, single-call retrieval, Reduces reliance on traditional session storage, Works across distributed systems 2.\u200b User Profiles & Preferences -\u200b Personalized user experiences can be enabled"
    },
    "157": {
        "file": "Extended Notes - NoSQL Intro + KV DBs.pdf",
        "page": 9,
        "chunk": "stored in a single PUT operation and retrieved instantly. -\u200b Ideal for stateless web applications, where user sessions must persist across multiple requests. -\u200b Benefits: Fast, single-call retrieval, Reduces reliance on traditional session storage, Works across distributed systems 2.\u200b User Profiles & Preferences -\u200b Personalized user experiences can be enabled with a single GET operation. -\u200b Stores UI preferences, language settings, themes, and notification settings. -\u200b Eliminates the need for complex joins in relational databases."
    },
    "158": {
        "file": "Extended Notes - NoSQL Intro + KV DBs.pdf",
        "page": 10,
        "chunk": "-\u200b Benefits: Fast lookup for user-specific settings, Simplifies UI personalization, Enables instant application of preferences across sessions 3.\u200b Shopping Cart Data -\u200b A shopping cart is tied to a user and must be accessible across browsers, devices, and sessions. -\u200b Key-value stores provide fast retrieval and eliminate session timeouts issues. -\u200b Benefits: Ensures shopping carts persist across devices, Improves checkout experience with faster load times, Eliminates need for complex SQL queries 4.\u200b Caching Layer (Speeding Up Database Queries) -\u200b Key-value stores like Redis and Memcached act as a caching layer in front of a disk-based database. -\u200b Frequently accessed queries can be stored for quick retrieval, reducing database load. -\u200b Benefits: Reduces load on primary database, Improves application response times, Supports scalability Redis DB (Remote Directory Server) -\u200b An open-source, in-memory database known for high speed and low latency. -\u200b Primarily a Key-Value (KV) Store, but supports multiple data models beyond key-value pairs. -\u200b Frequently used for caching, real-time analytics, and session management. Model Description Use Cases Key-Value Simple string-based KV store Caching, session storage Lists Ordered collections of strings Message queues, logs Sets Unordered collections with unique elements Tagging, leaderboards Sorted Sets Sets with a score for sorting Ranking systems, recommendation engines Hashes Key-value pairs inside a single key Storing user profiles, objects Bitmaps Efficiently store bits User activity tracking HyperLogLogs Approximate cardinality estimation Unique visitor counting Streams Time-ordered logs of events Event sourcing, real-time analytics Geospatial Store and query location-based data Location services, geofencing JSON Native JSON document store NoSQL-like document storage Vectors Store embeddings for similarity search AI, recommendation engines"
    },
    "159": {
        "file": "Extended Notes - NoSQL Intro + KV DBs.pdf",
        "page": 10,
        "chunk": "store NoSQL-like document storage Vectors Store embeddings for similarity search AI, recommendation engines"
    },
    "160": {
        "file": "Extended Notes - NoSQL Intro + KV DBs.pdf",
        "page": 11,
        "chunk": "Redis Performance & Ranking Redis consistently ranks among the top key-value stores due to: -\u200b Ultra-low latency (sub-millisecond response times), High throughput (millions of operations per second), Support for horizontal scaling and clustering, Atomic operations for data consistency, Replication and persistence for reliability, Ranking of KV Stores (DB-Engines.com, March 2025) -\u200b Redis. Amazon DynamoDB, etcd, RocksDB, Memcached Redis -\u200b In-Memory Database \u2013 Primarily operates in RAM for ultra-fast data access. -\u200b Durability Options: 1.\u200b Snapshotting (RDB) \u2013 Saves a snapshot of the dataset to disk at specified intervals. 2.\u200b Append-Only File (AOF) \u2013 Logs every write operation to disk, allowing roll-forward recovery in case of failure. -\u200b Developed in 2009 \u2013 Written in C++, designed for speed and efficiency. -\u200b High Performance \u2013 Can handle 100,000+ SET operations per second. -\u200b Rich Command Set \u2013 Supports various operations beyond simple key-value storage. -\u200b Limitations: -\u200b No Complex Queries \u2013 Does not support SQL-like querying. -\u200b No Secondary Indexes \u2013 Data can only be accessed by its primary key. Redis Data Types Keys: -\u200b Typically strings, but can be any binary sequence (e.g., numbers, encoded objects, or raw bytes). -\u200b Keys should be short and meaningful to optimize memory usage and retrieval speed. -\u200b Naming convention best practices: -\u200b Use colons (:) as namespace separators (e.g., user:1001:profile). -\u200b Keep key lengths small to improve lookup performance. Values: Data Type Description Use Cases Strings Basic key-value storage (binary-safe, up to 512MB) Caching, counters, session storage Lists Linked lists with fast push/pop operations Message queues, task scheduling Sets Unordered collection of unique elements Tagging, leaderboards, social networks"
    },
    "161": {
        "file": "Extended Notes - NoSQL Intro + KV DBs.pdf",
        "page": 11,
        "chunk": "Message queues, task scheduling Sets Unordered collection of unique elements Tagging, leaderboards, social networks"
    },
    "162": {
        "file": "Extended Notes - NoSQL Intro + KV DBs.pdf",
        "page": 12,
        "chunk": "Sorted Sets Similar to sets, but each element has a score for sorting Ranking systems, recommendation engines Hashes Key-value pairs inside a single key User profiles, object storage Geospatial Data Stores latitude/longitude coordinates with radius queries Location-based services, geofencing Redis Data Type Breakdown 1.\u200b Strings -\u200b The most basic data type in Redis. -\u200b Supports string manipulation, bitwise operations, and numeric operations. 2.\u200b Lists (Linked Lists) -\u200b Ordered collection of string elements, allowing fast insertions/removals from both ends. -\u200b Used for queues, logs, and messaging systems. 3.\u200b Sets -\u200b Unordered collection of unique elements (no duplicates). -\u200b Useful for tagging, leaderboards, and social media followers/following. 4.\u200b Sorted Sets (ZSets) -\u200b Similar to Sets, but each element has a numeric score that determines sorting order. -\u200b Used for ranking systems, priority queues, and real-time leaderboards. 5.\u200b Hashes -\u200b Key-value pairs stored within a single key, reducing memory overhead. -\u200b Great for storing user profiles, objects, and configurations. 6.\u200b Geospatial Data -\u200b Stores latitude/longitude coordinates, enabling location-based queries. -\u200b Used in geofencing, ride-sharing apps, and proximity searches. Why Use Redis Data Types? -\u200b Optimized for speed \u2013 Most operations are O(1) or O(log N). -\u200b Memory-efficient \u2013 Compact storage for large-scale applications. -\u200b Versatile \u2013 Supports multiple models beyond just key-value storage."
    },
    "163": {
        "file": "Extended Notes - Introduction to the  Graph Data Model.pdf",
        "page": 0,
        "chunk": "What is a Graph Database? A Graph Database is a type of NoSQL database designed to store and navigate relationships between data in the form of a graph. The core structure of a graph database is based on the graph theory and consists of nodes, edges, and properties. Key Components of a Graph Database 1.\u200b Nodes -\u200b Nodes represent entities or objects in the database, such as people, places, or things. -\u200b Each node is uniquely identified by an ID and can contain various properties (key-value pairs). For example: -\u200b Person node with properties like name, age, occupation. -\u200b City node with properties like name, population, area 2.\u200b Edges -\u200b Edges represent relationships between nodes. -\u200b Just like nodes, edges are also uniquely identified and can contain properties. -\u200b Directed edges indicate the direction of the relationship (e.g., \"is friends with\" or \"works at\"). -\u200b Undirected edges can represent bi-directional relationships (e.g., \"related to\" or \"connected to\"). 3.\u200b Properties -\u200b Both nodes and edges can have properties that provide additional information about the entity or relationship. -\u200b For instance, a \"friendship\" edge between two people might have a property like \"since\", indicating when the friendship began. Graph Databases & Their Querying Model Graph databases support queries that operate directly on the relationships between nodes rather than relying on joins, which are common in relational databases. The graph structure enables operations like: 1.\u200b Traversals -\u200b Traversal is the process of visiting nodes and edges in the graph, starting from a particular node and exploring its neighbors. -\u200b Traversals allow you to find connections between entities by following edges. -\u200b Example: Find all friends of a person (Alice) by following the \"is friends with\" edges from Alice\u2019s node. 2.\u200b Shortest Path -\u200b Shortest path queries identify the smallest number of edges between"
    },
    "164": {
        "file": "Extended Notes - Introduction to the  Graph Data Model.pdf",
        "page": 0,
        "chunk": "node and exploring its neighbors. -\u200b Traversals allow you to find connections between entities by following edges. -\u200b Example: Find all friends of a person (Alice) by following the \"is friends with\" edges from Alice\u2019s node. 2.\u200b Shortest Path -\u200b Shortest path queries identify the smallest number of edges between two nodes. -\u200b This is useful for things like finding the shortest route in a map or recommending connections in a social network. -\u200b Example: Find the shortest path from Alice to Bob through a mutual friend. 3.\u200b Pattern Matching"
    },
    "165": {
        "file": "Extended Notes - Introduction to the  Graph Data Model.pdf",
        "page": 1,
        "chunk": "-\u200b Query language in graph databases (e.g., Cypher in Neo4j) supports pattern matching, where you can define a pattern of nodes and relationships to search for. -\u200b Example: Find all people who are connected to a particular person through a series of relationships. 4.\u200b Graph Algorithms -\u200b Many graph databases come with built-in graph algorithms that help analyze relationships between nodes, such as: -\u200b Centrality: Which nodes are most important or central in the graph. -\u200b PageRank: Similar to Google's algorithm for ranking web pages based on link structure. -\u200b Community detection: Identifying groups of tightly connected nodes. Advantages of Graph Databases -\u200b Efficient Relationship Handling: Graph databases excel at handling complex relationships between entities, which can be cumbersome for relational databases. -\u200b Flexible Data Model: The graph model is highly flexible, allowing you to easily evolve the structure without needing major schema changes. -\u200b Performance: Graph databases are optimized for complex relationship queries and can outperform relational databases when querying highly connected data. -\u200b Intuitive Representation: The graph structure provides a more natural representation of relationships, making it easier to model real-world problems like social networks, recommendation systems, fraud detection, etc. Use Cases for Graph Databases 1.\u200b Social Networks -\u200b Represent relationships between users, such as friends, followers, or connections -\u200b Query for things like mutual friends, friend recommendations, or the shortest path between two users. 2.\u200b Recommendation Engines -\u200b In e-commerce, graph databases can model customer-product interactions, and make personalized recommendations based on user behavior or preferences. 3.\u200b Fraud Detection -\u200b Detect fraud by analyzing suspicious patterns in transactions and relationships between different entities (e.g., customers, accounts, merchants). 4.\u200b Network Topology -\u200b Model and analyze networks such as computer networks, supply chains, or transportation systems, where connections and routes are vital. Graph Databases vs. Relational Databases -\u200b Relational"
    },
    "166": {
        "file": "Extended Notes - Introduction to the  Graph Data Model.pdf",
        "page": 1,
        "chunk": "Fraud Detection -\u200b Detect fraud by analyzing suspicious patterns in transactions and relationships between different entities (e.g., customers, accounts, merchants). 4.\u200b Network Topology -\u200b Model and analyze networks such as computer networks, supply chains, or transportation systems, where connections and routes are vital. Graph Databases vs. Relational Databases -\u200b Relational Databases (RDBMS): Data is stored in tables with rows and columns. Operations like joins are used to connect related data, but as the complexity of relationships increases, these queries can become slow and difficult to manage."
    },
    "167": {
        "file": "Extended Notes - Introduction to the  Graph Data Model.pdf",
        "page": 2,
        "chunk": "-\u200b Graph Databases: Instead of using joins, graph databases store relationships as first-class citizens, making relationship-based queries much more efficient. Complex relationships and traversals become far simpler. Popular Graph Databases 1.\u200b Neo4j \u2013 One of the most popular graph databases. Uses the Cypher query language and is widely used in social networks, recommendation engines, and fraud detection. 2.\u200b Amazon Neptune \u2013 A fully managed graph database by AWS, supports both Property Graph and RDF graph models. 3.\u200b ArangoDB \u2013 A multi-model database that supports document, key-value, and graph data models. 4.\u200b OrientDB \u2013 A multi-model graph database with support for both graph and document models. Where do Graphs Show up? Graphs are a versatile data structure that naturally models relationships and connections in many real-world systems. They appear in a wide variety of domains where relationships or interactions between entities need to be represented. Below are some key areas where graphs are frequently used: 1.\u200b Social Networks -\u200b Social Networks like Facebook, Instagram, LinkedIn, and Twitter are classic examples of graph databases in action. -\u200b Nodes: Represent individuals or accounts. -\u200b Edges: Represent relationships or interactions such as friendships, follows, likes, or comments. -\u200b Graphs help answer questions like: -\u200b Who are my mutual friends with? -\u200b What is the shortest path between two users? -\u200b How can we recommend new connections? -\u200b Beyond just online social networks, social interactions are also analyzed in fields like psychology and sociology. In these contexts, graphs are used to model: -\u200b Group dynamics: How individuals interact within a group. -\u200b Influence propagation: How information or behaviors spread within communities. -\u200b Social structure analysis: Identifying key individuals (central nodes) in a social system. 2.\u200b The Web -\u200b The World Wide Web is essentially a massive graph, with web pages as nodes and hyperlinks as"
    },
    "168": {
        "file": "Extended Notes - Introduction to the  Graph Data Model.pdf",
        "page": 2,
        "chunk": "How individuals interact within a group. -\u200b Influence propagation: How information or behaviors spread within communities. -\u200b Social structure analysis: Identifying key individuals (central nodes) in a social system. 2.\u200b The Web -\u200b The World Wide Web is essentially a massive graph, with web pages as nodes and hyperlinks as edges. -\u200b Nodes: Web pages, blogs, documents, or other content. -\u200b Edges: Hyperlinks connecting one page to another, creating a network of linked information."
    },
    "169": {
        "file": "Extended Notes - Introduction to the  Graph Data Model.pdf",
        "page": 3,
        "chunk": "-\u200b Applications: -\u200b PageRank: Google's algorithm uses graph theory to rank web pages based on the links between them, determining how important or relevant a page is in relation to a search query. -\u200b Link Analysis: Graphs can help discover relationships between web pages and identify patterns in how they are linked. 3.\u200b Chemical and Biological Data -\u200b Graphs are also crucial in fields like chemistry, biochemistry, systems biology, and genetics, where the interactions between various entities can be represented in graph form: 1.\u200b Chemical Interactions -\u200b Molecules can be represented as nodes, and the chemical bonds between atoms (or between molecules) can be represented as edges. -\u200b Applications: Molecular structure analysis: Identifying key structural features, analyzing reaction pathways, and drug design. -\u200b Chemical reaction networks: Understanding how different chemicals interact and react with one another in a reaction pathway. 2.\u200b Biological Networks -\u200b Genes, proteins, and metabolites can be represented as nodes, with edges representing interactions like protein-protein interactions, gene regulatory networks, or metabolic pathways. -\u200b Applications: Gene networks: Mapping gene relationships and pathways to understand gene expression regulation. -\u200b Protein interaction networks: Studying how proteins work together to perform cellular functions. -\u200b Disease modeling: Identifying critical proteins or genes that may be related to diseases, or predicting disease outcomes. -\u200b Genomics: Graphs are used to represent and analyze relationships in genomic data, such as the interactions between different genes or gene networks that influence biological processes. Graph databases are often used to model phylogenetic trees and the evolutionary relationships between species. 4.\u200b Logistics and Supply Chains -\u200b Graphs are ideal for modeling logistics, transportation, and supply chains: -\u200b Nodes: Represent locations, warehouses, or transport hubs. -\u200b Edges: Represent routes, shipments, or paths between locations. -\u200b Applications: Optimizing routes: Finding the shortest or fastest paths for shipments or deliveries."
    },
    "170": {
        "file": "Extended Notes - Introduction to the  Graph Data Model.pdf",
        "page": 3,
        "chunk": "relationships between species. 4.\u200b Logistics and Supply Chains -\u200b Graphs are ideal for modeling logistics, transportation, and supply chains: -\u200b Nodes: Represent locations, warehouses, or transport hubs. -\u200b Edges: Represent routes, shipments, or paths between locations. -\u200b Applications: Optimizing routes: Finding the shortest or fastest paths for shipments or deliveries. -\u200b Supply chain analysis: Identifying key suppliers, customers, or bottlenecks in the supply chain."
    },
    "171": {
        "file": "Extended Notes - Introduction to the  Graph Data Model.pdf",
        "page": 4,
        "chunk": "5.\u200b Fraud Detection -\u200b In financial services or e-commerce, fraud detection systems often rely on graphs to analyze relationships between accounts, transactions, or devices: -\u200b Nodes: Represent accounts, customers, or devices. -\u200b Edges: Represent transactions, connections, or interactions. -\u200b Graphs are useful for: Identifying fraudulent connections or suspicious activity patterns. -\u200b Anomaly detection: Spotting unusual behaviors that could indicate fraud, such as a user creating multiple accounts or making fraudulent transactions. 6.\u200b Telecommunications -\u200b Telecommunications networks can be represented as graphs, with: -\u200b Nodes: Representing phones, routers, or base stations. -\u200b Edges: Representing calls, data connections, or routes in a network. -\u200b Applications: -\u200b Optimizing the layout of communication infrastructure. -\u200b Detecting faults or inefficiencies in the network. -\u200b Predicting how communication flows and interactions occur. 7.\u200b Knowledge Graphs -\u200b Knowledge Graphs use graphs to represent the relationships between entities in a domain (such as people, places, events, and concepts) and are commonly used in search engines, recommendation systems, and AI. -\u200b Nodes: Represent entities (e.g., books, movies, authors, topics). -\u200b Edges: Represent relationships (e.g., \"written by\", \"related to\", \"directed by\"). -\u200b Applications: -\u200b Search Engines: Enhancing search results by showing related entities and their relationships. -\u200b Recommendation Systems: Recommending items based on connections between entities (e.g., books related to a specific author or genre). 8.\u200b Network Security -\u200b Network security uses graphs to model relationships between users, devices, and systems within a network. -\u200b Nodes: Represent devices, users, or systems. -\u200b Edges: Represent the flow of data or access permissions. -\u200b Applications: Identifying vulnerabilities or security breaches in the network. -\u200b Analyzing how attackers might move across a network (e.g., lateral movement in a compromised system)."
    },
    "172": {
        "file": "Extended Notes - Introduction to the  Graph Data Model.pdf",
        "page": 4,
        "chunk": "permissions. -\u200b Applications: Identifying vulnerabilities or security breaches in the network. -\u200b Analyzing how attackers might move across a network (e.g., lateral movement in a compromised system)."
    },
    "173": {
        "file": "Extended Notes - Introduction to the  Graph Data Model.pdf",
        "page": 5,
        "chunk": "What is a graph? A graph is a data structure that represents relationships between entities. It is composed of nodes (also called vertices) and edges (also called relationships). These elements form the core structure of a graph, allowing for the representation of connected data and enabling the modeling of complex relationships between entities. Labeled Property Graph - Breakdown of the components: A Labeled Property Graph is a more specific and advanced type of graph structure, where nodes and edges can have both labels and properties. It is one of the most commonly used graph representations, especially in graph databases like Neo4j. 1.\u200b Nodes (Vertices) -\u200b Definition: A node represents an entity or object in the graph. Each node can represent anything, such as a person, place, product, event, etc. -\u200b Labels: A label is a way to categorize or group nodes. Labels allow nodes to be categorized by types or categories, such as Person, Product, or City. -\u200b Properties: Nodes can have attributes that describe them. These attributes are stored as key-value pairs (think of them like columns in a database). For example, a Person node might have properties like name, age, and city. -\u200b Example: Node: A Person node with properties like {name: \"Alice\", age: 30, city: \"New York\"}; Label: Person; Properties: {name: \"Alice\", age: 30, city: \"New York\"} 2.\u200b Edges (Relationships) -\u200b Definition: An edge represents a relationship between two nodes. It connects two nodes, establishing a link or connection between them. -\u200b Labels: Relationships can also have labels that describe the type of relationship between the nodes, such as \"FRIEND_OF\", \"WORKS_AT\", or \"LIKES\". -\u200b Properties: Just like nodes, edges can also have properties. These properties describe the relationship, and they are stored as key-value pairs. For example, a relationship between two people may have a property like"
    },
    "174": {
        "file": "Extended Notes - Introduction to the  Graph Data Model.pdf",
        "page": 5,
        "chunk": "labels that describe the type of relationship between the nodes, such as \"FRIEND_OF\", \"WORKS_AT\", or \"LIKES\". -\u200b Properties: Just like nodes, edges can also have properties. These properties describe the relationship, and they are stored as key-value pairs. For example, a relationship between two people may have a property like since to represent when the relationship started. -\u200b Example: Edge: A \"FRIEND_OF\" relationship between Alice and Bob, with properties like {since: 2015}. Label: FRIEND_OF Properties: {since: 2015} 3.\u200b Labels -\u200b Labels are used to group nodes into categories or types. They allow us to organize and categorize nodes in the graph. -\u200b Nodes: A node can have one or more labels (e.g., Person, City, Product). -\u200b Example: A person node might have the label Person, and a product node might have the label Product. -\u200b Note: Labels are not exclusive to a node, meaning a node could be labeled as Person and Employee simultaneously if it fits multiple categories. 4.\u200b Properties -\u200b Definition: Properties are key-value pairs that can be attached to both nodes and edges. They are used to store additional information about the node or the relationship."
    },
    "175": {
        "file": "Extended Notes - Introduction to the  Graph Data Model.pdf",
        "page": 6,
        "chunk": "-\u200b Node Properties: These define characteristics of nodes (e.g., age, city, name for a Person node). -\u200b Edge Properties: These describe characteristics of the relationship (e.g., since, strength for a friendship edge). -\u200b Example: Node: {name: \"Alice\", age: 30, city: \"New York\"} (Person node). Relationship: [:FRIEND_OF {since: 2015}] (Friendship edge). 5.\u200b Key Properties -\u200b Nodes without relationships: It is perfectly valid for a node to not have any relationships. This can happen in scenarios where a node exists independently, such as a person who has no current friendships or a product that hasn't been sold yet. -\u200b Edges not connected to nodes: This is not allowed. Edges must always connect to nodes, as they are used to represent relationships. Having edges that don\u2019t connect to nodes doesn\u2019t make sense in a graph structure and would violate the graph model. Key Characteristics of a Labeled Property Graph 1.\u200b Flexible Schema: Unlike relational databases, labeled property graphs allow for flexible schemas where each node and relationship can have different properties. This allows the graph to evolve over time without rigid schema constraints. 2.\u200b Intuitive Representation: Labeled property graphs are great for modeling complex and interconnected data, like social networks, recommendation systems, and fraud detection, where entities and their relationships are key to understanding the data. 3.\u200b Efficient Traversal: With the use of nodes and edges, graph databases are optimized for traversing relationships, making operations like finding the shortest path, recommendations, and influence propagation very efficient. Paths A path in a graph is a sequence of nodes connected by edges, where the nodes and edges in the path are visited only once. Paths are a fundamental concept in graph theory, and they are used to represent a variety of relationships and traversals within the graph. Key Characteristics of a Path 1.\u200b Ordered Sequence:"
    },
    "176": {
        "file": "Extended Notes - Introduction to the  Graph Data Model.pdf",
        "page": 6,
        "chunk": "a sequence of nodes connected by edges, where the nodes and edges in the path are visited only once. Paths are a fundamental concept in graph theory, and they are used to represent a variety of relationships and traversals within the graph. Key Characteristics of a Path 1.\u200b Ordered Sequence: The sequence of nodes in a path is ordered, meaning the order in which nodes appear is important. A path represents a specific traversal from one node to another. 2.\u200b Nodes and Edges: A path is formed by connecting nodes with edges. The edges are the links between the nodes, and they dictate the traversal of the graph. 3.\u200b No Repeated Nodes or Edges: A simple path ensures that no node or edge is revisited, which means each node and edge can only appear once in the path. This helps avoid loops or cycles in the traversal. -\u200b Simple Path: A path that doesn\u2019t revisit any node or edge. -\u200b Non-simple Path: If nodes or edges are repeated, it\u2019s not considered a simple path."
    },
    "177": {
        "file": "Extended Notes - Introduction to the  Graph Data Model.pdf",
        "page": 7,
        "chunk": "4.\u200b Directed vs. Undirected: In a directed graph, edges have a direction, so the path must follow the direction of the edges. In an undirected graph, the path can go in either direction along the edges. Types of Paths 1.\u200b Simple Path: As mentioned, a simple path doesn\u2019t revisit any nodes or edges. For example: Path: A \u2192 B \u2192 C \u2192 D -\u200b Here, each node is visited only once, and no edge is repeated. 2.\u200b Cycle: A cycle is a special kind of path where the start and end node are the same, and the path forms a loop. In a cycle, nodes (and sometimes edges) can be revisited. -\u200b Example: A \u2192 B \u2192 C \u2192 A (this is a cycle, since it starts and ends at A). 3.\u200b Shortest Path: The shortest path is the path that has the fewest edges (or minimum distance in weighted graphs). It\u2019s often a key concept in routing algorithms. -\u200b Example: Finding the quickest route between two cities. 4.\u200b Longest Path: Conversely, the longest path is the path that takes the greatest number of edges. This can be useful in certain algorithms, especially when measuring network latency or other factors. Applications of Paths in Graphs 1.\u200b Finding Shortest Paths: Algorithms like Dijkstra's or Bellman-Ford are used to find the shortest path between two nodes in a weighted graph. Paths can be used in network routing, GPS systems, and social network analysis. 2.\u200b Traversal: In graph traversal algorithms like Depth-First Search (DFS) or Breadth-First Search (BFS), paths are followed to explore the graph. DFS explores as far as possible along a branch, while BFS explores all neighbors at the present depth level before moving on. 3.\u200b Social Network Analysis: In social networks, paths can represent relationships between users. For example, a"
    },
    "178": {
        "file": "Extended Notes - Introduction to the  Graph Data Model.pdf",
        "page": 7,
        "chunk": "(DFS) or Breadth-First Search (BFS), paths are followed to explore the graph. DFS explores as far as possible along a branch, while BFS explores all neighbors at the present depth level before moving on. 3.\u200b Social Network Analysis: In social networks, paths can represent relationships between users. For example, a path could represent the \"connection\" between two individuals, and traversing the graph could help identify friends of friends or shortest paths between users. 4.\u200b Web Crawling: In the context of the web (as a graph of pages), paths can represent how one page links to another. A web crawler follows paths (hyperlinks) from one page to another in order to index the content of the web. 5.\u200b Recommendation Systems: In recommendation systems, paths can represent connections between users and products. For example, if user A liked product X, and user B liked product X, then the system may suggest product Y to user B if user A has also liked product Y. Path Length The length of a path is measured by the number of edges traversed. In the example above, the path from A to D has a length of 3, as it involves 3 edges."
    },
    "179": {
        "file": "Extended Notes - Introduction to the  Graph Data Model.pdf",
        "page": 8,
        "chunk": "Flavors of Graph Graphs can have various characteristics depending on how nodes and edges are defined and how they interact with each other. The following are common flavors of graphs, which differ based on key attributes like connectivity, edge weights, direction, and cycles. 1.\u200b Connected vs. Disconnected Graphs -\u200b Connected Graph: A graph is connected if there is a path between every pair of nodes. In other words, you can reach any node from any other node in the graph by traversing through edges. -\u200b Example: A social network graph where everyone is connected either directly or indirectly through friends. -\u200b Disconnected Graph: A graph is disconnected if there is at least one pair of nodes in the graph that is not connected by any path. A disconnected graph can be thought of as consisting of two or more isolated subgraphs. -\u200b Example: A graph representing different cities where some cities are completely disconnected from others. 2.\u200b Weighted vs. Unweighted Graphs -\u200b Weighted Graph: A graph is weighted if each edge has an associated weight or cost. This weight could represent various things like distance, time, cost, or any other quantity that can be measured. Weighted graphs are essential in algorithms that need to account for the cost of traversing between nodes, such as shortest path algorithms (e.g., Dijkstra's algorithm). -\u200b Example: A road network graph where each edge (road) has a weight representing the travel time or distance between two locations. -\u200b Unweighted Graph: A graph is unweighted if the edges do not carry any weight. All edges are treated equally in terms of cost or distance. Unweighted graphs are simpler to process and are often used when the concern is just whether a path exists between two nodes, not the cost of traveling that path. -\u200b Example: A"
    },
    "180": {
        "file": "Extended Notes - Introduction to the  Graph Data Model.pdf",
        "page": 8,
        "chunk": "the edges do not carry any weight. All edges are treated equally in terms of cost or distance. Unweighted graphs are simpler to process and are often used when the concern is just whether a path exists between two nodes, not the cost of traveling that path. -\u200b Example: A social network graph where connections between users (edges) are not weighted by any specific metric. 3.\u200b Directed vs. Undirected Graphs -\u200b Directed Graph (Digraph): A graph is directed if the edges have a direction, meaning each edge goes from a start node to an end node. Directed graphs are used when the relationship between nodes is asymmetric, i.e., the relationship flows in one direction. -\u200b Example: A Twitter-following graph, where a directed edge indicates that one user follows another but not necessarily the reverse. -\u200b Undirected Graph: A graph is undirected if the edges have no direction. The edges are bidirectional, meaning if there is an edge between node A and node B, you can traverse it in both directions. -\u200b Example: A Facebook friend network, where if A is friends with B, then B is also friends with A."
    },
    "181": {
        "file": "Extended Notes - Introduction to the  Graph Data Model.pdf",
        "page": 9,
        "chunk": "4.\u200b Acyclic vs. Cyclic Graphs -\u200b Acyclic Graph: A graph is acyclic if it contains no cycles. A cycle is a path in the graph where the start and end nodes are the same, and the path does not repeat any nodes or edges. Acyclic graphs are crucial for representing hierarchical or tree-like structures where there is no looping or circular reference. -\u200b Example: A Tree is an acyclic graph because it has no cycles. Another example is a Task Dependency Graph, where tasks must be completed in a specific order and cannot loop back on themselves. -\u200b Cyclic Graph: A graph is cyclic if there is at least one cycle in the graph. Cyclic graphs can model situations where entities are interconnected in a way that creates loops. Cyclic graphs are useful for representing systems where feedback loops or circular dependencies exist. -\u200b Example: A web page link structure, where one page links to another, which links back to the first page, forming a cycle. Real-World Applications of Graph Flavors 1.\u200b Connected Graphs: Used in social networking applications, where every user should be able to eventually connect to every other user (directly or indirectly). 2.\u200b Weighted Graphs: Essential for applications like navigation systems or telecommunication networks, where paths have different costs, and finding the most efficient path is important. 3.\u200b Directed Graphs: Useful in systems like recommendation engines, email systems, or web crawling, where relationships have clear directional flow (e.g., one entity influences another). 4.\u200b Acyclic Graphs: Found in project planning (where tasks must be completed in a specific order), and file system structures (directories and subdirectories). 5.\u200b Cyclic Graphs: Present in systems involving feedback loops, resource allocation, and recurrent processes like neural networks or certain types of dynamic systems. Types of Graph Algorithms - Pathfinding Pathfinding algorithms are"
    },
    "182": {
        "file": "Extended Notes - Introduction to the  Graph Data Model.pdf",
        "page": 9,
        "chunk": "project planning (where tasks must be completed in a specific order), and file system structures (directories and subdirectories). 5.\u200b Cyclic Graphs: Present in systems involving feedback loops, resource allocation, and recurrent processes like neural networks or certain types of dynamic systems. Types of Graph Algorithms - Pathfinding Pathfinding algorithms are fundamental to working with graphs, especially when you need to find the shortest path between nodes, detect cycles, or optimize flow through networks. Let's break down the key concepts of pathfinding and explore the various types of pathfinding algorithms: 1.\u200b Shortest Path Algorithms -\u200b The shortest path problem involves finding the path between two nodes that minimizes a specific criterion (typically the number of edges or the total weight of edges). This is one of the most common graph operations used in various applications such as navigation, routing, and network analysis. -\u200b Shortest Path: This can refer to either: -\u200b Fewest edges: The path with the minimum number of hops (unweighted graph)."
    },
    "183": {
        "file": "Extended Notes - Introduction to the  Graph Data Model.pdf",
        "page": 10,
        "chunk": "-\u200b Lowest weight: The path where the sum of the edge weights is minimized (weighted graph). -\u200b Common Shortest Path Algorithms: -\u200b Dijkstra\u2019s Algorithm: Dijkstra\u2019s algorithm finds the shortest path from a source node to all other nodes in a weighted graph. It works by maintaining a set of unvisited nodes and progressively expanding the shortest paths from the source. -\u200b Use case: Efficient for finding the shortest path in a weighted, directed graph where edges have non-negative weights (e.g., GPS navigation systems). -\u200b Bellman-Ford Algorithm: This algorithm is similar to Dijkstra\u2019s but can handle graphs with negative edge weights and can also detect negative weight cycles. -\u200b Use case: Used in applications like currency exchange or financial systems, where negative weights might be involved. -\u200b A (A-star) Algorithm*: A* is an informed search algorithm that combines aspects of Dijkstra\u2019s algorithm with heuristics to find the shortest path more efficiently. It uses an estimate of the remaining cost (heuristic) to prioritize which node to explore next. -\u200b Use case: Commonly used in gaming, AI for pathfinding, and GPS navigation systems. -\u200b Floyd-Warshall Algorithm: This is an all-pairs shortest path algorithm, meaning it finds the shortest paths between all pairs of nodes in a graph. It\u2019s particularly useful when you need to know the shortest path between any two nodes in a graph. -\u200b Use case: Used in network optimization problems, where you need to compute paths between all nodes. 2.\u200b Average Shortest Path (Efficiency and Resiliency) -\u200b The Average Shortest Path is a metric used to monitor the efficiency and resiliency of networks. It is the average of the shortest paths between all pairs of nodes in a graph. -\u200b Resiliency: A network with a lower average shortest path suggests that there is better connectivity and fewer isolated nodes or"
    },
    "184": {
        "file": "Extended Notes - Introduction to the  Graph Data Model.pdf",
        "page": 10,
        "chunk": "Path is a metric used to monitor the efficiency and resiliency of networks. It is the average of the shortest paths between all pairs of nodes in a graph. -\u200b Resiliency: A network with a lower average shortest path suggests that there is better connectivity and fewer isolated nodes or subgraphs, making it more resilient to failures. -\u200b Efficiency: A low average shortest path indicates that it\u2019s easy to get from one point to another in the network, implying efficient communication or transportation. -\u200b Use case: This metric is helpful in network optimization, where improving the average shortest path can lead to better performance in systems like communication networks, transportation systems, or social networks."
    },
    "185": {
        "file": "Extended Notes - Introduction to the  Graph Data Model.pdf",
        "page": 11,
        "chunk": "3.\u200b Minimum Spanning Tree (MST) -\u200b A Minimum Spanning Tree (MST) is a spanning tree of a weighted graph where the total edge weight is minimized. It connects all nodes in the graph with the minimum total weight and without any cycles. MST is useful for designing cost-efficient networks. -\u200b Common MST Algorithms: -\u200b Kruskal\u2019s Algorithm: Kruskal\u2019s algorithm works by sorting all edges in the graph by weight and adding edges to the tree in increasing order of weight, ensuring no cycles are formed. -\u200b Use case: Building minimum cost networks like telephone lines, electrical grids, or road networks. -\u200b Prim\u2019s Algorithm: Prim\u2019s algorithm starts with a single node and grows the MST by adding the least expensive edge connecting a node in the tree to a node outside it, until all nodes are included. -\u200b Use case: Used in network design, where it\u2019s important to connect all nodes at the minimum cost. 4.\u200b Cycle Detection -\u200b Cycle Detection is an important pathfinding task to identify whether a graph contains any cycles. Cycles can be problematic, especially in systems like task scheduling, where cycles represent circular dependencies that can prevent completion. -\u200b Cycle Detection Algorithms: -\u200b Depth-First Search (DFS): DFS can be used to detect cycles by keeping track of the nodes that are currently in the recursion stack. If you encounter a node that\u2019s already in the recursion stack, a cycle has been detected. -\u200b Use case: Important in dependency resolution (e.g., package managers, task schedulers), or network protocols to avoid infinite loops. -\u200b Union-Find (Disjoint Set Union): This algorithm can detect cycles in an undirected graph. It works by keeping track of the connected components and checking if adding an edge creates a cycle by connecting two previously disconnected components. -\u200b Use case: Efficient for detecting cycles in"
    },
    "186": {
        "file": "Extended Notes - Introduction to the  Graph Data Model.pdf",
        "page": 11,
        "chunk": "to avoid infinite loops. -\u200b Union-Find (Disjoint Set Union): This algorithm can detect cycles in an undirected graph. It works by keeping track of the connected components and checking if adding an edge creates a cycle by connecting two previously disconnected components. -\u200b Use case: Efficient for detecting cycles in network connectivity problems. 5.\u200b Maximum/Minimum Flow Algorithms -\u200b Flow algorithms are used to determine the maximum flow of material, data, or other resources through a network. These algorithms are commonly used in network design and transportation to model how resources can be efficiently routed through a system. -\u200b Common Flow Algorithms:"
    },
    "187": {
        "file": "Extended Notes - Introduction to the  Graph Data Model.pdf",
        "page": 12,
        "chunk": "-\u200b Ford-Fulkerson Algorithm: This algorithm finds the maximum flow in a flow network by repeatedly augmenting the flow along paths from the source to the sink until no more augmenting paths can be found. -\u200b Use case: Used in traffic flow optimization, network throughput maximization, and supply chain management. -\u200b Edmonds-Karp Algorithm: A specific implementation of Ford-Fulkerson that uses Breadth-First Search (BFS) to find augmenting paths. It guarantees polynomial time complexity and is used in scenarios that require maximum flow calculations. -\u200b Use case: Common in data packet routing and resource allocation. Other Pathfinding Variations -\u200b Maximal Paths: Algorithms that identify the longest paths or paths with the largest capacity, often used in optimization problems. -\u200b All-Pairs Shortest Path: Algorithms like Floyd-Warshall that calculate the shortest path between every pair of nodes in a graph. Types of Graph Algorithms - Centrality & Community Detection Graph algorithms related to centrality and community detection focus on understanding the structure and behavior of nodes within a graph. These algorithms help in identifying important nodes (centrality) and detecting groups of nodes that are strongly connected (community detection). These concepts are essential for applications like social networks, biological networks, transportation systems, and more. 1.\u200b Centrality Algorithms: Centrality measures how important or influential a node is within a graph. Nodes with higher centrality have more influence, connections, or \"importance\" relative to other nodes. Centrality measures are widely used in social networks, recommendation systems, and even in understanding traffic flow or information propagation. -\u200b Degree Centrality: The degree centrality of a node is simply the number of edges connected to it. In undirected graphs, it is the number of neighbors (connections), while in directed graphs, it can be divided into in-degree (number of incoming edges) and out-degree (number of outgoing edges). -\u200b Example: In a social network,"
    },
    "188": {
        "file": "Extended Notes - Introduction to the  Graph Data Model.pdf",
        "page": 12,
        "chunk": "degree centrality of a node is simply the number of edges connected to it. In undirected graphs, it is the number of neighbors (connections), while in directed graphs, it can be divided into in-degree (number of incoming edges) and out-degree (number of outgoing edges). -\u200b Example: In a social network, the person with the highest degree centrality might be the most connected (i.e., having the most friends or followers). -\u200b Closeness Centrality: Closeness centrality measures how close a node is to all other nodes in the graph. It is the reciprocal of the average shortest path distance from a node to all other nodes. Nodes with high closeness centrality are typically able to spread information quickly throughout the network. -\u200b Example: In a communication network, the node with the highest closeness centrality would be able to relay information to all other nodes faster."
    },
    "189": {
        "file": "Extended Notes - Introduction to the  Graph Data Model.pdf",
        "page": 13,
        "chunk": "-\u200b Betweenness Centrality: Betweenness centrality measures the extent to which a node lies on the shortest path between other nodes. Nodes with high betweenness centrality are considered brokers or gatekeepers in the network because they control information flow between other nodes. -\u200b Example: In a social network, a user with high betweenness centrality might play the role of a connector between different groups of people. -\u200b Eigenvector Centrality: Eigenvector centrality considers not just the number of connections a node has (like degree centrality) but also the quality of those connections. A node is considered important if it is connected to other important nodes. This measure is recursive and can be calculated using the power iteration method. -\u200b Example: In a recommendation system, a highly influential user might have connections to other influential users, which makes them more central in the network. -\u200b Use Cases for Centrality Algorithms: -\u200b Influencers in Social Networks: Identifying influential people (e.g., celebrities or thought leaders) who can affect trends and decisions. -\u200b Critical Infrastructure: Finding critical nodes in transportation, electrical grids, or communication networks, where failure could have widespread impacts. -\u200b Marketing and Ads: Determining which users to target in advertising campaigns for maximum reach and engagement. 2.\u200b Community Detection Algorithms: Community detection refers to the process of grouping or partitioning a graph's nodes into clusters (or communities), where nodes within the same cluster are more densely connected to each other than to nodes outside the cluster. The goal is to identify meaningful subgroups or communities within a graph. These communities can represent real-world groupings, like social groups, biological pathways, or clusters of related entities. -\u200b Modularity Maximization (Louvain Algorithm): Modularity is a measure that quantifies the strength of division of a network into communities. The Louvain method is an efficient algorithm that maximizes modularity"
    },
    "190": {
        "file": "Extended Notes - Introduction to the  Graph Data Model.pdf",
        "page": 13,
        "chunk": "communities within a graph. These communities can represent real-world groupings, like social groups, biological pathways, or clusters of related entities. -\u200b Modularity Maximization (Louvain Algorithm): Modularity is a measure that quantifies the strength of division of a network into communities. The Louvain method is an efficient algorithm that maximizes modularity by iteratively optimizing community assignments. It starts by treating each node as its own community and then merges communities that lead to a higher modularity score. -\u200b Example: Detecting communities in a social network (e.g., different interest groups or subcultures). -\u200b Girvan-Newman Algorithm: The Girvan-Newman algorithm detects communities by iteratively removing edges with the highest betweenness centrality. It works by progressively removing the most \"central\" edges in the graph, causing the graph to split into smaller connected components. This approach is computationally expensive but can work well for small to medium-sized graphs. -\u200b Example: Identifying groups within organizational structures or collaborative networks."
    },
    "191": {
        "file": "Extended Notes - Introduction to the  Graph Data Model.pdf",
        "page": 14,
        "chunk": "-\u200b Spectral Clustering: Spectral clustering uses the eigenvalues of the graph's Laplacian matrix to identify the graph's community structure. By performing eigenvalue decomposition, this algorithm finds groups of nodes that have similar connectivity patterns. Spectral clustering is particularly useful for graphs that are difficult to divide using traditional partitioning methods. -\u200b Example: Detecting communities in molecular networks or groupings in recommendation systems. -\u200b Label Propagation Algorithm: The Label Propagation Algorithm (LPA) is a simple, fast, and efficient algorithm that assigns a unique label to each node and then propagates the labels based on neighbors' labels until convergence. This algorithm works well for large networks due to its efficiency. -\u200b Example: Identifying communities in large-scale social media networks or knowledge graphs. -\u200b Infomap Algorithm: The Infomap algorithm models the graph as a network of flow and uses information theory to find communities by optimizing the compression of a random walk on the graph. It has been shown to perform well on large, real-world networks. -\u200b Example: Detecting hierarchical communities in large networks like the web or scientific citation graphs. -\u200b Use Cases for Community Detection Algorithms: -\u200b Social Network Analysis: Finding groups of users who share similar interests or behaviors (e.g., \"fan clubs\" on a social media platform). -\u200b Biological Networks: Identifying functional modules or gene clusters in molecular networks or protein interaction networks. -\u200b Recommendation Systems: Grouping users or items into similar clusters to make better recommendations. -\u200b Fraud Detection: Detecting hidden groups of fraudulent activities in financial transactions or social interactions. Neo4j Neo4j is a powerful, highly popular graph database that is designed specifically to store and query graph-based data efficiently. It supports both transactional and analytical processing of graph data, making it a versatile choice for a wide range of use cases, including real-time applications and complex analytics."
    },
    "192": {
        "file": "Extended Notes - Introduction to the  Graph Data Model.pdf",
        "page": 14,
        "chunk": "social interactions. Neo4j Neo4j is a powerful, highly popular graph database that is designed specifically to store and query graph-based data efficiently. It supports both transactional and analytical processing of graph data, making it a versatile choice for a wide range of use cases, including real-time applications and complex analytics. Key Features of Neo4j 1.\u200b Graph-Based Data Model: -\u200b Nodes and Relationships: Neo4j stores data as nodes (entities), edges (relationships), and properties (attributes). This structure is highly intuitive for graph data, allowing for efficient traversal and querying. -\u200b Schema Optional: While Neo4j can work with a flexible, schema-less design, it also allows users to define a schema if desired, making it adaptable to different use cases."
    },
    "193": {
        "file": "Extended Notes - Introduction to the  Graph Data Model.pdf",
        "page": 15,
        "chunk": "2.\u200b Transactional and Analytical Processing: -\u200b Neo4j is optimized for both transactional operations (like adding, deleting, and updating nodes and relationships) and analytical operations (such as complex graph algorithms like shortest path, centrality, and community detection). -\u200b This dual focus enables it to handle use cases that require both real-time transaction processing and large-scale graph analytics. 3.\u200b ACID Compliant: -\u200b Neo4j guarantees ACID (Atomicity, Consistency, Isolation, Durability) properties for database transactions, which ensures reliability and data integrity. This is crucial for applications where data accuracy is critical, such as financial systems or social networks. 4.\u200b Indexing: -\u200b Neo4j supports various indexing mechanisms to efficiently retrieve nodes and relationships. For example, B-tree and Lucene indexes can be used for faster lookups based on node properties. -\u200b Indexes are vital for performance, especially when working with large datasets. 5.\u200b Distributed Computing: -\u200b Neo4j supports distributed computing, allowing it to scale across multiple nodes and handle large graph datasets in a distributed fashion. This enables horizontal scalability and ensures the database can handle high workloads in a fault-tolerant manner. 6.\u200b Flexible Query Language (Cypher): -\u200b Neo4j uses Cypher, a graph-specific query language, which is intuitive and designed for easy interaction with graph structures. It enables users to express complex graph queries in a human-readable form. -\u200b Cypher allows users to perform graph traversals, pattern matching, and relationship analysis effectively. Comparison to Similar Graph Databases 1.\u200b Microsoft CosmosDB: -\u200b CosmosDB is a multi-model database that supports various types of data, including graph data, using Gremlin (another graph query language). -\u200b While it offers graph capabilities, CosmosDB is more of a general-purpose database, and Neo4j focuses specifically on graph-based workloads with more advanced graph-specific features like complex algorithms and native graph processing. 2.\u200b Amazon Neptune: -\u200b Neptune is a managed graph database service provided by"
    },
    "194": {
        "file": "Extended Notes - Introduction to the  Graph Data Model.pdf",
        "page": 15,
        "chunk": "Gremlin (another graph query language). -\u200b While it offers graph capabilities, CosmosDB is more of a general-purpose database, and Neo4j focuses specifically on graph-based workloads with more advanced graph-specific features like complex algorithms and native graph processing. 2.\u200b Amazon Neptune: -\u200b Neptune is a managed graph database service provided by AWS that supports both Gremlin (for property graphs) and SPARQL (for RDF graphs). -\u200b It is similar to Neo4j in that it handles graph-based data, but Neo4j has a larger focus on native graph analytics and a dedicated query language (Cypher) designed specifically for graph data, which can give it an edge in some use cases."
    },
    "195": {
        "file": "Extended Notes - Introduction to the  Graph Data Model.pdf",
        "page": 16,
        "chunk": "Common Use Cases for Neo4j -\u200b Social Networks: Mapping and analyzing connections between users, friends, and interests. Neo4j\u2019s graph-based structure allows for fast and efficient traversal and analysis of social interactions. -\u200b Recommendation Systems: Based on user preferences, behavior, and relationships, Neo4j can recommend products, content, or services by analyzing user connections and behavior patterns. -\u200b Fraud Detection: In financial systems, Neo4j can analyze transaction data for patterns and connections that may indicate fraudulent activity. The graph model makes it easy to track relationships across multiple nodes (e.g., users, transactions). -\u200b Network and IT Operations: Neo4j is used to model complex networks (like telecom or IT networks) and to optimize network operations, monitor infrastructure, or detect anomalies in the graph of network connections. -\u200b Knowledge Graphs: Neo4j is popular in building knowledge graphs that represent relationships between concepts, entities, and facts, providing a foundation for intelligent applications like search engines and virtual assistants."
    },
    "196": {
        "file": "05 - NoSQL Intro + KV DBs.pdf",
        "page": 0,
        "chunk": "DS 4300 NoSQL & KV DBs Mark Fontenot, PhD Northeastern University Some material used with permission from Dr. Rachlin, with thanks!"
    },
    "197": {
        "file": "05 - NoSQL Intro + KV DBs.pdf",
        "page": 1,
        "chunk": "Distributed DBs and ACID - Pessimistic Concurrency \u25cfACID transactions \u25cb Focuses on \u201cdata safety\u201d \u25cb considered a pessimistic concurrency model because it assumes one transaction has to protect itself from other transactions \u25a0 IOW, it assumes that if something can go wrong, it will. \u25cb Con\ufb02icts are prevented by locking resources until a transaction is complete (there are both read and write locks) \u25cb Write Lock Analogy \u2192 borrowing a book from a library\u2026 If you have it, no one else can. 2 See https://www.freecodecamp.org/news/how-databases-guarantee-isolation for more for a deeper dive."
    },
    "198": {
        "file": "05 - NoSQL Intro + KV DBs.pdf",
        "page": 2,
        "chunk": "Optimistic Concurrency \u25cfTransactions do not obtain locks on data when they read or write \u25cfOptimistic because it assumes con\ufb02icts are unlikely to occur \u25cb Even if there is a con\ufb02ict, everything will still be OK. \u25cfBut how? \u25cb Add last update timestamp and version number columns to every table\u2026 read them when changing. THEN, check at the end of transaction to see if any other transaction has caused them to be modi\ufb01ed. 3"
    },
    "199": {
        "file": "05 - NoSQL Intro + KV DBs.pdf",
        "page": 3,
        "chunk": "Optimistic Concurrency \u25cfLow Con\ufb02ict Systems (backups, analytical dbs, etc.) \u25cb Read heavy systems \u25cb the con\ufb02icts that arise can be handled by rolling back and re-running a transaction that notices a con\ufb02ict. \u25cb So, optimistic concurrency works well - allows for higher concurrency \u25cfHigh Con\ufb02ict Systems \u25cb rolling back and rerunning transactions that encounter a con\ufb02ict \u2192 less ef\ufb01cient \u25cb So, a locking scheme (pessimistic model) might be preferable 4"
    },
    "200": {
        "file": "05 - NoSQL Intro + KV DBs.pdf",
        "page": 4,
        "chunk": "NoSQL - \u201cNoSQL\u201d \ufb01rst used in 1998 by Carlo Strozzi to describe his relational database system that did not use SQL. - More common, modern meaning is \u201cNot Only SQL\u201d - But, sometimes thought of as non-relational DBs - Idea originally developed, in part, as a response to processing unstructured web-based data. 5 https://www.dataversity.net/a-brief-history-of-non-relational-databases/"
    },
    "201": {
        "file": "05 - NoSQL Intro + KV DBs.pdf",
        "page": 5,
        "chunk": "CAP Theorem Review 6 Reference: https://alperenbayramoglu.com/posts/understanding-cap-theorem/ You can have 2, but not 3, of the following: - Consistency*: Every user of the DB has an identical view of the data at any given instant - Availability: In the event of a failure, the database system remains operational - Partition Tolerance: The database can maintain operations in the event of the network\u2019s failing between two segments of the distributed system * Note, the definition of Consistency in CAP is different from that of ACID."
    },
    "202": {
        "file": "05 - NoSQL Intro + KV DBs.pdf",
        "page": 6,
        "chunk": "CAP Theorem Review 7 Reference: https://alperenbayramoglu.com/posts/understanding-cap-theorem/ - Consistency + Availability: System always responds with the latest data and every request gets a response, but may not be able to deal with network partitions - Consistency + Partition Tolerance: If system responds with data from the distrib. system, it is always the latest, else data request is dropped. - Availability + Partition Tolerance: System always sends are responds based on distributed store, but may not be the absolute latest data."
    },
    "203": {
        "file": "05 - NoSQL Intro + KV DBs.pdf",
        "page": 7,
        "chunk": "ACID Alternative for Distrib Systems - BASE \u25cfBasically Available \u25cbGuarantees the availability of the data (per CAP), but response can be \u201cfailure\u201d/\u201cunreliable\u201d because the data is in an inconsistent or changing state \u25cbSystem appears to work most of the time 8"
    },
    "204": {
        "file": "05 - NoSQL Intro + KV DBs.pdf",
        "page": 8,
        "chunk": "ACID Alternative for Distrib Systems - BASE \u25cfSoft State - The state of the system could change over time, even w/o input. Changes could be result of eventual consistency. \u25cbData stores don\u2019t have to be write-consistent \u25cbReplicas don\u2019t have to be mutually consistent 9"
    },
    "205": {
        "file": "05 - NoSQL Intro + KV DBs.pdf",
        "page": 9,
        "chunk": "ACID Alternative for Distrib Systems - BASE \u25cfEventual Consistency - The system will eventually become consistent \u25cbAll writes will eventually stop so all nodes/replicas can be updated 10"
    },
    "206": {
        "file": "05 - NoSQL Intro + KV DBs.pdf",
        "page": 10,
        "chunk": "Categories of NoSQL DBs - Review 11"
    },
    "207": {
        "file": "05 - NoSQL Intro + KV DBs.pdf",
        "page": 11,
        "chunk": "First Up \u2192 Key-Value Databases 12"
    },
    "208": {
        "file": "05 - NoSQL Intro + KV DBs.pdf",
        "page": 12,
        "chunk": "Key Value Stores key = value - Key-value stores are designed around: - simplicity - the data model is extremely simple - comparatively, tables in a RDBMS are very complex. - lends itself to simple CRUD ops and API creation 13"
    },
    "209": {
        "file": "05 - NoSQL Intro + KV DBs.pdf",
        "page": 13,
        "chunk": "Key Value Stores key = value - Key-value stores are designed around: - speed - usually deployed as in-memory DB - retrieving a value given its key is typically a O(1) op b/c hash tables or similar data structs used under the hood - no concept of complex queries or joins\u2026 they slow things down 14"
    },
    "210": {
        "file": "05 - NoSQL Intro + KV DBs.pdf",
        "page": 14,
        "chunk": "Key Value Stores key = value - Key-value stores are designed around: - scalability - Horizontal Scaling is simple - add more nodes - Typically concerned with eventual consistency, meaning in a distributed environment, the only guarantee is that all nodes will eventually converge on the same value. 15"
    },
    "211": {
        "file": "05 - NoSQL Intro + KV DBs.pdf",
        "page": 15,
        "chunk": "KV DS Use Cases - EDA/Experimentation Results Store - store intermediate results from data preprocessing and EDA - store experiment or testing (A/B) results w/o prod db - Feature Store - store frequently accessed feature \u2192 low-latency retrieval for model training and prediction - Model Monitoring - store key metrics about performance of model, for example, in real-time inferencing. 16"
    },
    "212": {
        "file": "05 - NoSQL Intro + KV DBs.pdf",
        "page": 16,
        "chunk": "KV SWE Use Cases - Storing Session Information - everything about the current session can be stored via a single PUT or POST and retrieved with a single GET \u2026. VERY Fast - User Pro\ufb01les & Preferences - User info could be obtained with a single GET operation\u2026 language, TZ, product or UI preferences - Shopping Cart Data - Cart data is tied to the user - needs to be available across browsers, machines, sessions - Caching Layer: - In front of a disk-based database 17"
    },
    "213": {
        "file": "05 - NoSQL Intro + KV DBs.pdf",
        "page": 17,
        "chunk": "Redis DB - Redis (Remote Directory Server) - Open source, in-memory database - Sometimes called a data structure store - Primarily a KV store, but can be used with other models: Graph, Spatial, Full Text Search, Vector, Time Series - From db-engines.com Ranking of KV Stores: 18"
    },
    "214": {
        "file": "05 - NoSQL Intro + KV DBs.pdf",
        "page": 18,
        "chunk": "Redis - It is considered an in-memory database system, but\u2026 - Supports durability of data by: a) essentially saving snapshots to disk at speci\ufb01c intervals or b) append-only \ufb01le which is a journal of changes that can be used for roll-forward if there is a failure - Originally developed in 2009 in C++ - Can be very fast \u2026 > 100,000 SET ops / second - Rich collection of commands - Does NOT handle complex data. No secondary indexes. Only supports lookup by Key. 19"
    },
    "215": {
        "file": "05 - NoSQL Intro + KV DBs.pdf",
        "page": 19,
        "chunk": "Redis Data Types Keys: - usually strings but can be any binary sequence Values: - Strings - Lists (linked lists) - Sets (unique unsorted string elements) - Sorted Sets - Hashes (string \u2192 string) - Geospatial data 20"
    },
    "216": {
        "file": "05 - NoSQL Intro + KV DBs.pdf",
        "page": 20,
        "chunk": "Setting Up Redis in Docker - In Docker Desktop, search for Redis. - Pull/Run the latest image (see above) - Optional Settings: add 6379 to Ports to expose that port so we can connect to it. - Normally, you would not expose the Redis port for security reasons - If you did this in a prod environment, major security hole. - Notice, we didn\u2019t set a password\u2026 21"
    },
    "217": {
        "file": "05 - NoSQL Intro + KV DBs.pdf",
        "page": 21,
        "chunk": "Connecting from DataGrip - File > New > Data Source > Redis - Give the Data Source a Name - Make sure the port is 6379 - Test the connection \u2705 22"
    },
    "218": {
        "file": "05 - NoSQL Intro + KV DBs.pdf",
        "page": 22,
        "chunk": "Redis Database and Interaction - Redis provides 16 databases by default - They are numbered 0 to 15 - There is no other name associated - Direct interaction with Redis is through a set of commands related to setting and getting k/v pairs (and variations) - Many language libraries available as well. 23"
    },
    "219": {
        "file": "05 - NoSQL Intro + KV DBs.pdf",
        "page": 23,
        "chunk": "Foundation Data Type - String - Sequence of bytes - text, serialized objects, bin arrays - Simplest data type - Maps a string to another string - Use Cases: - caching frequently accessed HTML/CSS/JS fragments - con\ufb01g settings, user settings info, token management - counting web page/app screen views OR rate limiting 24"
    },
    "220": {
        "file": "05 - NoSQL Intro + KV DBs.pdf",
        "page": 24,
        "chunk": "Some Initial Basic Commands - SET /path/to/resource 0 SET user:1 \u201cJohn Doe\u201d GET /path/to/resource EXISTS user:1 DEL user:1 KEYS user* - SELECT 5 - select a different database 25"
    },
    "221": {
        "file": "05 - NoSQL Intro + KV DBs.pdf",
        "page": 25,
        "chunk": "Some Basic Commands - SET someValue 0 INCR someValue #increment by 1 INCRBY someValue 10 #increment by 10 DECR someValue #decrement by 1 DECRBY someValue 5 #decrement by 5 - INCR parses the value as int and increments (or adds to value) - SETNX key value - only sets value to key if key does not already exist 26"
    },
    "222": {
        "file": "05 - NoSQL Intro + KV DBs.pdf",
        "page": 26,
        "chunk": "Hash Type 27 - Value of KV entry is a collection of \ufb01eld-value pairs - Use Cases: - Can be used to represent basic objects/structures - number of \ufb01eld/value pairs per hash is 2^32-1 - practical limit: available system resources (e.g. memory) - Session information management - User/Event tracking (could include TTL) - Active Session Tracking (all sessions under one hash key)"
    },
    "223": {
        "file": "05 - NoSQL Intro + KV DBs.pdf",
        "page": 27,
        "chunk": "Hash Commands 28 HSET bike:1 model Demios brand Ergonom price 1971 HGET bike:1 model HGET bike:1 price HGETALL bike:1 HMGET bike:1 model price weight HINCRBY bike:1 price 100 What is returned?"
    },
    "224": {
        "file": "05 - NoSQL Intro + KV DBs.pdf",
        "page": 28,
        "chunk": "List Type - Value of KV Pair is linked lists of string values - Use Cases: - implementation of stacks and queues - queue management & message passing queues (producer/consumer model) - logging systems (easy to keep in chronological order) - build social media streams/feeds - message history in a chat application - batch processing by queueing up a set of tasks to be executed sequentially at a later time 29"
    },
    "225": {
        "file": "05 - NoSQL Intro + KV DBs.pdf",
        "page": 29,
        "chunk": "Linked Lists Crash Course - Sequential data structure of linked nodes (instead of contiguously allocated memory) - Each node points to the next element of the list (except the last one - points to nil/null) - O(1) to insert new value at front or insert new value at end 30 10 front back nil"
    },
    "226": {
        "file": "05 - NoSQL Intro + KV DBs.pdf",
        "page": 30,
        "chunk": "List Commands - Queue Queue-like Ops LPUSH bikes:repairs bike:1 LPUSH bikes:repairs bike:2 RPOP bikes:repairs RPOP biles:repairs 31"
    },
    "227": {
        "file": "05 - NoSQL Intro + KV DBs.pdf",
        "page": 31,
        "chunk": "List Commands - Stack Stack-like Ops LPUSH bikes:repairs bike:1 LPUSH bikes:repairs bike:2 LPOP bikes:repairs LPOP biles:repairs 32"
    },
    "228": {
        "file": "05 - NoSQL Intro + KV DBs.pdf",
        "page": 32,
        "chunk": "List Commands - Others Other List Ops LLEN mylist LRANGE <key> <start> <stop> LRANGE mylist 0 3 LRANGE mylist 0 0 LRANGE mylist -2 -1 33 LPUSH mylist \u201cone\u201d LPUSH mylist \u201ctwo\u201d LPUSH mylist \u201cthree\u201d"
    },
    "229": {
        "file": "05 - NoSQL Intro + KV DBs.pdf",
        "page": 33,
        "chunk": "JSON Type - Full support of the JSON standard - Uses JSONPath syntax for parsing/navigating a JSON document - Internally, stored in binary in a tree-structure \u2192 fast access to sub elements 34"
    },
    "230": {
        "file": "05 - NoSQL Intro + KV DBs.pdf",
        "page": 34,
        "chunk": "Set Type - Unordered collection of unique strings (members) - Use Cases: - track unique items (IP addresses visiting a site, page, screen) - primitive relation (set of all students in DS4300) - access control lists for users and permission structures - social network friends lists and/or group membership - Supports set operations!! 35"
    },
    "231": {
        "file": "05 - NoSQL Intro + KV DBs.pdf",
        "page": 35,
        "chunk": "Set Commands SADD ds4300 \u201cMark\u201d SADD ds4300 \u201cSam\u201d SADD cs3200 \u201cNick\u201d SADD cs3200 \u201cSam\u201d SISMEMBER ds4300 \u201cMark\u201d SISMEMBER ds4300 \u201cNick\u201d SCARD ds4300 36"
    },
    "232": {
        "file": "05 - NoSQL Intro + KV DBs.pdf",
        "page": 36,
        "chunk": "Set Commands SADD ds4300 \u201cMark\u201d SADD ds4300 \u201cSam\u201d SADD cs3200 \u201cNick\u201d SADD cs3200 \u201cSam\u201d SCARD ds4300 SINTER ds4300 cs3200 SDIFF ds4300 cs3200 SREM ds4300 \u201cMark\u201d SRANDMEMBER ds4300 37"
    },
    "233": {
        "file": "05 - NoSQL Intro + KV DBs.pdf",
        "page": 37,
        "chunk": "?? 38"
    },
    "234": {
        "file": "Misc Professor Questions.pdf",
        "page": 0,
        "chunk": "1.\u200b Redis in Machine Learning: a.\u200b Pipeline: Data Source (AWS) -> Transformations (Apache Spark) -> Inference Store (Stored on Redis) + Training Store (Stored on AWS) i.\u200b Question: Why would you add complexity by adding the inference store on Redis? ii.\u200b Answer: For latency issues: accessing a key from Redis is much faster than running a SELECT statement. Redis is stored on disk which is faster than accessing from RAM. 2.\u200b Port mapping a:b a.\u200b A = host port on the physical machine b.\u200b B = port inside the container c.\u200b Maximum port number = 65535 d.\u200b At the operating system level, http maps to port 80, https maps to port 443, ssh maps to port 22, ftp maps to 21 e.\u200b Range of ports reserved for system services: 0 - 102"
    },
    "235": {
        "file": "07 - Document DBs and Mongo.pdf",
        "page": 0,
        "chunk": "DS 4300 Document Databases & MongoDB Mark Fontenot, PhD Northeastern University Some material used with permission from Dr. Rachlin, with thanks!"
    },
    "236": {
        "file": "07 - Document DBs and Mongo.pdf",
        "page": 1,
        "chunk": "Document Database A Document Database is a non-relational database that stores data as structured documents, usually in JSON. They are designed to be simple, \ufb02exible, and scalable. 2"
    },
    "237": {
        "file": "07 - Document DBs and Mongo.pdf",
        "page": 2,
        "chunk": "What is JSON? \u25cf JSON (JavaScript Object Notation) \u25cb a lightweight data-interchange format \u25cb It is easy for humans to read and write. \u25cb It is easy for machines to parse and generate. \u25cf JSON is built on two structures: \u25cb A collection of name/value pairs. In various languages, this is operationalized as an object, record, struct, dictionary, hash table, keyed list, or associative array. \u25cb An ordered list of values. In most languages, this is operationalized as an array, vector, list, or sequence. \u25cf These are two universal data structures supported by virtually all modern programming languages \u25cb Thus, JSON makes a great data interchange format. 3"
    },
    "238": {
        "file": "07 - Document DBs and Mongo.pdf",
        "page": 3,
        "chunk": "JSON Syntax 4 https://www.json.org/json-en.html"
    },
    "239": {
        "file": "07 - Document DBs and Mongo.pdf",
        "page": 4,
        "chunk": "Binary JSON? BSON - BSON \u2192 Binary JSON - binary-encoded serialization of a JSON-like document structure - supports extended types not part of basic JSON (e.g. Date, BinaryData, etc) - Lightweight - keep space overhead to a minimum - Traversable - designed to be easily traversed, which is vitally important to a document DB - Ef\ufb01cient - encoding and decoding must be ef\ufb01cient - Supported by many modern programming languages 5"
    },
    "240": {
        "file": "07 - Document DBs and Mongo.pdf",
        "page": 5,
        "chunk": "XML (eXtensible Markup Language) \u25cfPrecursor to JSON as data exchange format \u25cfXML + CSS \u2192 web pages that separated content and formatting \u25cfStructurally similar to HTML, but tag set is extensible 6"
    },
    "241": {
        "file": "07 - Document DBs and Mongo.pdf",
        "page": 6,
        "chunk": "XML-Related Tools/Technologies - Xpath - a syntax for retrieving speci\ufb01c elements from an XML doc - Xquery - a query language for interrogating XML documents; the SQL of XML - DTD - Document Type De\ufb01nition - a language for describing the allowed structure of an XML document - XSLT - eXtensible Stylesheet Language Transformation - tool to transform XML into other formats, including non-XML formats such as HTML. 7"
    },
    "242": {
        "file": "07 - Document DBs and Mongo.pdf",
        "page": 7,
        "chunk": "Why Document Databases? - Document databases address the impedance mismatch problem between object persistence in OO systems and how relational DBs structure data. - OO Programming \u2192 Inheritance and Composition of types. - How do we save a complex object to a relational database? We basically have to deconstruct it. - The structure of a document is self-describing. - They are well-aligned with apps that use JSON/XML as a transport layer 8"
    },
    "243": {
        "file": "07 - Document DBs and Mongo.pdf",
        "page": 8,
        "chunk": "MongoDB 9"
    },
    "244": {
        "file": "07 - Document DBs and Mongo.pdf",
        "page": 9,
        "chunk": "MongoDB - Started in 2007 after Doubleclick was acquired by Google, and 3 of its veterans realized the limitations of relational databases for serving > 400,000 ads per second - MongoDB was short for Humongous Database - MongoDB Atlas released in 2016 \u2192 documentdb as a service 10 https://www.mongodb.com/company/our-story"
    },
    "245": {
        "file": "07 - Document DBs and Mongo.pdf",
        "page": 10,
        "chunk": "MongoDB Structure 11 Database Collection A Collection B Collection C Document 1 Document 2 Document 3 Document 1 Document 2 Document 3 Document 1 Document 2 Document 3"
    },
    "246": {
        "file": "07 - Document DBs and Mongo.pdf",
        "page": 11,
        "chunk": "MongoDB Documents - No prede\ufb01ned schema for documents is needed - Every document in a collection could have different data/schema 12"
    },
    "247": {
        "file": "07 - Document DBs and Mongo.pdf",
        "page": 12,
        "chunk": "Relational vs Mongo/Document DB 13 RDBMS MongoDB Database Database Table/View Collection Row Document Column Field Index Index Join Embedded Document Foreign Key Reference"
    },
    "248": {
        "file": "07 - Document DBs and Mongo.pdf",
        "page": 13,
        "chunk": "MongoDB Features - Rich Query Support - robust support for all CRUD ops - Indexing - supports primary and secondary indices on document \ufb01elds - Replication - supports replica sets with automatic failover - Load balancing built in 14"
    },
    "249": {
        "file": "07 - Document DBs and Mongo.pdf",
        "page": 14,
        "chunk": "MongoDB Versions \u25cfMongoDB Atlas \u25cb Fully managed MongoDB service in the cloud (DBaaS) \u25cfMongoDB Enterprise \u25cb Subscription-based, self-managed version of MongoDB \u25cfMongoDB Community \u25cb source-available, free-to-use, self-managed 15"
    },
    "250": {
        "file": "07 - Document DBs and Mongo.pdf",
        "page": 15,
        "chunk": "Interacting with MongoDB \u25cfmongosh \u2192 MongoDB Shell \u25cb CLI tool for interacting with a MongoDB instance \u25cfMongoDB Compass \u25cb free, open-source GUI to work with a MongoDB database \u25cfDataGrip and other 3rd Party Tools \u25cfEvery major language has a library to interface with MongoDB \u25cb PyMongo (Python), Mongoose (JavaScript/node), \u2026 16"
    },
    "251": {
        "file": "07 - Document DBs and Mongo.pdf",
        "page": 16,
        "chunk": "Mongodb Community Edition in Docker - Create a container - Map host:container port 27017 - Give initial username and password for superuser 17 E D"
    },
    "252": {
        "file": "07 - Document DBs and Mongo.pdf",
        "page": 17,
        "chunk": "MongoDB Compass - GUI Tool for interacting with MongoDB instance - Download and install from > here <. 18"
    },
    "253": {
        "file": "07 - Document DBs and Mongo.pdf",
        "page": 18,
        "chunk": "Load MFlix Sample Data Set - In Compass, create a new Database named m\ufb02ix - Download m\ufb02ix sample dataset and unzip it - Import JSON \ufb01les for users, theaters, movies, and comments into new collections in the m\ufb02ix database 19"
    },
    "254": {
        "file": "07 - Document DBs and Mongo.pdf",
        "page": 19,
        "chunk": "Creating a Database and Collection 20 m\ufb02ix users To Create a new DB: To Create a new Collection:"
    },
    "255": {
        "file": "07 - Document DBs and Mongo.pdf",
        "page": 20,
        "chunk": "mongosh - Mongo Shell - \ufb01nd(...) is like SELECT 21 collection.find({ ____ }, { ____ }) \ufb01lters projections"
    },
    "256": {
        "file": "07 - Document DBs and Mongo.pdf",
        "page": 21,
        "chunk": "mongosh - \ufb01nd() - SELECT * FROM users; 22 use mflix db.users.find()"
    },
    "257": {
        "file": "07 - Document DBs and Mongo.pdf",
        "page": 22,
        "chunk": "mongosh - \ufb01nd() - SELECT * FROM users WHERE name = \u201cDavos Seaworth\u201d; 23 db.users.find({\"name\": \"Davos Seaworth\"}) \ufb01lter"
    },
    "258": {
        "file": "07 - Document DBs and Mongo.pdf",
        "page": 23,
        "chunk": "mongosh - \ufb01nd() - SELECT * FROM movies WHERE rated in (\"PG\", \"PG-13\") 24 db.movies.find({rated: {$in:[ \"PG\", \"PG-13\" ]}})"
    },
    "259": {
        "file": "07 - Document DBs and Mongo.pdf",
        "page": 24,
        "chunk": "mongosh - \ufb01nd() - Return movies which were released in Mexico and have an IMDB rating of at least 7 25 db.movies.find( { \"countries\": \"Mexico\", \"imdb.rating\": { $gte: 7 } } )"
    },
    "260": {
        "file": "07 - Document DBs and Mongo.pdf",
        "page": 25,
        "chunk": "mongosh - \ufb01nd() - Return movies from the movies collection which were released in 2010 and either won at least 5 awards or have a genre of Drama 26 db.movies.find( { \u201cyear\u201d: 2010, $or: [ { \"awards.wins\": { $gte: 5 } }, { \u201cgenres\u201d: \"Drama\" } ] })"
    },
    "261": {
        "file": "07 - Document DBs and Mongo.pdf",
        "page": 26,
        "chunk": "Comparison Operators 27"
    },
    "262": {
        "file": "07 - Document DBs and Mongo.pdf",
        "page": 27,
        "chunk": "mongosh - countDocuments() - How many movies from the movies collection were released in 2010 and either won at least 5 awards or have a genre of Drama 28 db.movies.countDocuments( { \u201cyear\u201d: 2010, $or: [ { \"awards.wins\": { $gte: 5 } }, { \u201cgenres\u201d: \"Drama\" } ] })"
    },
    "263": {
        "file": "07 - Document DBs and Mongo.pdf",
        "page": 28,
        "chunk": "mongosh - project - Return the names of all movies from the movies collection that were released in 2010 and either won at least 5 awards or have a genre of Drama 29 db.movies.countDocuments( { \u201cyear\u201d: 2010, $or: [ { \"awards.wins\": { $gte: 5 } }, { \u201cgenres\u201d: \"Drama\" } ] }, {\u201cname\u201d: 1, \u201c_id\u201d: 0} ) 1 = return; 0 = don\u2019t return"
    },
    "264": {
        "file": "07 - Document DBs and Mongo.pdf",
        "page": 29,
        "chunk": "PyMongo 30"
    },
    "265": {
        "file": "07 - Document DBs and Mongo.pdf",
        "page": 30,
        "chunk": "PyMongo \u25cfPyMongo is a Python library for interfacing with MongoDB instances 31 from pymongo import MongoClient client = MongoClient( \u2018mongodb://user_name:pw@localhost:27017\u2019 )"
    },
    "266": {
        "file": "07 - Document DBs and Mongo.pdf",
        "page": 31,
        "chunk": "Getting a Database and Collection 32 from pymongo import MongoClient client = MongoClient( \u2018mongodb://user_name:pw@localhost:27017\u2019 ) db = client[\u2018ds4300\u2019] collection = db[\u2018myCollection\u2019]"
    },
    "267": {
        "file": "07 - Document DBs and Mongo.pdf",
        "page": 32,
        "chunk": "Inserting a Single Document 33 db = client[\u2018ds4300\u2019] collection = db[\u2018myCollection\u2019] post = { \u201cauthor\u201d: \u201cMark\u201d, \u201ctext\u201d: \u201cMongoDB is Cool!\u201d, \u201ctags\u201d: [\u201cmongodb\u201d, \u201cpython\u201d] } post_id = collection.insert_one(post).inserted_id print(post_id)"
    },
    "268": {
        "file": "07 - Document DBs and Mongo.pdf",
        "page": 33,
        "chunk": "Count Documents in Collection - SELECT count(*) FROM collection 34 demodb.collection.count_documents({})"
    },
    "269": {
        "file": "07 - Document DBs and Mongo.pdf",
        "page": 34,
        "chunk": "?? 35"
    },
    "270": {
        "file": "Extended Notes - Open DSA B+ Trees.pdf",
        "page": 0,
        "chunk": "12.6. B-Trees 12.6.1. B-Trees \u00b6 This module presents the B-tree. B-trees are usually attributed to R. Bayer and E. McCreight who described the B-tree in a 1972 paper. By 1979, B-trees had replaced virtually all large-file access methods other than hashing. B-trees, or some variant of B-trees, are the standard file organization for applications requiring insertion, deletion, and key range searches. They are used to implement most modern file systems. B-trees address effectively all of the major problems encountered when implementing disk-based search trees: 1. The B-tree is shallow, in part because the tree is always height balanced (all leaf nodes are at the same level), and in part because the branching factor is quite high. So only a small number of disk blocks are accessed to reach a given record. 2. Update and search operations affect only those disk blocks on the path from the root to the leaf node containing the query record. The fewer the number of disk blocks affected during an operation, the less disk I/O is required. 3. B-trees keep related records (that is, records with similar key values) on the same disk block, which helps to minimize disk I/O on range searches. 4. B-trees guarantee that every node in the tree will be full at least to a certain minimum percentage. This improves space efficiency while reducing the typical number of disk fetches necessary during a search or update operation. A B-tree of order is defined to have the following shape properties: The root is either a leaf or has at least two children. Each internal node, except for the root, has between and children. All leaves are at the same level in the tree, so the tree is always height balanced. The B-tree is a generalization of the 2-3 tree. Put another way,"
    },
    "271": {
        "file": "Extended Notes - Open DSA B+ Trees.pdf",
        "page": 0,
        "chunk": "is either a leaf or has at least two children. Each internal node, except for the root, has between and children. All leaves are at the same level in the tree, so the tree is always height balanced. The B-tree is a generalization of the 2-3 tree. Put another way, a 2-3 tree is a B-tree of order three. Normally, the size of a node in the B-tree is chosen to fill a disk block. A B-tree node implementation typically allows 100 or more children. Thus, a B-tree node is equivalent to a disk block, and a \u201cpointer\u201d value stored in the tree is actually the number of the block containing the child node (usually interpreted as an offset from the beginning of the corresponding disk file). In a typical application, the B-tree\u2019s access to the disk file will be managed using a buffer pool and a block-replacement scheme such as LRU. Figure 12.6.1 shows a B-tree of order four. Each node contains up to three keys, and internal nodes have up to four children. Figure 12.6.1: A B-tree of order four. Search in a B-tree is a generalization of search in a 2-3 tree. It is an alternating two-step process, beginning with the root node of the B-tree. 1. Perform a binary search on the records in the current node. If a record with the search key is found, then return that record. If the current node is a leaf node and the key is not found, then report an unsuccessful search. 2. Otherwise, follow the proper branch and repeat the process. For example, consider a search for the record with key value 47 in the tree of Figure 12.6.1. The root node is examined and the second (right) branch taken. After examining the node at level 1, the"
    },
    "272": {
        "file": "Extended Notes - Open DSA B+ Trees.pdf",
        "page": 0,
        "chunk": "an unsuccessful search. 2. Otherwise, follow the proper branch and repeat the process. For example, consider a search for the record with key value 47 in the tree of Figure 12.6.1. The root node is examined and the second (right) branch taken. After examining the node at level 1, the third branch is taken to the next level to arrive at the leaf node containing a record with key value 47. B-tree insertion is a generalization of 2-3 tree insertion. The first step is to find the leaf node that should contain the key to be inserted, space permitting. If there is room in this node, then insert the key. If there is not, then split the node into two and promote the middle key to the parent. If the parent becomes full, then it is split in turn, and its middle key promoted. Note that this insertion process is guaranteed to keep all nodes at least half full. For example, when we attempt to insert into a full internal node of a B-tree of order four, there will now be five children that must be dealt with. The node is split into two nodes containing two keys each, thus retaining the B-tree property. The middle of the five children is promoted to its parent. 12.6.1.1. B+ Trees The previous section mentioned that B-trees are universally used to implement large-scale disk-based systems. Actually, the B-tree as described in the previous section is almost never implemented. What is most commonly implemented is a variant of the B-tree, called the tree. When greater efficiency is required, a more complicated variant known as the tree is used. Consider again the linear index. When the collection of records will not change, a linear index provides an extremely efficient way to search. The problem is"
    },
    "273": {
        "file": "Extended Notes - Open DSA B+ Trees.pdf",
        "page": 0,
        "chunk": "is a variant of the B-tree, called the tree. When greater efficiency is required, a more complicated variant known as the tree is used. Consider again the linear index. When the collection of records will not change, a linear index provides an extremely efficient way to search. The problem is how to handle those pesky inserts and deletes. We could try to keep the core idea of storing a sorted array-based list, but make it more flexible by breaking the list into manageable chunks that are more easily updated. How might we do that? First, we need to decide how big the chunks should be. Since the data are on disk, it seems reasonable to store a chunk that is the size of a disk block, or a small multiple of the disk block size. If the next record to be inserted belongs to a chunk that hasn\u2019t filled its block then we can just insert it there. The fact that this might cause other records in that chunk to move a little bit in the array is not important, since this does not cause any extra disk accesses so long as we move data within that chunk. But what if the chunk fills up the entire block that contains it? We could just split it in half. What if we want to delete a record? We could just take the deleted record out of the chunk, but we might not want a lot of near-empty chunks. So we could put adjacent chunks together if they have only a small amount of data between them. Or we could shuffle data between adjacent chunks that together contain more data. The big problem would be how to find the desired chunk when processing a record with a given key. Perhaps some sort"
    },
    "274": {
        "file": "Extended Notes - Open DSA B+ Trees.pdf",
        "page": 0,
        "chunk": "put adjacent chunks together if they have only a small amount of data between them. Or we could shuffle data between adjacent chunks that together contain more data. The big problem would be how to find the desired chunk when processing a record with a given key. Perhaps some sort of tree-like structure could be used to locate the appropriate chunk. These ideas are exactly what motivate the tree. The tree is essentially a mechanism for managing a sorted array-based list, where the list is broken into chunks. The most significant difference between the tree and the BST or the standard B-tree is that the tree stores records only at the leaf nodes. Internal nodes store key values, but these are used solely as placeholders to guide the search. This means that internal nodes are significantly different in structure from leaf nodes. Internal nodes store keys to guide the search, associating m \u2308m/2\u2309 m 24 15 20 33 45 48 10 12 18 21 23 30 30 38 47 50 52 60 B+ B\u2217 B+ B+ B+ B+"
    },
    "275": {
        "file": "Extended Notes - Open DSA B+ Trees.pdf",
        "page": 1,
        "chunk": "each key with a pointer to a child tree node. Leaf nodes store actual records, or else keys and pointers to actual records in a separate disk file if the tree is being used purely as an index. Depending on the size of a record as compared to the size of a key, a leaf node in a tree of order might have enough room to store more or less than records. The requirement is simply that the leaf nodes store enough records to remain at least half full. The leaf nodes of a tree are normally linked together to form a doubly linked list. Thus, the entire collection of records can be traversed in sorted order by visiting all the leaf nodes on the linked list. Here is a Java-like pseudocode representation for the tree node interface. Leaf node and internal node subclasses would implement this interface. /** Interface for B+ Tree nodes */ public interface BPNode<Key,E> { public boolean isLeaf(); public int numrecs(); public Key[] keys(); } An important implementation detail to note is that while Figure 12.6.1 shows internal nodes containing three keys and four pointers, class BPNode is slightly different in that it stores key/pointer pairs. Figure 12.6.1 shows the tree as it is traditionally drawn. To simplify implementation in practice, nodes really do associate a key with each pointer. Each internal node should be assumed to hold in the leftmost position an additional key that is less than or equal to any possible key value in the node\u2019s leftmost subtree. tree implementations typically store an additional dummy record in the leftmost leaf node whose key value is less than any legal key value. Let\u2019s see in some detail how the simplest tree works. This would be the \u201c tree\u201d, or a tree of order 3."
    },
    "276": {
        "file": "Extended Notes - Open DSA B+ Trees.pdf",
        "page": 1,
        "chunk": "in the node\u2019s leftmost subtree. tree implementations typically store an additional dummy record in the leftmost leaf node whose key value is less than any legal key value. Let\u2019s see in some detail how the simplest tree works. This would be the \u201c tree\u201d, or a tree of order 3. Figure 12.6.2: An example of building a tree Next, let\u2019s see how to search. Figure 12.6.3: An example of searching a tree Finally, let\u2019s see an example of deleting from the tree B+ B+ B+ m m B+ B+ B+ B+ B+ 2 \u22123+ B+ 1 / 28 << < > >> Example 2-3+ Tree Visualization: Insert 2 \u22123+ 1 / 10 << < > >> Example 2-3+ Tree Visualization: Search 15 J 22 X 52 B 33 65 S 71 W 89 M 71 46 65 33 O 46 H 47 L 52 2 \u22123+ 2 \u22123+ 1 / 33 << < > >> Example 2-3+ Tree Visualization: Delete 46 65"
    },
    "277": {
        "file": "Extended Notes - Open DSA B+ Trees.pdf",
        "page": 2,
        "chunk": "Figure 12.6.4: An example of deleting from a tree Now, let\u2019s extend these ideas to a tree of higher order. trees are exceptionally good for range queries. Once the first record in the range has been found, the rest of the records with keys in the range can be accessed by sequential processing of the remaining records in the first node, and then continuing down the linked list of leaf nodes as far as necessary. Figure illustrates the tree. Figure 12.6.5: An example of search in a B+ tree of order four. Internal nodes must store between two and four children. Search in a tree is nearly identical to search in a regular B-tree, except that the search must always continue to the proper leaf node. Even if the search-key value is found in an internal node, this is only a placeholder and does not provide access to the actual record. Here is a pseudocode sketch of the tree search algorithm. private E findhelp(BPNode<Key,E> rt, Key k) { int currec = binaryle(rt.keys(), rt.numrecs(), k); if (rt.isLeaf()) { if ((((BPLeaf<Key,E>)rt).keys())[currec] == k) { return ((BPLeaf<Key,E>)rt).recs(currec); } else { return null; } } else{ return findhelp(((BPInternal<Key,E>)rt).pointers(currec), k); } } tree insertion is similar to B-tree insertion. First, the leaf that should contain the record is found. If is not full, then the new record is added, and no other tree nodes are affected. If is already full, split it in two (dividing the records evenly among the two nodes) and promote a copy of the least-valued key in the newly formed right node. As with the 2-3 tree, promotion might cause the parent to split in turn, perhaps eventually leading to splitting the root and causing the tree to gain a new level. tree insertion keeps all leaf nodes at equal depth."
    },
    "278": {
        "file": "Extended Notes - Open DSA B+ Trees.pdf",
        "page": 2,
        "chunk": "copy of the least-valued key in the newly formed right node. As with the 2-3 tree, promotion might cause the parent to split in turn, perhaps eventually leading to splitting the root and causing the tree to gain a new level. tree insertion keeps all leaf nodes at equal depth. Figure illustrates the insertion process through several examples. 15 J 71 W 89 M 22 65 S 70 F 51 B 52 T 71 46 H 47 L 22 X 33 O 51 2 \u22123+ B+ B+ B+ 1 / 10 << < > >> Example B+ Tree Visualization: Search in a tree of degree 4 10 S 18 E 40 Q 55 F 25 40 77 A 89 B 98 A 127 V 25 T 39 F 98 77 B+ B+ B+ L L B+ L B+ B+ 1 / 42 << < > >> Example B+ Tree Visualization: Insert into a tree of degree 4"
    },
    "279": {
        "file": "Extended Notes - Open DSA B+ Trees.pdf",
        "page": 3,
        "chunk": "Figure 12.6.6: An example of building a B+ tree of order four. Here is a a Java-like pseudocode sketch of the tree insert algorithm. private BPNode<Key,E> inserthelp(BPNode<Key,E> rt, Key k, E e) { BPNode<Key,E> retval; if (rt.isLeaf()) { // At leaf node: insert here return ((BPLeaf<Key,E>)rt).add(k, e); } // Add to internal node int currec = binaryle(rt.keys(), rt.numrecs(), k); BPNode<Key,E> temp = inserthelp( ((BPInternal<Key,E>)root).pointers(currec), k, e); if (temp != ((BPInternal<Key,E>)rt).pointers(currec)) { return ((BPInternal<Key,E>)rt). add((BPInternal<Key,E>)temp); } else{ return rt; } } Here is an exercise to see if you get the basic idea of tree insertion. To delete record from the tree, first locate the leaf that contains . If is more than half full, then we need only remove , leaving still at least half full. This is demonstrated by Figure . Figure 12.6.7: An example of deletion in a B+ tree of order four. B+ B+ B+ Tree Insertion Instructions: In this exercise your job is to insert the values from the stack to the B+ tree. Search for the leaf node where the topmost value of the stack should be inserted, and click on that node. The exercise will take care of the rest. Continue this procedure until you have inserted all the values in the stack. Undo Reset Model Answer Grade 47 64 54 72 38 90 92 14 21 76 95 49 35 13 10 34 60 86 R B+ L R L R L 1 / 23 << < > >> Example B+ Tree Visualization: Delete from a tree of degree 4 5 F 10 S 44 Q 48 E 12 44 67 A 88 B 58 A 60 F 12 V 27 T 67 58"
    },
    "280": {
        "file": "Extended Notes - Open DSA B+ Trees.pdf",
        "page": 3,
        "chunk": "Delete from a tree of degree 4 5 F 10 S 44 Q 48 E 12 44 67 A 88 B 58 A 60 F 12 V 27 T 67 58"
    },
    "281": {
        "file": "Extended Notes - Open DSA B+ Trees.pdf",
        "page": 4,
        "chunk": "If deleting a record reduces the number of records in the node below the minimum threshold (called an underflow), then we must do something to keep the node sufficiently full. The first choice is to look at the node\u2019s adjacent siblings to determine if they have a spare record that can be used to fill the gap. If so, then enough records are transferred from the sibling so that both nodes have about the same number of records. This is done so as to delay as long as possible the next time when a delete causes this node to underflow again. This process might require that the parent node has its placeholder key value revised to reflect the true first key value in each node. If neither sibling can lend a record to the under-full node (call it ), then must give its records to a sibling and be removed from the tree. There is certainly room to do this, because the sibling is at most half full (remember that it had no records to contribute to the current node), and has become less than half full because it is under-flowing. This merge process combines two subtrees of the parent, which might cause it to underflow in turn. If the last two children of the root merge together, then the tree loses a level. Here is a Java-like pseudocode for the tree delete algorithm. /** Delete a record with the given key value, and return true if the root underflows */ private boolean removehelp(BPNode<Key,E> rt, Key k) { int currec = binaryle(rt.keys(), rt.numrecs(), k); if (rt.isLeaf()) { if (((BPLeaf<Key,E>)rt).keys()[currec] == k) { return ((BPLeaf<Key,E>)rt).delete(currec); } else { return false; } } else{ // Process internal node if (removehelp(((BPInternal<Key,E>)rt).pointers(currec), k)) { // Child will merge if necessary return ((BPInternal<Key,E>)rt).underflow(currec); } else"
    },
    "282": {
        "file": "Extended Notes - Open DSA B+ Trees.pdf",
        "page": 4,
        "chunk": "*/ private boolean removehelp(BPNode<Key,E> rt, Key k) { int currec = binaryle(rt.keys(), rt.numrecs(), k); if (rt.isLeaf()) { if (((BPLeaf<Key,E>)rt).keys()[currec] == k) { return ((BPLeaf<Key,E>)rt).delete(currec); } else { return false; } } else{ // Process internal node if (removehelp(((BPInternal<Key,E>)rt).pointers(currec), k)) { // Child will merge if necessary return ((BPInternal<Key,E>)rt).underflow(currec); } else { return false; } } } The tree requires that all nodes be at least half full (except for the root). Thus, the storage utilization must be at least 50%. This is satisfactory for many implementations, but note that keeping nodes fuller will result both in less space required (because there is less empty space in the disk file) and in more efficient processing (fewer blocks on average will be read into memory because the amount of information in each block is greater). Because B-trees have become so popular, many algorithm designers have tried to improve B-tree performance. One method for doing so is to use the tree variant known as the tree. The tree is identical to the tree, except for the rules used to split and merge nodes. Instead of splitting a node in half when it overflows, the tree gives some records to its neighboring sibling, if possible. If the sibling is also full, then these two nodes split into three. Similarly, when a node underflows, it is combined with its two siblings, and the total reduced to two nodes. Thus, the nodes are always at least two thirds full. [1] Finally, here is an example of building a B+ Tree of order five. You can compare this to the example above of building a tree of order four with the same records. Figure 12.6.8: An example of building a B+ tree of degree 5 Click here for a visualization that will let you construct and interact"
    },
    "283": {
        "file": "Extended Notes - Open DSA B+ Trees.pdf",
        "page": 4,
        "chunk": "building a B+ Tree of order five. You can compare this to the example above of building a tree of order four with the same records. Figure 12.6.8: An example of building a B+ tree of degree 5 Click here for a visualization that will let you construct and interact with a tree. This visualization was written by David Galles of the University of San Francisco as part of his Data Structure Visualizations package. [1] This concept can be extended further if higher space utilization is required. However, the update routines become much more complicated. I once worked on a project where we implemented 3- for-4 node split and merge routines. This gave better performance than the 2-for-3 node split and merge routines of the tree. However, the spitting and merging routines were so complicated that even their author could no longer understand them once they were completed! 12.6.1.2. B-Tree Analysis The asymptotic cost of search, insertion, and deletion of records from B-trees, trees, and trees is where is the total number of records in the tree. However, the base of the log is the (average) branching factor of the tree. Typical database applications use extremely high branching factors, perhaps 100 or more. Thus, in practice the B-tree and its variants are extremely shallow. As an illustration, consider a tree of order 100 and leaf nodes that contain up to 100 records. A B- tree with height one (that is, just a single leaf node) can have at most 100 records. A tree with height two (a root internal node whose children are leaves) must have at least 100 records (2 leaves with 50 records each). It has at most 10,000 records (100 leaves with 100 records each). A tree with height three must have at least 5000 records (two"
    },
    "284": {
        "file": "Extended Notes - Open DSA B+ Trees.pdf",
        "page": 4,
        "chunk": "records. A tree with height two (a root internal node whose children are leaves) must have at least 100 records (2 leaves with 50 records each). It has at most 10,000 records (100 leaves with 100 records each). A tree with height three must have at least 5000 records (two second-level nodes with 50 children containing 50 records each) and at most one million records (100 second-level nodes N N N B+ B+ B+ B\u2217 B\u2217 B+ B\u2217 1 / 33 << < > >> Example B+ Tree Visualization: Insert into a tree of degree 5 B+ B\u2217 B+ B\u2217 \u0398(log n) n B+ B+ B+ B+"
    },
    "285": {
        "file": "Extended Notes - Open DSA B+ Trees.pdf",
        "page": 5,
        "chunk": "with 100 full children each). A tree with height four must have at least 250,000 records and at most 100 million records. Thus, it would require an extremely large database to generate a tree of more than height four. The tree split and insert rules guarantee that every node (except perhaps the root) is at least half full. So they are on average about 3/4 full. But the internal nodes are purely overhead, since the keys stored there are used only by the tree to direct search, rather than store actual data. Does this overhead amount to a significant use of space? No, because once again the high fan-out rate of the tree structure means that the vast majority of nodes are leaf nodes. A K-ary tree has approximately of its nodes as internal nodes. This means that while half of a full binary tree\u2019s nodes are internal nodes, in a tree of order 100 probably only about of its nodes are internal nodes. This means that the overhead associated with internal nodes is very low. We can reduce the number of disk fetches required for the B-tree even more by using the following methods. First, the upper levels of the tree can be stored in main memory at all times. Because the tree branches so quickly, the top two levels (levels 0 and 1) require relatively little space. If the B-tree is only height four, then at most two disk fetches (internal nodes at level two and leaves at level three) are required to reach the pointer to any given record. A buffer pool could be used to manage nodes of the B-tree. Several nodes of the tree would typically be in main memory at one time. The most straightforward approach is to use a standard method such as"
    },
    "286": {
        "file": "Extended Notes - Open DSA B+ Trees.pdf",
        "page": 5,
        "chunk": "level three) are required to reach the pointer to any given record. A buffer pool could be used to manage nodes of the B-tree. Several nodes of the tree would typically be in main memory at one time. The most straightforward approach is to use a standard method such as LRU to do node replacement. However, sometimes it might be desirable to \u201clock\u201d certain nodes such as the root into the buffer pool. In general, if the buffer pool is even of modest size (say at least twice the depth of the tree), no special techniques for node replacement will be required because the upper-level nodes will naturally be accessed frequently. B+ B+ B+ 1/K B+ 1/75"
    },
    "287": {
        "file": "Hashmaps and B+ Trees.pdf",
        "page": 0,
        "chunk": "Hashmaps \u25cf\u200b Like a python dictionary \u25cf\u200b Collection of each slots; each slot has an \u201caddress\u201d \u25cf\u200b Given a key, apply some hashing function to it that maps it to an integer from 0 to n - 1, where n is the size of the table \u25cf\u200b To ensure that the hash function maps to an integer between 0 and n - 1, you can mod it by the table size \u25cf\u200b Many values may map to the same slot, therefore we have to store the key and the value to get the information we\u2019re looking for \u25cf\u200b The load factor \u03bb can be computed by dividing the the number of inserted values (n) by the table size (m) \u25cb\u200b \u03bb = n / m \u25cb\u200b We usually like to keep the load factor under a particular threshold \u25cf\u200b Why would we use hash tables over AVL trees? \u25cb\u200b Hash functions take O(1) time and passing the same input results in the same output each time, making it consistent across insertion and searching \u25cb\u200b Insertion can be done in O(1) time, making it faster than AVL insertion \u25cf\u200b Collisions - occurs when two keys are inserted into the same slot \u25cb\u200b Resolutions \u25a0\u200b Not insert \u25a0\u200b Look for next open space (open addressing) \u25a0\u200b Make the value a list of values (separate chaining) \u25cf\u200b How to choose the best table size? \u25cb\u200b Start with some set size \u25cb\u200b With each insertion, check the load factor, and resize if needed \u25cb\u200b When you do need to resize, multiply the original table size by some factor k \u25cb\u200b You have to re-hash every time you resize the table \u25cf\u200b Hash Table efficiency \u25cb\u200b Searching in a large table with short chains is practically constant time \u25a0\u200b Finding one slot out of 100,000"
    },
    "288": {
        "file": "Hashmaps and B+ Trees.pdf",
        "page": 0,
        "chunk": "When you do need to resize, multiply the original table size by some factor k \u25cb\u200b You have to re-hash every time you resize the table \u25cf\u200b Hash Table efficiency \u25cb\u200b Searching in a large table with short chains is practically constant time \u25a0\u200b Finding one slot out of 100,000 = O(1) \u25a0\u200b Once you find a slot, finding one key out of maximum five key-value pairs is also constant time, similar to searching in an AVL tree with height log(100,000) \u25cb\u200b We want hash tables with good dispersion (broad distribution) B+ Trees \u25cf\u200b B+ Trees are the most common indexing structure used in RBDs"
    },
    "289": {
        "file": "Hashmaps and B+ Trees.pdf",
        "page": 1,
        "chunk": "\u25cf\u200b Optimized for disk-based accessing \u25cf\u200b A B+ tree is an m-way tree with order M \u25cb\u200b M is the maximum number of keys in each node \u25cb\u200b M+1 is the maximum number of children that each node can have \u25cb\u200b Node structure for M = 3 \u25a0\u200b A, B, C are keys, where A < B < C \u25a0\u200b The left and right children of the keys are pointers. \u25cf\u200b The left pointer of A contains keys that are < A. The right pointer of A contains values that are \u2265 A, but < B. The right pointer of B contains values that are \u2265B but < C. The right pointer of C contains values that are \u2265 C. \u25cf\u200b Properties of a B+ Tree \u25cb\u200b All nodes (except the root) must be at least half full (of children) \u25cb\u200b Root node does not have to be half full \u25cb\u200b Insertions are always done at the leaf level \u25cb\u200b Leaves are stored as a doubly-linked list \u25cb\u200b For the same set of values, a B+ tree will be shallowers, but wider, than a BST \u25cb\u200b Within a node, keys ar kept sortd \u25cf\u200b Insertion in a B+ Tree \u25cb\u200b The insertion process involves adding new keys to the appropriate leaf nodes while maintaining the tree's balanced properties. \u25cb\u200b Steps for Insertion in B+ Tree: \u25a0\u200b Locate the Appropriate Leaf Node: \u25cf\u200b Begin by traversing the tree from the root to find the correct leaf node where the new key should be inserted. \u25a0\u200b Insert the Key: \u25cf\u200b If the leaf node has fewer than the maximum allowed keys, insert the new key in the correct position to maintain sorted order. \u25cf\u200b If the leaf node is full, split it into two nodes: \u25cf\u200b Distribute the existing keys and the"
    },
    "290": {
        "file": "Hashmaps and B+ Trees.pdf",
        "page": 1,
        "chunk": "should be inserted. \u25a0\u200b Insert the Key: \u25cf\u200b If the leaf node has fewer than the maximum allowed keys, insert the new key in the correct position to maintain sorted order. \u25cf\u200b If the leaf node is full, split it into two nodes: \u25cf\u200b Distribute the existing keys and the new key between the two nodes evenly. \u25cf\u200b Copy the middle key (median) to the parent node to act as a separator between the two new nodes. \u25a0\u200b Handle Parent Node: \u25cf\u200b If the parent node also becomes full due to the insertion of the median key, repeat the splitting process recursively up the tree."
    },
    "291": {
        "file": "Hashmaps and B+ Trees.pdf",
        "page": 2,
        "chunk": "\u25cf\u200b If the splitting reaches the root and it becomes full, create a new root node, increasing the height of the tree."
    },
    "292": {
        "file": "Extended Notes - Document Databases & MongoDB.pdf",
        "page": 0,
        "chunk": "Document Database: A Flexible NoSQL Solution -\u200b A document database is a type of NoSQL database that stores, retrieves, and manages data in the form of structured documents, typically JSON, BSON, or XML. Unlike relational databases that store data in rows and columns, document databases store data as self-contained, hierarchical documents that can be easily modified and scaled. Key Characteristics of Document Databases 1.\u200b Schema Flexibility -\u200b Unlike relational databases, document databases do not require a predefined schema. -\u200b Each document can have different fields and structures, making it easy to store complex and nested data without requiring multiple tables. -\u200b Schema evolution is seamless, allowing for rapid changes without migration issues. -\u200b Example: In a relational database, this would require multiple tables (users, preferences), but in a document database, it is stored as a single document. 2.\u200b JSON-Based Storage Model -\u200b Data is typically stored in JSON-like formats (e.g., BSON in MongoDB). -\u200b JSON is human-readable, hierarchical, and self-contained, making it a natural choice for modern applications. -\u200b Documents can store arrays and nested structures directly. -\u200b Example: Storing product details in an e-commerce platform 3.\u200b High Scalability -\u200b Document databases are designed for horizontal scaling using sharding and replication. -\u200b Distributed across multiple nodes, making them ideal for large-scale applications. -\u200b Can handle massive workloads and high read/write throughput. 4.\u200b Optimized for Read and Write Performance -\u200b Direct lookup by key enables fast reads. -\u200b Indexes on document fields can optimize queries for performance. -\u200b Unlike relational databases, which require complex joins, document databases store related data in a single document, reducing query complexity. Advantages: -\u200b Flexible schema \u2013 Easily adapt to changing data structures. -\u200b Simplifies development \u2013 JSON format aligns with modern web & mobile applications. -\u200b Fast read/write performance \u2013 No complex joins, data stored"
    },
    "293": {
        "file": "Extended Notes - Document Databases & MongoDB.pdf",
        "page": 0,
        "chunk": "which require complex joins, document databases store related data in a single document, reducing query complexity. Advantages: -\u200b Flexible schema \u2013 Easily adapt to changing data structures. -\u200b Simplifies development \u2013 JSON format aligns with modern web & mobile applications. -\u200b Fast read/write performance \u2013 No complex joins, data stored in a hierarchical structure. -\u200b Scalability \u2013 Can scale out horizontally across multiple servers."
    },
    "294": {
        "file": "Extended Notes - Document Databases & MongoDB.pdf",
        "page": 1,
        "chunk": "Popular Document Databases -\u200b MongoDB \u2013 Most widely used document database, supports indexing, aggregation, and distributed scaling. -\u200b CouchDB \u2013 Focuses on offline sync and distributed data storage. -\u200b Firebase Firestore \u2013 Serverless, real-time NoSQL document store. -\u200b Amazon DocumentDB \u2013 Managed document database service by AWS, compatible with MongoDB. Use Cases of Document Databases 1.\u200b Content Management Systems (CMS) -\u200b Store articles, blog posts, and media content in JSON format. -\u200b Supports nested data, making it easy to retrieve full documents efficiently. 2.\u200b E-Commerce Platforms -\u200b Store product catalogs, orders, and customer profiles in a flexible schema. -\u200b Each product or order can have dynamic fields, avoiding rigid table constraints. 3.\u200b Real-Time Analytics -\u200b Store logs and events in a structured format. -\u200b Quickly analyze large volumes of semi-structured data. What is JSON? -\u200b JSON (JavaScript Object Notation) is a lightweight data-interchange format that is widely used for storing and exchanging data. It is a text-based, human-readable format that is easy for both humans to read and write and machines to parse and generate. Why JSON? -\u200b Simple and Readable \u2013 Uses a clear and structured syntax. -\u200b Language-Independent \u2013 Supported by almost all modern programming languages. -\u200b Efficient for Data Exchange \u2013 Commonly used in web APIs, databases, and configuration files. -\u200b Lightweight \u2013 Minimal syntax overhead compared to XML. JSON is Built on Two Core Structures 1.\u200b Name/Value Pairs (Key-Value Pairs / Objects) -\u200b Represented as a collection of key-value pairs, similar to dictionaries (Python), hashes (Ruby), objects (JavaScript), or associative arrays (PHP, Java). -\u200b Each key is a string, and each value can be a string, number, boolean, array, object, or null. 2.\u200b Ordered Lists of Values (Arrays) -\u200b A list of values, where each value can be of any JSON-supported type. -\u200b Similar to arrays"
    },
    "295": {
        "file": "Extended Notes - Document Databases & MongoDB.pdf",
        "page": 1,
        "chunk": "objects (JavaScript), or associative arrays (PHP, Java). -\u200b Each key is a string, and each value can be a string, number, boolean, array, object, or null. 2.\u200b Ordered Lists of Values (Arrays) -\u200b A list of values, where each value can be of any JSON-supported type. -\u200b Similar to arrays (JavaScript, Java, C), lists (Python), or vectors (C++). Why JSON is a Great Data Interchange Format -\u200b Universally Supported \u2013 Works with nearly all programming languages."
    },
    "296": {
        "file": "Extended Notes - Document Databases & MongoDB.pdf",
        "page": 2,
        "chunk": "-\u200b Easily Convertible \u2013 Can be transformed into objects in languages like JavaScript, Python, Java, and C#. -\u200b Widely Used in Web and APIs \u2013 JSON is the standard format for RESTful APIs. -\u200b Compact and Efficient \u2013 Less verbose than XML, making it faster to parse. Common Uses of JSON -\u200b Configuration files (config.json in applications). -\u200b Web APIs (REST, GraphQL, Firebase Firestore). -\u200b Database Storage (MongoDB, CouchDB). -\u200b Data exchange between web clients and servers (JavaScript fetch requests). Binary JSON? BSON 1.\u200b Binary-encoded serialization of a JSON-like document structure: -\u200b BSON is essentially a binary representation of JSON (JavaScript Object Notation). While JSON is a human-readable text format used to represent data, BSON is a more compact binary format that is optimized for storage and processing efficiency. -\u200b BSON retains the hierarchical structure of JSON documents (objects, arrays, etc.), but encodes them in a binary format for more efficient parsing, storage, and transmission. This makes it particularly useful in systems where performance and memory efficiency are critical, such as in databases. 2.\u200b Supports extended types not part of basic JSON (e.g., Date, BinaryData, etc.): -\u200b BSON is not limited to the basic JSON data types (strings, numbers, booleans, arrays, and objects). It extends JSON to support additional types that are often required in real-world applications. -\u200b Date: BSON includes a native Date type for efficiently representing timestamps, which is particularly useful for databases that need to store and query time-related data. -\u200b BinaryData: BSON also supports a binary data type, which is important for handling raw binary data, such as images or files. -\u200b Other types like ObjectId, Regular Expression, Decimal128, and more can be supported by BSON, making it versatile in handling different data formats. 3.\u200b Lightweight - keeps space overhead to a minimum: -\u200b One of"
    },
    "297": {
        "file": "Extended Notes - Document Databases & MongoDB.pdf",
        "page": 2,
        "chunk": "data type, which is important for handling raw binary data, such as images or files. -\u200b Other types like ObjectId, Regular Expression, Decimal128, and more can be supported by BSON, making it versatile in handling different data formats. 3.\u200b Lightweight - keeps space overhead to a minimum: -\u200b One of the key advantages of BSON over JSON is that it is more compact. JSON stores data as human-readable text, which can lead to a lot of redundant or unnecessary space usage (e.g., extra characters like quotes and braces). BSON, on the other hand, uses a binary format that eliminates this overhead. -\u200b BSON is designed to minimize space while maintaining sufficient metadata to describe the data structure, ensuring efficient storage and transmission. 4.\u200b Traversable - designed to be easily traversed, which is vitally important to a document DB:"
    },
    "298": {
        "file": "Extended Notes - Document Databases & MongoDB.pdf",
        "page": 3,
        "chunk": "-\u200b BSON is optimized for fast traversal and easy access to nested data structures. This is especially important for document-oriented databases (like MongoDB) where documents can have complex, deeply nested structures. -\u200b The format supports indexing of documents, allowing for rapid retrieval of information, and is designed to be traversed efficiently by the database engine. 5.\u200b Efficient - encoding and decoding must be efficient: -\u200b BSON is designed to allow for quick encoding (serialization) and decoding (deserialization) of data. This is essential for high-performance applications, especially when dealing with large volumes of data or when working with real-time systems. -\u200b The binary format allows for faster processing compared to text-based formats like JSON, which requires parsing text into data structures before it can be used. With BSON, the data is already in a binary format, so it can be accessed more quickly. 6.\u200b Supported by many modern programming languages: -\u200b BSON is widely supported by many programming languages, making it a versatile choice for developers working in different environments. Languages such as JavaScript, Python, Java, C++, and others have libraries or native support for working with BSON, particularly in the context of databases like MongoDB. -\u200b The wide support for BSON means that developers can easily integrate BSON into their applications without having to worry about compatibility or performance issues across platforms. XML (eXtensible Markup Language) 1.\u200b Precursor to JSON as a Data Exchange Format -\u200b Before JSON became the dominant format for data exchange, XML was widely used for storing and transmitting structured data between systems. -\u200b XML was particularly common in web services (SOAP-based APIs) and enterprise applications due to its structured, hierarchical nature. -\u200b Unlike JSON, which is lightweight and easy to parse, XML is more verbose, requiring additional tags for defining data structure, which can increase"
    },
    "299": {
        "file": "Extended Notes - Document Databases & MongoDB.pdf",
        "page": 3,
        "chunk": "storing and transmitting structured data between systems. -\u200b XML was particularly common in web services (SOAP-based APIs) and enterprise applications due to its structured, hierarchical nature. -\u200b Unlike JSON, which is lightweight and easy to parse, XML is more verbose, requiring additional tags for defining data structure, which can increase file size and parsing complexity. 2.\u200b XML + CSS \u2192 Web Pages That Separated Content and Formatting -\u200b XML was used in combination with CSS (Cascading Style Sheets) to create structured web pages where content and formatting were separated. -\u200b The idea was similar to the separation of HTML (for structure) and CSS (for styling) in modern web development. -\u200b By applying CSS styles to XML documents, users could control the appearance of data without altering the content itself. 3.\u200b Structurally Similar to HTML, but Tag Set is Extensible -\u200b XML shares a similar hierarchical structure with HTML, using opening (<tag>) and closing (</tag>) tags."
    },
    "300": {
        "file": "Extended Notes - Document Databases & MongoDB.pdf",
        "page": 4,
        "chunk": "-\u200b However, unlike HTML, XML does not have predefined tags. Instead, users can define their own tags, making it highly flexible for different applications. -\u200b This extensibility allows XML to be used for a wide range of applications, including document storage, configuration files, and data representation in various domains (e.g., RSS feeds, SVG graphics, and Microsoft Office file formats). XML-Related Tools/Technologies 1.\u200b XPath \u2013 A Syntax for Retrieving Specific Elements from an XML Document -\u200b XPath (XML Path Language) is used to navigate and select specific parts of an XML document. -\u200b It provides a way to query XML elements, attributes, and text nodes based on their hierarchical structure. -\u200b XPath expressions are commonly used in XSLT transformations and XQuery to filter or manipulate XML data. 2.\u200b XQuery \u2013 The SQL of XML -\u200b XQuery (XML Query Language) is a powerful language for querying and manipulating XML data, similar to how SQL is used for relational databases. -\u200b It allows searching, filtering, and transforming XML documents, making it ideal for XML-based databases and structured data retrieval. 3.\u200b DTD \u2013 Document Type Definition -\u200b DTD (Document Type Definition) defines the structure, elements, and attributes that are allowed in an XML document. -\u200b It ensures data consistency by validating whether an XML document conforms to a predefined structure. 4.\u200b XSLT \u2013 eXtensible Stylesheet Language Transformation -\u200b XSLT (Extensible Stylesheet Language Transformations) is used to convert XML documents into different formats, such as HTML, plain text, or another XML format. -\u200b It enables dynamic content transformation, making XML more useful for web and data processing applications. Why Document Databases? 1.\u200b Addressing the Impedance Mismatch Problem -\u200b The impedance mismatch problem arises when there is a disconnect between how data is structured in object-oriented (OO) programming languages and how it must be stored in"
    },
    "301": {
        "file": "Extended Notes - Document Databases & MongoDB.pdf",
        "page": 4,
        "chunk": "dynamic content transformation, making XML more useful for web and data processing applications. Why Document Databases? 1.\u200b Addressing the Impedance Mismatch Problem -\u200b The impedance mismatch problem arises when there is a disconnect between how data is structured in object-oriented (OO) programming languages and how it must be stored in relational databases. -\u200b Object-Oriented Programming (OOP): Objects in languages like Python, Java, or C# are hierarchical and support complex structures, including inheritance, composition, and nested types. -\u200b Relational Databases (RDBs): RDBs require data to be stored in flat, structured tables with rows and columns. Relationships between objects must be manually established through foreign keys and joins."
    },
    "302": {
        "file": "Extended Notes - Document Databases & MongoDB.pdf",
        "page": 5,
        "chunk": "-\u200b This means that saving an object in an RDB requires breaking it down into multiple tables and reassembling it when retrieving data. -\u200b In a relational database, we must store this data as multiple tables: -\u200b BlogPost Table (ID, Title, Author), Tags Table (BlogPostID, TagName), Comments Table (BlogPostID, User, Text) -\u200b To retrieve the full blog post, we would need multiple JOIN queries across these tables, which can be inefficient and complex to manage. 2.\u200b The Structure of a Document is Self-Describing -\u200b In a relational database, we need a predefined schema that specifies column names, data types, and relationships before inserting any data. -\u200b Document databases are schema-less, meaning that each document can have its own structure without requiring predefined schemas. -\u200b This flexibility is useful when dealing with evolving data models (e.g., adding new fields without altering the database schema). 3.\u200b Well-Aligned with JSON/XML Transport Layers -\u200b Many modern applications, particularly web APIs, use JSON or XML to send and receive data. -\u200b Since document databases store data in JSON-like formats (JSON, BSON, or XML), they integrate seamlessly with APIs and web services. -\u200b No need for conversion between objects, JSON, and database storage, unlike relational databases that require object-relational mapping (ORM) tools. MongoDB MongoDB is a widely used NoSQL document database designed for scalability, flexibility, and performance. It emerged as a response to the limitations of relational databases when handling large-scale, high-throughput applications. 1.\u200b Origins: Why MongoDB Was Created -\u200b MongoDB was founded in 2007 by Dwight Merriman, Eliot Horowitz, and Kevin Ryan, who were part of DoubleClick, a digital advertising company later acquired by Google. -\u200b DoubleClick needed to serve over 400,000 ads per second, but traditional relational databases struggled with the high-speed, high-volume data needs. -\u200b Challenges with relational databases: -\u200b Scalability: RDBMS struggled"
    },
    "303": {
        "file": "Extended Notes - Document Databases & MongoDB.pdf",
        "page": 5,
        "chunk": "Dwight Merriman, Eliot Horowitz, and Kevin Ryan, who were part of DoubleClick, a digital advertising company later acquired by Google. -\u200b DoubleClick needed to serve over 400,000 ads per second, but traditional relational databases struggled with the high-speed, high-volume data needs. -\u200b Challenges with relational databases: -\u200b Scalability: RDBMS struggled with horizontal scaling for high traffic loads. -\u200b Complex queries: SQL-based joins became a bottleneck at massive scales. -\u200b Schema rigidity: Frequent schema changes caused downtime and required migrations. -\u200b Seeing these challenges, they set out to build a new kind of database designed for modern applications."
    },
    "304": {
        "file": "Extended Notes - Document Databases & MongoDB.pdf",
        "page": 6,
        "chunk": "2.\u200b Why the Name \"MongoDB\"? -\u200b \"MongoDB\" comes from \"Humongous Database\", emphasizing its ability to handle large-scale data efficiently. -\u200b Unlike traditional databases, MongoDB uses a flexible document model instead of rigid tables and schemas. 3.\u200b Key Innovations of MongoDB (MongoDB introduced several features that made it a preferred choice for modern applications): -\u200b Document-Oriented Storage \u2013 Stores data in JSON-like BSON (Binary JSON) format, making it more natural for object-oriented programming. -\u200b Schema Flexibility \u2013 No predefined schema is required; each document can have different fields. -\u200b Horizontal Scalability \u2013 Uses sharding to distribute data across multiple servers, handling high traffic and large datasets. -\u200b High Performance \u2013 Avoids complex joins and supports indexing for fast lookups. -\u200b Replication & High Availability \u2013 Uses replica sets to ensure fault tolerance and automatic failover. 4.\u200b MongoDB Atlas: DocumentDB as a Service (2016) -\u200b In 2016, MongoDB released MongoDB Atlas, a fully managed cloud database service that made it easier to deploy and scale MongoDB without managing infrastructure. -\u200b Benefits of MongoDB Atlas: -\u200b Automated scaling and performance tuning. -\u200b Multi-cloud support (AWS, Azure, GCP). -\u200b Built-in security and backup features. -\u200b Integration with modern data tools and analytics platforms. 5.\u200b MongoDB's Role in Modern Applications (MongoDB has become a dominant database for): -\u200b Big Data & Analytics \u2013 Handles massive data volumes efficiently. -\u200b Real-Time Applications \u2013 Used in ad tech, IoT, and financial services. -\u200b Microservices & Cloud-Based Systems \u2013 Provides a flexible and scalable data backend. -\u200b Content Management & E-Commerce \u2013 Supports complex, dynamic data structures like catalogs and user-generated content. MongoDB Documents - Schema Flexibility & Dynamic Structure One of MongoDB\u2019s biggest advantages is its schema-less nature, meaning that documents in a collection do not require a predefined schema. This allows for high flexibility and makes"
    },
    "305": {
        "file": "Extended Notes - Document Databases & MongoDB.pdf",
        "page": 6,
        "chunk": "Management & E-Commerce \u2013 Supports complex, dynamic data structures like catalogs and user-generated content. MongoDB Documents - Schema Flexibility & Dynamic Structure One of MongoDB\u2019s biggest advantages is its schema-less nature, meaning that documents in a collection do not require a predefined schema. This allows for high flexibility and makes MongoDB well-suited for applications with evolving data structures. 1.\u200b What is a MongoDB Document? -\u200b In MongoDB, data is stored as documents in a JSON-like BSON (Binary JSON) format."
    },
    "306": {
        "file": "Extended Notes - Document Databases & MongoDB.pdf",
        "page": 7,
        "chunk": "-\u200b Each document is a self-contained unit of data, similar to an object in object-oriented programming. -\u200b Documents are grouped into collections, which are analogous to tables in relational databases. -\u200b Unlike relational databases, MongoDB does not enforce a fixed schema for documents. 2.\u200b No Predefined Schema: Why is this Useful? -\u200b Flexibility \u2013 Developers can add, remove, or modify fields in documents without altering a predefined schema. -\u200b Faster Iteration \u2013 Applications can evolve without requiring database migrations. -\u200b Different Data Structures \u2013 Documents in the same collection can have different fields and types. 3.\u200b When is Schema Flexibility Beneficial? -\u200b MongoDB\u2019s schema-less approach is ideal for: 1.\u200b Applications with frequently changing data models (e.g., startups, evolving projects). 2.\u200b Content management systems (CMS) where different types of content require different fields. 3.\u200b IoT and event logging where incoming data structures may vary. 4.\u200b User profiles & e-commerce, where different users or products may have different attributes. RDBMS MongoDB Database Database Table/View Collection Row Document Column Field Index Index Join Embedded Document Foreign Key Reference MongoDB Features: Why It\u2019s a Powerful NoSQL Database MongoDB offers a range of features that make it a high-performance, scalable, and flexible document database. Let\u2019s break down some of its key capabilities: 1.\u200b Rich Query Support -\u200b MongoDB provides powerful and flexible querying capabilities beyond simple key-value lookups. -\u200b It supports CRUD operations (Create, Read, Update, Delete) with complex query options, including:"
    },
    "307": {
        "file": "Extended Notes - Document Databases & MongoDB.pdf",
        "page": 8,
        "chunk": "-\u200b Filtering \u2013 Find documents using field values, ranges, and conditions. -\u200b Aggregation Framework \u2013 Perform complex data processing like grouping, filtering, and transformations (similar to SQL\u2019s GROUP BY). -\u200b Full-Text Search \u2013 Built-in support for text indexing and searching. -\u200b Geospatial Queries \u2013 Search by location, distance, or geospatial relationships. 2.\u200b Indexing -\u200b MongoDB supports primary and secondary indexes to improve query performance. -\u200b Without indexes, MongoDB would have to scan every document in a collection to find a match. -\u200b Types of Indexes: -\u200b Single Field Index \u2013 Speeds up lookups on a specific field. -\u200b Compound Index \u2013 Optimized searches across multiple fields. -\u200b Text Index \u2013 Enables efficient full-text search. -\u200b Geospatial Index \u2013 Supports location-based queries. 3.\u200b Replication (High Availability & Fault Tolerance) -\u200b MongoDB ensures high availability by supporting replica sets, which automatically maintain multiple copies of data across different servers. -\u200b How it works: -\u200b One primary node handles all writes. -\u200b Multiple secondary nodes replicate the data in real-time. -\u200b If the primary fails, MongoDB automatically elects a new primary (automatic failover). -\u200b This ensures zero downtime in case of server failures. 4.\u200b Load Balancing (Sharding for Horizontal Scaling) -\u200b MongoDB has built-in load balancing through sharding, which allows databases to scale horizontally. -\u200b Why use sharding? 1.\u200b Handles large datasets by distributing data across multiple servers. 2.\u200b Supports high-throughput applications with heavy read/write operations. 3.\u200b Prevents bottlenecks by dividing query loads among different servers. -\u200b How Sharding Works: 1.\u200b Data is split into chunks based on a shard key (e.g., userID). 2.\u200b Chunks are distributed across multiple servers (shards). 3.\u200b A Mongos query router directs queries to the correct shard. 4.\u200b As data grows, new shards can be added dynamically. MongoDB Versions: Understanding the Different Offerings MongoDB offers several versions of"
    },
    "308": {
        "file": "Extended Notes - Document Databases & MongoDB.pdf",
        "page": 8,
        "chunk": "split into chunks based on a shard key (e.g., userID). 2.\u200b Chunks are distributed across multiple servers (shards). 3.\u200b A Mongos query router directs queries to the correct shard. 4.\u200b As data grows, new shards can be added dynamically. MongoDB Versions: Understanding the Different Offerings MongoDB offers several versions of its database, each catering to different needs and use cases. Here\u2019s a breakdown of the three primary versions: MongoDB Atlas, MongoDB Enterprise, and MongoDB Community:"
    },
    "309": {
        "file": "Extended Notes - Document Databases & MongoDB.pdf",
        "page": 9,
        "chunk": "1.\u200b MongoDB Atlas -\u200b Fully Managed MongoDB Service: MongoDB Atlas is a Database as a Service (DBaaS), providing a fully managed cloud-based MongoDB experience. -\u200b What it offers: -\u200b Fully managed \u2013 MongoDB Atlas handles all database management tasks such as provisioning, monitoring, scaling, and backups. -\u200b Multi-Cloud \u2013 Available across all major cloud providers (AWS, Azure, Google Cloud), allowing for flexible deployment options. -\u200b Automated Scaling \u2013 Atlas can automatically scale resources as demand increases. -\u200b Built-in Security \u2013 Offers data encryption at rest, VPC peering, advanced access controls, and more. -\u200b Performance Optimization \u2013 Includes real-time performance monitoring and alerts to help optimize database performance. -\u200b Backup and Disaster Recovery \u2013 Automated backups with easy restoration options. -\u200b Ideal Use Cases: -\u200b MongoDB Atlas is perfect for organizations that want to focus on development and scaling without worrying about managing infrastructure. It is commonly used by startups, enterprises, and developers who need cloud-native database solutions. -\u200b Key Features of MongoDB Atlas: -\u200b Global distribution \u2013 Deploy MongoDB clusters in multiple regions for reduced latency and better redundancy. -\u200b Data Insights \u2013 Offers built-in analytics tools like MongoDB Charts to visualize your data directly. -\u200b Serverless Options \u2013 MongoDB Atlas also offers serverless databases, where you only pay for the compute and storage you use. 2.\u200b MongoDB Enterprise -\u200b Subscription-Based, Self-Managed Version: MongoDB Enterprise is the enterprise-grade, subscription-based version of MongoDB, designed for large organizations with self-managed infrastructure needs. -\u200b What it offers: -\u200b Advanced Security \u2013 Includes LDAP integration, Kerberos authentication, and auditing capabilities. -\u200b Commercial Support \u2013 Includes 24/7 enterprise-level support and professional services. -\u200b Increased Scalability \u2013 MongoDB Enterprise is optimized for larger, more complex deployments with advanced features such as sharding and replication. -\u200b Additional Tools \u2013 Comes with additional features such as MongoDB Ops"
    },
    "310": {
        "file": "Extended Notes - Document Databases & MongoDB.pdf",
        "page": 9,
        "chunk": "integration, Kerberos authentication, and auditing capabilities. -\u200b Commercial Support \u2013 Includes 24/7 enterprise-level support and professional services. -\u200b Increased Scalability \u2013 MongoDB Enterprise is optimized for larger, more complex deployments with advanced features such as sharding and replication. -\u200b Additional Tools \u2013 Comes with additional features such as MongoDB Ops Manager (for automation, backups, and monitoring) and MongoDB Compass (for graphical data exploration)."
    },
    "311": {
        "file": "Extended Notes - Document Databases & MongoDB.pdf",
        "page": 10,
        "chunk": "-\u200b Compliance \u2013 Features designed to meet compliance requirements like HIPAA, SOC2, and more. -\u200b Ideal Use Cases: -\u200b MongoDB Enterprise is typically used by large organizations that require extra security, compliance, and support for mission-critical applications. It is suitable for companies that prefer to manage their own infrastructure while benefiting from additional tools and services. -\u200b Key Features of MongoDB Enterprise: -\u200b MongoDB Ops Manager \u2013 Automates operational tasks such as provisioning, backups, and performance tuning. -\u200b Commercial Licensing \u2013 Offers full access to MongoDB\u2019s proprietary features and the ability to deploy on-premises or in the cloud. 3.\u200b MongoDB Community -\u200b Free, Source-Available Version: MongoDB Community is the open-source version of MongoDB, which is completely free to use and self-managed. -\u200b What it offers: -\u200b Core MongoDB Features \u2013 The Community edition provides all of MongoDB\u2019s core features, such as collections, documents, indexing, and queries. -\u200b Self-Managed \u2013 MongoDB Community requires users to manage their own infrastructure, including installation, scaling, backups, and monitoring. -\u200b Source-Available \u2013 The source code for MongoDB Community is publicly available, allowing users to inspect and modify it if needed. -\u200b Community Support \u2013 MongoDB Community users can rely on online documentation and community forums for support. -\u200b Ideal Use Cases: -\u200b MongoDB Community is suitable for developers, small teams, or anyone working on personal projects or proof-of-concept applications. It\u2019s a great choice for startups, educational purposes, and low-cost environments where enterprise-grade support and features are not required. -\u200b Key Features of MongoDB Community: -\u200b Open Source \u2013 Fully free and open-source. -\u200b No Commercial Features \u2013 Does not include advanced features such as LDAP integration, advanced monitoring, or commercial support."
    },
    "312": {
        "file": "Extended Notes - Document Databases & MongoDB.pdf",
        "page": 10,
        "chunk": "Open Source \u2013 Fully free and open-source. -\u200b No Commercial Features \u2013 Does not include advanced features such as LDAP integration, advanced monitoring, or commercial support."
    },
    "313": {
        "file": "Extended Notes - UCI AVL Trees.pdf",
        "page": 0,
        "chunk": "ICS 46 Spring 2022 | News | Course Reference | Schedule | Project Guide | Notes and Examples | Reinforcement Exercises | Grade Calculator | About Alex ICS 46 Spring 2022 Notes and Examples: AVL Trees Why we must care about binary search tree balancing We've seen previously that the performance characteristics of binary search trees can vary rather wildly, and that they're mainly dependent on the shape of the tree, with the height of the tree being the key determining factor. By definition, binary search trees restrict what keys are allowed to present in which nodes \u2014 smaller keys have to be in left subtrees and larger keys in right subtrees \u2014 but they specify no restriction on the tree's shape, meaning that both of these are perfectly legal binary search trees containing the keys 1, 2, 3, 4, 5, 6, and 7. Yet, while both of these are legal, one is better than the other, because the height of the first tree (called a perfect binary tree) is smaller than the height of the second (called a degenerate tree). These two shapes represent the two extremes \u2014 the best and worst possible shapes for a binary search tree containing seven keys. Of course, when all you have is a very small number of keys like this, any shape will do. But as the number of keys grows, the distinction between these two tree shapes becomes increasingly vital. What's more, the degenerate shape isn't even necessarily a rare edge case: It's what you get when you start with an empty tree and add keys that are already in order, which is a surprisingly common scenario in real-world programs. For example, one very obvious algorithm for generating unique integer keys \u2014 when all you care about is that they're unique"
    },
    "314": {
        "file": "Extended Notes - UCI AVL Trees.pdf",
        "page": 0,
        "chunk": "case: It's what you get when you start with an empty tree and add keys that are already in order, which is a surprisingly common scenario in real-world programs. For example, one very obvious algorithm for generating unique integer keys \u2014 when all you care about is that they're unique \u2014 is to generate them sequentially. What's so bad about a degenerate tree, anyway? Just looking at a picture of a degenerate tree, your intuition should already be telling you that something is amiss. In particular, if you tilt your head 45 degrees to the right, they look just like linked lists; that perception is no accident, as they behave like them, too (except that they're more complicated, to boot!). From a more analytical perspective, there are three results that should give us pause: Every time you perform a lookup in a degenerate binary search tree, it will take O(n) time, because it's possible that you'll have to reach every node in the tree before you're done. As n grows, this is a heavy burden to bear. If you implement your lookup recursively, you might also be using O(n) memory, too, as you might end up with as many as n frames on your run-time stack \u2014 one for every recursive call. There are ways to mitigate this \u2014 for example, some kinds of carefully-written recursion (in some programming languages, including C++) can avoid run-time stack growth as you recurse \u2014 but it's still a sign of potential trouble. The time it will take you to build the degenerate tree will also be prohibitive. If you start with an empty binary search tree and add keys to it in order, how long does it take to do it? The first key you add will go directly to the root. You"
    },
    "315": {
        "file": "Extended Notes - UCI AVL Trees.pdf",
        "page": 0,
        "chunk": "time it will take you to build the degenerate tree will also be prohibitive. If you start with an empty binary search tree and add keys to it in order, how long does it take to do it? The first key you add will go directly to the root. You could think of this as taking a single step: creating the node. The second key you add will require you to look at the root node, then take one step to the right. You could think of this as taking two steps. Each subsequent key you add will require one more step than the one before it. The total number of steps it would take to add n keys would be determined by the sum 1 + 2 + 3 + ... + n. This sum, which we'll see several times throughout this course, is equal to n(n + 1) / 2. So, the total number of steps to build the entire tree would be \u0398(n2). Overall, when n gets large, the tree would be hideously expensive to build, and then every subsequent search would be painful, as well. So this, in general, is a situation we need to be sure to avoid, or else we should probably consider a data structure other than a binary search tree; the worst case is simply too much of a burden to bear if n might get large. But if we can find a way to control the tree's shape more carefully, to force it to remain more balanced, we'll be fine. The question, of course, is how to do it, and, as importantly, whether we can do it while keeping the cost low enough that it doesn't outweigh the benefit. Aiming for perfection The best goal for us to shoot for would"
    },
    "316": {
        "file": "Extended Notes - UCI AVL Trees.pdf",
        "page": 0,
        "chunk": "force it to remain more balanced, we'll be fine. The question, of course, is how to do it, and, as importantly, whether we can do it while keeping the cost low enough that it doesn't outweigh the benefit. Aiming for perfection The best goal for us to shoot for would be to maintain perfection. In other words, every time we insert a key into our binary search tree, it would ideally still be a perfect binary tree, in which case we'd know that the height of the tree would always be \u0398(log n), with a commensurate effect on performance. However, when we consider this goal, a problem emerges almost immediately. The following are all perfect binary trees, by definition: The perfect binary trees pictured above have 1, 3, 7, and 15 nodes respectively, and are the only possible perfect shapes for binary trees with that number of nodes. The problem, though, lies in the fact that there is no valid perfect binary tree with 2 nodes, or with 4, 5, 6, 8, 9, 10, 11, 12, 13, or 14 nodes. So, generally, it's impossible for us to guarantee that a binary search tree will always be \"perfect,\" by our definition, because there's simply no way to represent most numbers of keys. So, first thing's first: We'll need to relax our definition of \"perfection\" to accommodate every possible number of keys we might want to store."
    },
    "317": {
        "file": "Extended Notes - UCI AVL Trees.pdf",
        "page": 1,
        "chunk": "Complete binary trees A somewhat more relaxed notion of \"perfection\" is something called a complete binary tree, which is defined as follows. A complete binary tree of height h is a binary tree where: If h = 0, its left and right subtrees are empty. If h > 0, one of two things is true: The left subtree is a perfect binary tree of height h \u2212 1 and the right subtree is a complete binary tree of height h \u2212 1 The left subtree is a complete binary tree of height h \u2212 1 and the right subtree is a perfect binary tree of height h \u2212 2 That can be a bit of a mind-bending definition, but it actually leads to a conceptually simple result: On every level of a complete binary tree, every node that could possibly be present will be, except the last level might be missing nodes, but if it is missing nodes, the nodes that are there will be as far to the left as possible. The following are all complete binary trees: Furthermore, these are the only possible complete binary trees with these numbers of nodes in them; any other arrangement of, say, 6 keys besides the one shown above would violate the definition. We've seen that the height of a perfect binary tree is \u0398(log n). It's not a stretch to see that the height a complete binary tree will be \u0398(log n), as well, and we'll accept that via our intuition for now and proceed. All in all, a complete binary tree would be a great goal for us to attain: If we could keep the shape of our binary search trees complete, we would always have binary search trees with height \u0398(log n). The cost of maintaining completeness The trouble,"
    },
    "318": {
        "file": "Extended Notes - UCI AVL Trees.pdf",
        "page": 1,
        "chunk": "for now and proceed. All in all, a complete binary tree would be a great goal for us to attain: If we could keep the shape of our binary search trees complete, we would always have binary search trees with height \u0398(log n). The cost of maintaining completeness The trouble, of course, is that we need an algorithm for maintaining completeness. And before we go to the trouble of trying to figure one out, we should consider whether it's even worth our time. What can we deduce about the cost of maintaining completeness, even if we haven't figured out an algorithm yet? One example demonstrates a very big problem. Suppose we had the binary search tree on the left \u2014 which is complete, by our definition \u2014 and we wanted to insert the key 1 into it. If so, we would need an algorithm that would transform the tree on the left into the tree on the right. The tree on the right is certainly complete, so this would be the outcome we'd want. But consider what it would take to do it. Every key in the tree had to move! So, no matter what algorithm we used, we would still have to move every key. If there are n keys in the tree, that would take \u03a9(n) time \u2014 moving n keys takes at least linear time, even if you have the best possible algorithm for moving them; the work still has to get done. So, in the worst case, maintaining completeness after a single insertion requires \u03a9(n) time. Unfortunately, this is more time than we ought to be spending on maintaining balance. This means we'll need to come up with a compromise; as is often the case when we learn or design algorithms, our willingness to tolerate an"
    },
    "319": {
        "file": "Extended Notes - UCI AVL Trees.pdf",
        "page": 1,
        "chunk": "case, maintaining completeness after a single insertion requires \u03a9(n) time. Unfortunately, this is more time than we ought to be spending on maintaining balance. This means we'll need to come up with a compromise; as is often the case when we learn or design algorithms, our willingness to tolerate an imperfect result that's still \"good enough\" for our uses will often lead to an algorithm that is much faster than one that achieves a perfect result. So what would a \"good enough\" result be? What is a \"good\" balance condition Our overall goal is for lookups, insertions, and removals from a binary search tree to require O(log n) time in every case, rather than letting them degrade to a worst-case behavior of O(n). To do that, we need to decide on a balance condition, which is to say that we need to understand what shape is considered well-enough balanced for our purposes, even if not perfect. A \"good\" balance condition has two properties: The height of a binary search tree meeting the condition is \u0398(log n). It takes O(log n) time to re-balance the tree on insertions and removals. In other words, it guarantees that the height of the tree is still logarithmic, which will give us logarithmic-time lookups, and the time spent re-balancing won't exceed the logarithmic time we would otherwise spend on an insertion or removal when the tree has logarithmic height. The cost won't outweigh the benefit. Coming up with a balance condition like this on our own is a tall task, but we can stand on the shoulders of the giants who came before us, with the definition above helping to guide us toward an understanding of whether we've found what we're looking for. A compromise: AVL trees There are a few well-known approaches for maintaining"
    },
    "320": {
        "file": "Extended Notes - UCI AVL Trees.pdf",
        "page": 1,
        "chunk": "own is a tall task, but we can stand on the shoulders of the giants who came before us, with the definition above helping to guide us toward an understanding of whether we've found what we're looking for. A compromise: AVL trees There are a few well-known approaches for maintaining binary search trees in a state of near-balance that meets our notion of a \"good\" balance condition. One of them is called an AVL tree, which we'll explore here. Others, which are outside the scope of this course, include red-black trees (which meet our definition of \"good\") and splay trees (which don't always meet our definition of \"good\", but do meet it on an amortized basis), but we'll stick with the one solution to the problem for now. AVL trees AVL trees are what you might called \"nearly balanced\" binary search trees. While they certainly aren't as perfectly-balanced as possible, they nonetheless achieve the goals we've decided on: maintaining logarithmic height at no more than logarithmic cost. So, what makes a binary search tree \"nearly balanced\" enough to be considered an AVL tree? The core concept is embodied by something called the AVL property. We say that a node in a binary search tree has the AVL property if the heights of its left and right subtrees differ by no more than 1. In other words, we tolerate a certain amount of imbalance \u2014 heights of subtrees can be slightly different, but no more than that \u2014 in hopes that we can more efficiently maintain it. Since we're going to be comparing heights of subtrees, there's one piece of background we need to consider. Recall that the height of a tree is the length of its longest path. By definition, the height of a tree with just a root node"
    },
    "321": {
        "file": "Extended Notes - UCI AVL Trees.pdf",
        "page": 1,
        "chunk": "we can more efficiently maintain it. Since we're going to be comparing heights of subtrees, there's one piece of background we need to consider. Recall that the height of a tree is the length of its longest path. By definition, the height of a tree with just a root node (and empty subtrees) would then be zero. But what about a tree that's totally empty? To maintain a clear pattern, relative to other tree heights, we'll say that the height of an empty tree is -1. This means that a node with, say, a childless left child and no right child would still be considered balanced. This leads us, finally, to the definition of an AVL tree:"
    },
    "322": {
        "file": "Extended Notes - UCI AVL Trees.pdf",
        "page": 2,
        "chunk": "An AVL tree is a binary search tree in which all nodes have the AVL property. Below are a few binary trees, two of which are AVL and two of which are not. The thing to keep in mind about AVL is that it's not a matter of squinting at a tree and deciding whether it \"looks\" balanced. There's a precise definition, and the two trees above that don't meet that definition fail to meet it because they each have at least one node (marked in the diagrams by a dashed square) that doesn't have the AVL property. AVL trees, by definition, are required to meet the balance condition after every operation; every time you insert or remove a key, every node in the tree should have the AVL property. To meet that requirement, we need to restructure the tree periodically, essentially detecting and correcting imbalance whenever and wherever it happens. To do that, we need to rearrange the tree in ways that improve its shape without losing the essential ordering property of a binary search tree: smaller keys toward the left, larger ones toward the right. Rotations Re-balancing of AVL trees is achieved using what are called rotations, which, when used at the proper times, efficiently improve the shape of the tree by altering a handful of pointers. There are a few kinds of rotations; we should first understand how they work, then focus our attention on when to use them. The first kind of rotation is called an LL rotation, which takes the tree on the left and turns it into the tree on the right. The circle with A and B written in them are each a single node containing a single key; the triangles with T1, T2, and T3 written in them are arbitrary subtrees, which"
    },
    "323": {
        "file": "Extended Notes - UCI AVL Trees.pdf",
        "page": 2,
        "chunk": "LL rotation, which takes the tree on the left and turns it into the tree on the right. The circle with A and B written in them are each a single node containing a single key; the triangles with T1, T2, and T3 written in them are arbitrary subtrees, which may be empty or may contain any number of nodes (but which are, themselves, binary search trees). It's important to remember that both of these trees \u2014 before and after \u2014 are binary search trees; the rotation doesn't harm the ordering of the keys in nodes, because the subtrees T1, T2, and T3 maintain the appropriate positions relative to the keys A and B: All keys in T1 are smaller than A. All keys in T2 are larger than A and smaller than B. All keys in T3 are larger than B. Performing this rotation would be a simple matter of adjusting a few pointers \u2014 notably, a constant number of pointers, no matter how many nodes are in the tree, which means that this rotation would run in \u0398(1) time: B's parent would now point to A where it used to point to B A's right child would now be B instead of the root of T2 B's left child would now be the root of T2 instead of A A second kind of rotation is an RR rotation, which makes a similar adjustment. Note that an RR rotation is the mirror image of an LL rotation. A third kind of rotation is an LR rotation, which makes an adjustment that's slightly more complicated. An LR rotation requires five pointer updates instead of three, but this is still a constant number of changes and runs in \u0398(1) time. Finally, there is an RL rotation, which is the mirror image"
    },
    "324": {
        "file": "Extended Notes - UCI AVL Trees.pdf",
        "page": 2,
        "chunk": "kind of rotation is an LR rotation, which makes an adjustment that's slightly more complicated. An LR rotation requires five pointer updates instead of three, but this is still a constant number of changes and runs in \u0398(1) time. Finally, there is an RL rotation, which is the mirror image of an LR rotation."
    },
    "325": {
        "file": "Extended Notes - UCI AVL Trees.pdf",
        "page": 3,
        "chunk": "Once we understand the mechanics of how rotations work, we're one step closer to understanding AVL trees. But these rotations aren't arbitrary; they're used specifically to correct imbalances that are detected after insertions or removals. An insertion algorithm Inserting a key into an AVL tree starts out the same way as insertion into a binary search tree: Perform a lookup. If you find the key already in the tree, you're done, because keys in a binary search tree must be unique. When the lookup terminates without the key being found, add a new node in the appropriate leaf position where the lookup ended. The problem is that adding the new node introduced the possibility of an imbalance. For example, suppose we started with this AVL tree: and then we inserted the key 35 into it. A binary search tree insertion would give us this as a result: But this resulting tree is not an AVL tree, because the node containing the key 40 does not have the AVL property, because the difference in the heights of its subtrees is 2. (Its left subtree has height 1, its right subtree \u2014 which is empty \u2014 has height -1.) What can we do about it? The answer lies in the following algorithm, which we perform after the normal insertion process: Work your way back up the tree from the position where you just added a node. (This could be quite simple if the insertion was done recursively.) Compare the heights of the left and right subtrees of each node. When they differ by more than 1, choose a rotation that will fix the imbalance. Note that comparing the heights of the left and right subtrees would be quite expensive if you didn't already know what they were. The solution to this problem"
    },
    "326": {
        "file": "Extended Notes - UCI AVL Trees.pdf",
        "page": 3,
        "chunk": "left and right subtrees of each node. When they differ by more than 1, choose a rotation that will fix the imbalance. Note that comparing the heights of the left and right subtrees would be quite expensive if you didn't already know what they were. The solution to this problem is for each node to store its height (i.e., the height of the subtree rooted there). This can be cheaply updated after every insertion or removal as you unwind the recursion. The rotation is chosen considering the two links along the path below the node where the imbalance is, heading back down toward where you inserted a node. (If you were wondering where the names LL, RR, LR, and RL come from, this is the answer to that mystery.) If the two links are both to the left, perform an LL rotation rooted where the imbalance is. If the two links are both to the right, perform an RR rotation rooted where the imbalance is. If the first link is to the left and the second is to the right, perform an LR rotation rooted where the imbalance is. If the first link is to the right and the second is to the left, perform an RL rotation rooted where the imbalance is. It can be shown that any one of these rotations \u2014 LL, RR, LR, or RL \u2014 will correct any imbalance brought on by inserting a key. In this case, we'd perform an LR rotation \u2014 the first two links leading from 40 down toward 35 are a Left and a Right \u2014 rooted at 40, which would correct the imbalance, and the tree would be rearranged to look like this: Compare this to the diagram describing an LR rotation: The node containing 40 is C The"
    },
    "327": {
        "file": "Extended Notes - UCI AVL Trees.pdf",
        "page": 3,
        "chunk": "first two links leading from 40 down toward 35 are a Left and a Right \u2014 rooted at 40, which would correct the imbalance, and the tree would be rearranged to look like this: Compare this to the diagram describing an LR rotation: The node containing 40 is C The node containing 30 is A The node containing 35 is B The (empty) left subtree of the node containing 30 is T1"
    },
    "328": {
        "file": "Extended Notes - UCI AVL Trees.pdf",
        "page": 4,
        "chunk": "The (empty) left subtree of the node containing 35 is T2 The (empty) right subtree of the node containing 35 is T3 The (empty) right subtree of the node containing 40 is T4 After the rotation, we see what we'd expect: The node B, which in our example contained 35, is now the root of the newly-rotated subtree The node A, which in our example contained 30, is now the left child of the root of the newly-rotated subtree The node C, which in our example contained 40, is now the right child of the root of the newly-rotated subtree The four subtrees T1, T2, T3, and T4 were all empty, so they are still empty. Note, too, that the tree is more balanced after the rotation than it was before. This is no accident; a single rotation (LL, RR, LR, or RL) is all that's necessary to correct an imbalance introduced by the insertion algorithm. A removal algorithm Removals are somewhat similar to insertions, in the sense that you would start with the usual binary search tree removal algorithm, then find and correct imbalances while the recursion unwinds. The key difference is that removals can require more than one rotation to correct imbalances, but will still only require rotations on the path back up to the root from where the removal occurred \u2014 so, generally, O(log n) rotations. Asymptotic analysis The key question here is What is the height of an AVL tree with n nodes? If the answer is \u0398(log n), then we can be certain that lookups, insertions, and removals will take O(log n) time. How can we be so sure? Lookups would be O(log n) because they're the same as they are in a binary search tree that doesn't have the AVL property. If the height"
    },
    "329": {
        "file": "Extended Notes - UCI AVL Trees.pdf",
        "page": 4,
        "chunk": "is \u0398(log n), then we can be certain that lookups, insertions, and removals will take O(log n) time. How can we be so sure? Lookups would be O(log n) because they're the same as they are in a binary search tree that doesn't have the AVL property. If the height of the tree is \u0398(log n), lookups will run in O(log n) time. Insertions and removals, despite being slightly more complicated in an AVL tree, do their work by traversing a single path in the tree \u2014 potentially all the way down to a leaf position, then all the way back up. If the length of the longest path \u2014 that's what the height of a tree is! \u2014 is \u0398(log n), then we know that none of these paths is longer than that, so insertions and removals will take O(log n) time. So we're left with that key question. What is the height of an AVL tree with n nodes? (If you're not curious, you can feel free to just assume this; if you want to know more, keep reading.) What is the height of an AVL tree with n nodes? (Optional) The answer revolves around noting how many nodes, at minimum, could be in a binary search tree of height n and still have it be an AVL tree. It turns out AVL trees of height n \u2265 2 that have the minimum number of nodes in them all share a similar property: The AVL tree with height h \u2265 2 with the minimum number of nodes consists of a root node with two subtrees, one of which is an AVL tree with height h \u2212 1 with the minimum number of nodes, the other of which is an AVL tree with height h \u2212 2 with the"
    },
    "330": {
        "file": "Extended Notes - UCI AVL Trees.pdf",
        "page": 4,
        "chunk": "h \u2265 2 with the minimum number of nodes consists of a root node with two subtrees, one of which is an AVL tree with height h \u2212 1 with the minimum number of nodes, the other of which is an AVL tree with height h \u2212 2 with the minimum number of nodes. Given that observation, we can write a recurrence that describes the number of nodes, at minimum, in an AVL tree of height h. M(0) = 1 When height is 0, minimum number of nodes is 1 (a root node with no children) M(1) = 2 When height is 1, minimum number of nodes is 2 (a root node with one child and not the other) M(h) = 1 + M(h - 1) + M(h - 2) While the repeated substitution technique we learned previously isn't a good way to try to solve this particular recurrence, we can prove something interesting quite easily. We know for sure that AVL trees with larger heights have a bigger minimum number of nodes than AVL trees with smaller heights \u2014 that's fairly self-explanatory \u2014 which means that we can be sure that 1 + M(h \u2212 1) \u2265 M(h \u2212 2). Given that, we can conclude the following: M(h) \u2265 2M(h - 2) We can then use the repeated substitution technique to determine a lower bound for this recurrence: M(h) \u2265 2M(h - 2) \u2265 2(2M(h - 4)) \u2265 4M(h - 4) \u2265 4(2M(h - 6)) \u2265 8M(h - 6) ... \u2265 2jM(h - 2j) We could prove this by induction on j, but we'll accept it on faith let j = h/2 \u2265 2h/2M(h - h) \u2265 2h/2M(0) M(h) \u2265 2h/2 So, we've shown that the minimum number of nodes that can be present in an AVL tree of"
    },
    "331": {
        "file": "Extended Notes - UCI AVL Trees.pdf",
        "page": 4,
        "chunk": "... \u2265 2jM(h - 2j) We could prove this by induction on j, but we'll accept it on faith let j = h/2 \u2265 2h/2M(h - h) \u2265 2h/2M(0) M(h) \u2265 2h/2 So, we've shown that the minimum number of nodes that can be present in an AVL tree of height h is at least 2h/2. In reality, it's actually more than that, but this gives us something useful to work with; we can use this result to figure out what we're really interested in, which is the opposite: what is the height of an AVL tree with n nodes? M(h) \u2265 2h/2 log2M(h) \u2265 h/2 2 log2M(h) \u2265 h Finally, we see that, for AVL trees of height h with the minimum number of nodes, the height is no more than 2 log2n, where n is the number of nodes in the tree. For AVL trees with more than the minimum number of nodes, the relationship between the number of nodes and the height is even better, though, for reasons we've seen previously, we know that the relationship between the number of nodes and the height of a binary tree can never be better than logarithmic. So, ultimately, we see that the height of an AVL tree with n nodes is \u0398(log n). (In reality, it turns out that the bound is lower than 2 log2n; it's something more akin to about 1.44 log2n, even for AVL trees with the minimum number of nodes, though the proof of that is more involved and doesn't change the asymptotic result.)"
    },
    "332": {
        "file": "Extended Notes - UCI AVL Trees.pdf",
        "page": 4,
        "chunk": "is more involved and doesn't change the asymptotic result.)"
    },
    "333": {
        "file": "03 - Moving Beyond the Relational Model.pdf",
        "page": 0,
        "chunk": "DS 4300 Moving Beyond the Relational Model Mark Fontenot, PhD Northeastern University"
    },
    "334": {
        "file": "03 - Moving Beyond the Relational Model.pdf",
        "page": 1,
        "chunk": "Bene\ufb01ts of the Relational Model - (Mostly) Standard Data Model and Query Language - ACID Compliance (more on this in a second) - Atomicity, Consistency, Isolation, Durability - Works well will highly structured data - Can handle large amounts of data - Well understood, lots of tooling, lots of experience 2"
    },
    "335": {
        "file": "03 - Moving Beyond the Relational Model.pdf",
        "page": 2,
        "chunk": "Relational Database Performance Many ways that a RDBMS increases ef\ufb01ciency: - indexing (the topic we focused on) - directly controlling storage - column oriented storage vs row oriented storage - query optimization - caching/prefetching - materialized views - precompiled stored procedures - data replication and partitioning 3"
    },
    "336": {
        "file": "03 - Moving Beyond the Relational Model.pdf",
        "page": 3,
        "chunk": "Transaction Processing - Transaction - a sequence of one or more of the CRUD operations performed as a single, logical unit of work - Either the entire sequence succeeds (COMMIT) - OR the entire sequence fails (ROLLBACK or ABORT) - Help ensure - Data Integrity - Error Recovery - Concurrency Control - Reliable Data Storage - Simpli\ufb01ed Error Handling 4"
    },
    "337": {
        "file": "03 - Moving Beyond the Relational Model.pdf",
        "page": 4,
        "chunk": "ACID Properties - Atomicity - transaction is treated as an atomic unit - it is fully executed or no parts of it are executed - Consistency - a transaction takes a database from one consistent state to another consistent state - consistent state - all data meets integrity constraints 5"
    },
    "338": {
        "file": "03 - Moving Beyond the Relational Model.pdf",
        "page": 5,
        "chunk": "ACID Properties - Isolation - Two transactions T1 and T2 are being executed at the same time but cannot affect each other - If both T1 and T2 are reading the data - no problem - If T1 is reading the same data that T2 may be writing, can result in: - Dirty Read - Non-repeatable Read - Phantom Reads 6"
    },
    "339": {
        "file": "03 - Moving Beyond the Relational Model.pdf",
        "page": 6,
        "chunk": "Isolation: Dirty Read 7 Figure from: https://www.mybluelinux.com/relational-databases-explained/ Dirty Read - a transaction T1 is able to read a row that has been modi\ufb01ed by another transaction T2 that hasn\u2019t yet executed a COMMIT"
    },
    "340": {
        "file": "03 - Moving Beyond the Relational Model.pdf",
        "page": 7,
        "chunk": "Isolation: Non-Repeatable Read 8 Figure from: https://www.mybluelinux.com/relational-databases-explained/ Non-repeatable Read - two queries in a single transaction T1 execute a SELECT but get different values because another transaction T2 has changed data and COMMITTED"
    },
    "341": {
        "file": "03 - Moving Beyond the Relational Model.pdf",
        "page": 8,
        "chunk": "Isolation: Phantom Reads 9 Figure from: https://www.mybluelinux.com/relational-databases-explained/ Phantom Reads - when a transaction T1 is running and another transaction T2 adds or deletes rows from the set T1 is using"
    },
    "342": {
        "file": "03 - Moving Beyond the Relational Model.pdf",
        "page": 9,
        "chunk": "Example Transaction - Transfer $$ 10 DELIMITER // CREATE PROCEDURE transfer( IN sender_id INT, IN receiver_id INT, IN amount DECIMAL(10,2) ) BEGIN DECLARE rollback_message VARCHAR(255) DEFAULT 'Transaction rolled back: Insufficient funds'; DECLARE commit_message VARCHAR(255) DEFAULT 'Transaction committed successfully'; -- Start the transaction START TRANSACTION; -- Attempt to debit money from account 1 UPDATE accounts SET balance = balance - amount WHERE account_id = sender_id; -- Attempt to credit money to account 2 UPDATE accounts SET balance = balance + amount WHERE account_id = receiver_id; -- Continued Next Slide"
    },
    "343": {
        "file": "03 - Moving Beyond the Relational Model.pdf",
        "page": 10,
        "chunk": "Example Transaction - Transfer $$ 11 -- Continued from previous slide -- Check if there are sufficient funds in account 1 -- Simulate a condition where there are insufficient funds IF (SELECT balance FROM accounts WHERE account_id = sender_id) < 0 THEN -- Roll back the transaction if there are insufficient funds ROLLBACK; SIGNAL SQLSTATE '45000' -- 45000 is unhandled, user-defined error SET MESSAGE_TEXT = rollback_message; ELSE -- Log the transactions if there are sufficient funds INSERT INTO transactions (account_id, amount, transaction_type) VALUES (sender_id, -amount, 'WITHDRAWAL'); INSERT INTO transactions (account_id, amount, transaction_type) VALUES (receiver_id, amount, 'DEPOSIT'); -- Commit the transaction COMMIT; SELECT commit_message AS 'Result'; END IF; END // DELIMITER ;"
    },
    "344": {
        "file": "03 - Moving Beyond the Relational Model.pdf",
        "page": 11,
        "chunk": "ACID Properties - Durability - Once a transaction is completed and committed successfully, its changes are permanent. - Even in the event of a system failure, committed transactions are preserved - For more info on Transactions, see: - Kleppmann Book Chapter 7 12"
    },
    "345": {
        "file": "03 - Moving Beyond the Relational Model.pdf",
        "page": 12,
        "chunk": "But \u2026 Relational Databases may not be the solution to all problems\u2026 - sometimes, schemas evolve over time - not all apps may need the full strength of ACID compliance - joins can be expensive - a lot of data is semi-structured or unstructured (JSON, XML, etc) - Horizontal scaling presents challenges - some apps need something more performant (real time, low latency systems) 13"
    },
    "346": {
        "file": "03 - Moving Beyond the Relational Model.pdf",
        "page": 13,
        "chunk": "Scalability - Up or Out? Conventional Wisdom: Scale vertically (up, with bigger, more powerful systems) until the demands of high-availability make it necessary to scale out with some type of distributed computing model But why? Scaling up is easier - no need to really modify your architecture. But there are practical and \ufb01nancial limits However: There are modern systems that make horizontal scaling less problematic. 14"
    },
    "347": {
        "file": "03 - Moving Beyond the Relational Model.pdf",
        "page": 14,
        "chunk": "So what? Distributed Data when Scaling Out A distributed system is \u201ca collection of independent computers that appear to its users as one computer.\u201d -Andrew Tennenbaum Characteristics of Distributed Systems: - computers operate concurrently - computers fail independently - no shared global clock 15"
    },
    "348": {
        "file": "03 - Moving Beyond the Relational Model.pdf",
        "page": 15,
        "chunk": "Distributed Storage - 2 Directions 16 Single Main Node"
    },
    "349": {
        "file": "03 - Moving Beyond the Relational Model.pdf",
        "page": 16,
        "chunk": "Distributed Data Stores - Data is stored on > 1 node, typically replicated - i.e. each block of data is available on N nodes - Distributed databases can be relational or non-relational - MySQL and PostgreSQL support replication and sharding - CockroachDB - new player on the scene - Many NoSQL systems support one or both models - But remember: Network partitioning is inevitable! - network failures, system failures - Overall system needs to be Partition Tolerant - System can keep running even w/ network partition 17"
    },
    "350": {
        "file": "03 - Moving Beyond the Relational Model.pdf",
        "page": 17,
        "chunk": "The CAP Theorem 18"
    },
    "351": {
        "file": "03 - Moving Beyond the Relational Model.pdf",
        "page": 18,
        "chunk": "The CAP Theorem 19 The CAP Theorem states that it is impossible for a distributed data store to simultaneously provide more than two out of the following three guarantees: - Consistency - Every read receives the most recent write or error thrown - Availability - Every request receives a (non-error) response - but no guarantee that the response contains the most recent write - Partition Tolerance - The system can continue to operate despite arbitrary network issues."
    },
    "352": {
        "file": "03 - Moving Beyond the Relational Model.pdf",
        "page": 19,
        "chunk": "CAP Theorem - Database View 20 Reference: https://alperenbayramoglu.com/posts/understanding-cap-theorem/ - Consistency*: Every user of the DB has an identical view of the data at any given instant - Availability: In the event of a failure, the database remains operational - Partition Tolerance: The database can maintain operations in the event of the network\u2019s failing between two segments of the distributed system * Note, the definition of Consistency in CAP is different from that of ACID."
    },
    "353": {
        "file": "03 - Moving Beyond the Relational Model.pdf",
        "page": 20,
        "chunk": "CAP Theorem - Database View 21 Reference: https://alperenbayramoglu.com/posts/understanding-cap-theorem/ - Consistency + Availability: System always responds with the latest data and every request gets a response, but may not be able to deal with network issues - Consistency + Partition Tolerance: If system responds with data from a distributed store, it is always the latest, else data request is dropped. - Availability + Partition Tolerance: System always sends are responds based on distributed store, but may not be the absolute latest data."
    },
    "354": {
        "file": "03 - Moving Beyond the Relational Model.pdf",
        "page": 21,
        "chunk": "CAP in Reality What it is really saying: - If you cannot limit the number of faults, requests can be directed to any server, and you insist on serving every request, then you cannot possibly be consistent. But it is interpreted as: - You must always give up something: consistency, availability, or tolerance to failure. 22"
    },
    "355": {
        "file": "03 - Moving Beyond the Relational Model.pdf",
        "page": 22,
        "chunk": "?? 23"
    },
    "356": {
        "file": "Memory Structure and Indexing.pdf",
        "page": 0,
        "chunk": "Computer Memory Structure 1.\u200b Processor a.\u200b CPU i.\u200b Super fast ii.\u200b Super expensive iii.\u200b Tiny capacity b.\u200b CPU Cache (L1 - L3 Cache) i.\u200b Not as fast as CPU ii.\u200b Expensive iii.\u200b Small capacity 2.\u200b RAM a.\u200b Physical Memory (Random Access Memory) i.\u200b Not as fast as CPU cache ii.\u200b Priced reasonably iii.\u200b Average Capacity 3.\u200b Solid State Drives (Solid State Memory) a.\u200b Non-Volatile Flash-Based Memory i.\u200b Average Speed ii.\u200b Priced reasonable iii.\u200b Average capacity 4.\u200b Mechanical Hard Drives (Virtual Memory) a.\u200b File-Based Memory i.\u200b Slow ii.\u200b Cheap iii.\u200b Large Capacity \u25cf\u200b For fast databases, we want to minimize access to solid state drives and hard drives \u25cf\u200b A 64-bit integer takes up 8 bytes of memory \u25cf\u200b Say you have a 2048 byte block size. To get 1, 64-bit (8 byte) integer out of memory, the DB has to read 2048 bytes. \u25cf\u200b In an AVL node containing a key-value pair of two 64-bit integers, we are using 32 bytes of memory \u25cb\u200b 8 bytes for key, 8 bytes for value, 8 bytes for right child pointer, 8 bytes for left child pointer \u25cb\u200b Therefore, we are not optimizing the amount of memory we have available. We can increase the performance by decreasing the height of the tree \u25cf\u200b Consider the following case: We have a sorted array of 128 integers"
    },
    "357": {
        "file": "Memory Structure and Indexing.pdf",
        "page": 1,
        "chunk": "\u25cb\u200b 128 * 8 = 1024 bytes \u25cb\u200b In the worst case, binary search o 128 integers is still much faster than a single additional disk access \u25cb\u200b Reading only one block of memory from a hard drive is significantly faster than reading for a second time \u25cb\u200b What if we could have two keys per node? Or three or four\u2026? \u25a0\u200b As you add more keys to each node, the tree gets shallower (smaller height) and searching gets faster \u25a0\u200b The main point: in databases, we want to minimize hard drive access \u25cf\u200b The ideal structure or databases and indexing are B+ trees \u25cb\u200b Each node can have up to 128, 256, etc values (keys) for indexing. \u25cb\u200b For n-1 keys, we can have n children \u25cb\u200b Searching for a child is still much faster than another disk read \u25cb\u200b This way, we can store more values and keys in much fewer levels \u25cf\u200b The goal of indexing structures = minimize the height of the tree"
    },
    "358": {
        "file": "Extended Notes - U Rochester BST.pdf",
        "page": 0,
        "chunk": "Chapter 12: Binary Search Trees A binary search tree is a binary tree with a special property called the BST-property, which is given as follows: \u22c6 For all nodes x and y, if y belongs to the left subtree of x, then the key at y is less than the key at x, and if y belongs to the right subtree of x, then the key at y is greater than the key at x. We will assume that the keys of a BST are pairwise distinct. Each node has the following attributes: \u2022 p, left, and right, which are pointers to the parent, the left child, and the right child, respectively, and \u2022 key, which is key stored at the node. 1"
    },
    "359": {
        "file": "Extended Notes - U Rochester BST.pdf",
        "page": 1,
        "chunk": "An example 4 2 3 6 5 12 9 8 11 15 19 20 7 2"
    },
    "360": {
        "file": "Extended Notes - U Rochester BST.pdf",
        "page": 2,
        "chunk": "Traversal of the Nodes in a BST By \u201ctraversal\u201d we mean visiting all the nodes in a graph. Traversal strategies can be speci\ufb01ed by the ordering of the three objects to visit: the current node, the left subtree, and the right subtree. We assume the the left subtree always comes before the right subtree. Then there are three strategies. 1. Inorder. The ordering is: the left subtree, the current node, the right subtree. 2. Preorder. The ordering is: the current node, the left subtree, the right subtree. 3. Postorder. The ordering is: the left subtree, the right subtree, the current node. 3"
    },
    "361": {
        "file": "Extended Notes - U Rochester BST.pdf",
        "page": 3,
        "chunk": "Inorder Traversal Pseudocode This recursive algorithm takes as the input a pointer to a tree and executed inorder traversal on the tree. While doing traversal it prints out the key of each node that is visited. Inorder-Walk(x) 1: if x = nil then return 2: Inorder-Walk(left[x]) 3: Print key[x] 4: Inorder-Walk(right[x]) We can write a similar pseudocode for preorder and postorder. 4"
    },
    "362": {
        "file": "Extended Notes - U Rochester BST.pdf",
        "page": 4,
        "chunk": "preorder postorder inorder 2 1 3 1 1 2 2 3 3 4 2 3 6 5 12 9 8 11 15 19 20 7 What is the outcome of inorder traversal on this BST? How about postorder traversal and preorder traversal? 5"
    },
    "363": {
        "file": "Extended Notes - U Rochester BST.pdf",
        "page": 5,
        "chunk": "Inorder traversal gives: 2, 3, 4, 5, 6, 7, 8 , 9, 11, 12, 15, 19, 20. Preorder traversal gives: 7, 4, 2, 3, 6, 5, 12, 9, 8, 11, 19, 15, 20. Postorder traversal gives: 3, 2, 5, 6, 4, 8, 11, 9, 15, 20, 19, 12, 7. So, inorder travel on a BST \ufb01nds the keys in nondecreasing order! 6"
    },
    "364": {
        "file": "Extended Notes - U Rochester BST.pdf",
        "page": 6,
        "chunk": "Operations on BST 1. Searching for a key We assume that a key and the subtree in which the key is searched for are given as an input. We\u2019ll take the full advantage of the BST-property. Suppose we are at a node. If the node has the key that is being searched for, then the search is over. Otherwise, the key at the current node is either strictly smaller than the key that is searched for or strictly greater than the key that is searched for. If the former is the case, then by the BST property, all the keys in th left subtree are strictly less than the key that is searched for. That means that we do not need to search in the left subtree. Thus, we will examine only the right subtree. If the latter is the case, by symmetry we will examine only the right subtree. 7"
    },
    "365": {
        "file": "Extended Notes - U Rochester BST.pdf",
        "page": 7,
        "chunk": "Algorithm Here k is the key that is searched for and x is the start node. BST-Search(x, k) 1: y \u2190x 2: while y \u0338= nil do 3: if key[y] = k then return y 4: else if key[y] < k then y \u2190right[y] 5: else y \u2190left[y] 6: return (\u201cNOT FOUND\u201d) 8"
    },
    "366": {
        "file": "Extended Notes - U Rochester BST.pdf",
        "page": 8,
        "chunk": "An Example search for 8 7 4 2 6 9 13 11 NIL What is the running time of search? 9"
    },
    "367": {
        "file": "Extended Notes - U Rochester BST.pdf",
        "page": 9,
        "chunk": "2. The Maximum and the Minimum To \ufb01nd the minimum identify the leftmost node, i.e. the farthest node you can reach by following only left branches. To \ufb01nd the maximum identify the rightmost node, i.e. the farthest node you can reach by following only right branches. BST-Minimum(x) 1: if x = nil then return (\u201cEmpty Tree\u201d) 2: y \u2190x 3: while left[y] \u0338= nil do y \u2190left[y] 4: return (key[y]) BST-Maximum(x) 1: if x = nil then return (\u201cEmpty Tree\u201d) 2: y \u2190x 3: while right[y] \u0338= nil do y \u2190right[y] 4: return (key[y]) 10"
    },
    "368": {
        "file": "Extended Notes - U Rochester BST.pdf",
        "page": 10,
        "chunk": "3. Insertion Suppose that we need to insert a node z such that k = key[z]. Using binary search we \ufb01nd a nil such that replacing it by z does not break the BST-property. 11"
    },
    "369": {
        "file": "Extended Notes - U Rochester BST.pdf",
        "page": 11,
        "chunk": "BST-Insert(x, z, k) 1: if x = nil then return \u201cError\u201d 2: y \u2190x 3: while true do { 4: if key[y] < k 5: then z \u2190left[y] 6: else z \u2190right[y] 7: if z = nil break 8: } 9: if key[y] > k then left[y] \u2190z 10: else right[p[y]] \u2190z 12"
    },
    "370": {
        "file": "Extended Notes - U Rochester BST.pdf",
        "page": 12,
        "chunk": "4. The Successor and The Predecessor The successor (respectively, the predecessor) of a key k in a search tree is the smallest (respectively, the largest) key that belongs to the tree and that is strictly greater than (respectively, less than) k. The idea for \ufb01nding the successor of a given node x. \u2022 If x has the right child, then the successor is the minimum in the right subtree of x. \u2022 Otherwise, the successor is the parent of the farthest node that can be reached from x by following only right branches backward. 13"
    },
    "371": {
        "file": "Extended Notes - U Rochester BST.pdf",
        "page": 13,
        "chunk": "An Example 4 2 3 6 5 12 9 8 11 15 19 20 7 25 23 14"
    },
    "372": {
        "file": "Extended Notes - U Rochester BST.pdf",
        "page": 14,
        "chunk": "Algorithm BST-Successor(x) 1: if right[x] \u0338= nil then 2: { y \u2190right[x] 3: while left[y] \u0338= nil do y \u2190left[y] 4: return (y) } 5: else 6: { y \u2190x 7: while right[p[x]] = x do y \u2190p[x] 8: if p[x] \u0338= nil then return (p[x]) 9: else return (\u201cNO SUCCESSOR\u201d) } 15"
    },
    "373": {
        "file": "Extended Notes - U Rochester BST.pdf",
        "page": 15,
        "chunk": "The predecessor can be found similarly with the roles of left and right exchanged and with the roles of maximum and minimum exchanged. For which node is the successor unde\ufb01ned? What is the running time of the successor algorithm? 16"
    },
    "374": {
        "file": "Extended Notes - U Rochester BST.pdf",
        "page": 16,
        "chunk": "5. Deletion Suppose we want to delete a node z. 1. If z has no children, then we will just replace z by nil. 2. If z has only one child, then we will promote the unique child to z\u2019s place. 3. If z has two children, then we will identify z\u2019s successor. Call it y. The successor y either is a leaf or has only the right child. Promote y to z\u2019s place. Treat the loss of y using one of the above two solutions. 17"
    },
    "375": {
        "file": "Extended Notes - U Rochester BST.pdf",
        "page": 17,
        "chunk": "10 10 5 6 1 3 2 4 7 9 13 11 8 5 6 1 3 2 4 9 13 11 8 4 2 3 5 6 1 3 2 4 7 9 13 11 8 5 6 9 13 11 8 7 10 10 10 5 6 1 3 2 4 7 9 13 11 8 5 6 1 3 2 4 13 11 9 10 18"
    },
    "376": {
        "file": "Extended Notes - U Rochester BST.pdf",
        "page": 18,
        "chunk": "Algorithm This algorithm deletes z from BST T. BST-Delete(T, z) 1: if left[z] = nil or right[z] = nil 2: then y \u2190z 3: else y \u2190BST-Successor(z) 4: \u0003 y is the node that\u2019s actually removed. 5: \u0003 Here y does not have two children. 6: if left[y] \u0338= nil 7: then x \u2190left[y] 8: else x \u2190right[y] 9: \u0003 x is the node that\u2019s moving to y\u2019s position. 10: if x \u0338= nil then p[x] \u2190p[y] 11: \u0003 p[x] is reset If x isn\u2019t NIL. 12: \u0003 Resetting is unnecessary if x is NIL. 19"
    },
    "377": {
        "file": "Extended Notes - U Rochester BST.pdf",
        "page": 19,
        "chunk": "Algorithm (cont\u2019d) 13: if p[y] = nil then root[T] \u2190x 14: \u0003 If y is the root, then x becomes the root. 15: \u0003 Otherwise, do the following. 16: else if y = left[p[y]] 17: then left[p[y]] \u2190x 18: \u0003 If y is the left child of its parent, then 19: \u0003 Set the parent\u2019s left child to x. 20: else right[p[y]] \u2190x 21: \u0003 If y is the right child of its parent, then 22: \u0003 Set the parent\u2019s right child to x. 23: if y \u0338= z then 24: { key[z] \u2190key[y] 25: Move other data from y to z } 27: return (y) 20"
    },
    "378": {
        "file": "06 - Redis + Python.pdf",
        "page": 0,
        "chunk": "DS 4300 Redis + Python Mark Fontenot, PhD Northeastern University"
    },
    "379": {
        "file": "06 - Redis + Python.pdf",
        "page": 1,
        "chunk": "Redis-py 2 - Redis-py is the standard client for Python. - Maintained by the Redis Company itself - GitHub Repo: redis/redis-py - In your 4300 Conda Environment: pip install redis"
    },
    "380": {
        "file": "06 - Redis + Python.pdf",
        "page": 2,
        "chunk": "Connecting to the Server - For your Docker deployment, host could be localhost or 127.0.0.1 - Port is the port mapping given when you created the container (probably the default 6379) - db is the database 0-15 you want to connect to - decode_responses \u2192 data comes back from server as bytes. Setting this true converter them (decodes) to strings. 3 import redis redis_client = redis.Redis(host=\u2019localhost\u2019, port=6379, db=2, decode_responses=True)"
    },
    "381": {
        "file": "06 - Redis + Python.pdf",
        "page": 3,
        "chunk": "Redis Command List - Full List > here < - Use Filter to get to command for the particular data structure you\u2019re targeting (list, hash, set, etc.) - Redis.py Documentation > here < - The next slides are not meant to be an exhaustive list of commands, only some highlights. Check the documentation for a complete list. 4"
    },
    "382": {
        "file": "06 - Redis + Python.pdf",
        "page": 4,
        "chunk": "String Commands # r represents the Redis client object r.set(\u2018clickCount:/abc\u2019, 0) val = r.get(\u2018clickCount:/abc\u2019) r.incr(\u2018clickCount:/abc\u2019) ret_val = r.get(\u2018clickCount:/abc\u2019) print(f\u2019click count = {ret_val}\u2019) 5"
    },
    "383": {
        "file": "06 - Redis + Python.pdf",
        "page": 5,
        "chunk": "String Commands - 2 # r represents the Redis client object redis_client.mset({'key1': 'val1', 'key2': 'val2', 'key3': 'val3'}) print(redis_client.mget('key1', 'key2', 'key3')) # returns as list [\u2018val1\u2019, \u2018val2\u2019, \u2018val3\u2019] 6"
    },
    "384": {
        "file": "06 - Redis + Python.pdf",
        "page": 6,
        "chunk": "String Commands - 3 - set(), mset(), setex(), msetnx(), setnx() - get(), mget(), getex(), getdel() - incr(), decr(), incrby(), decrby() - strlen(), append() 7"
    },
    "385": {
        "file": "06 - Redis + Python.pdf",
        "page": 7,
        "chunk": "List Commands - 1 # create list: key = \u2018names\u2019 # values = [\u2018mark\u2019, \u2018sam\u2019, \u2018nick\u2019] redis_client.rpush('names', 'mark', 'sam', 'nick') # prints [\u2018mark\u2019, \u2018sam\u2019, \u2018nick\u2019] print(redis_client.lrange('names', 0, -1)) 8"
    },
    "386": {
        "file": "06 - Redis + Python.pdf",
        "page": 8,
        "chunk": "List Commands - 2 - lpush(), lpop(), lset(), lrem() - rpush(), rpop() - lrange(), llen(), lpos() - Other commands include moving elements between lists, popping from multiple lists at the same time, etc. 9"
    },
    "387": {
        "file": "06 - Redis + Python.pdf",
        "page": 9,
        "chunk": "Hash Commands - 1 redis_client.hset('user-session:123', mapping={'first': 'Sam', 'last': 'Uelle', 'company': 'Redis', 'age': 30 }) # prints: #{'name': 'Sam', 'surname': 'Uelle', 'company': 'Redis', 'age': '30'} print(redis_client.hgetall('user-session:123')) 10"
    },
    "388": {
        "file": "06 - Redis + Python.pdf",
        "page": 10,
        "chunk": "Hash Commands - 2 - hset(), hget(), hgetall() - hkeys() - hdel(), hexists(), hlen(), hstrlen() 11"
    },
    "389": {
        "file": "06 - Redis + Python.pdf",
        "page": 11,
        "chunk": "Redis Pipelines - Helps avoid multiple related calls to the server \u2192 less network overhead 12 r = redis.Redis(decode_responses=True) pipe = r.pipeline() for i in range(5): pipe.set(f\"seat:{i}\", f\"#{i}\") set_5_result = pipe.execute() print(set_5_result) # >>> [True, True, True, True, True] pipe = r.pipeline() # \"Chain\" pipeline commands together. get_3_result = pipe.get(\"seat:0\").get(\"seat:3\").get(\"seat:4\").execute() print(get_3_result) # >>> ['#0', '#3', '#4']"
    },
    "390": {
        "file": "06 - Redis + Python.pdf",
        "page": 12,
        "chunk": "Redis in Context 13"
    },
    "391": {
        "file": "06 - Redis + Python.pdf",
        "page": 13,
        "chunk": "Redis in ML - Simpli\ufb01ed Example 14 Source: https://www.featureform.com/post/feature-stores-explained-the-three-common-architectures"
    },
    "392": {
        "file": "06 - Redis + Python.pdf",
        "page": 14,
        "chunk": "Redis in DS/ML 15 Source: https://madewithml.com/courses/mlops/feature-store/"
    },
    "393": {
        "file": "Copy of Midterm Sample Questions.pdf",
        "page": 0,
        "chunk": "DS 4300 - Spring 2025 Sample Midterm Questions (& HW 04) Below are some sample exam questions with which you can test your RAG \u201cCheat Sheet\u201d. Question: What is the difference between a list where memory is contiguously allocated and a list where linked structures are used? Answer: In a list where memory is contiguously allocated, there is no need for pointers. The next element in the list simply lies in the next block of memory. Therefore, a program can easily find the next element in the list by increasing the memory address by 1 unit. In a case where linked structures are used, the memory address of the next element in the list is not necessarily the next memory address like in contiguous allocation. Therefore, in linked structures, each element must have a pointer to the next element\u2019s memory address. Question: When are linked lists faster than contiguously-allocated lists? Answer: Linked lists are faster when you want to insert an element anywhere. Question: Add 23 to the AVL Tree below. What imbalance case is created with inserting 23? \u200b \u200b 30 \u200b / \\ \u200b 25 35 \u200b / 20\u200b Answer: Since 23 is less than 30, we go to 30\u2019s left child (25). Since 25 is less than 25, we go to 25\u2019s left child (20). Since 23 is greater than 20, we place 23 as 20\u2019s right child. This presents a left-right insertion imbalance. 25 is the node of imbalance, and 23 was inserted into the right subtree of the left child of 25. Question: Why is a B+ Tree a better than an AVL tree when indexing a large dataset? Answer: B+ trees are the most optimal structure for indexing large datasets because they minimize disk access. B+ trees are \u201cwider\u201d than AVL trees, but this"
    },
    "394": {
        "file": "Copy of Midterm Sample Questions.pdf",
        "page": 0,
        "chunk": "subtree of the left child of 25. Question: Why is a B+ Tree a better than an AVL tree when indexing a large dataset? Answer: B+ trees are the most optimal structure for indexing large datasets because they minimize disk access. B+ trees are \u201cwider\u201d than AVL trees, but this allows them to be shallower. The shallower a tree is, the less disk access we need to make to retrieve an element. Since each node in a B+ tree can contain an arbitrary number of sorted keys and their values, we can maximize the storage efficiency on disk. Question: What is disk-based indexing and why is it important for database systems? Answer: Disk-based indexing is a process by which we store frequently accessed data on disk. This allows for quick retrieval which is essential for database systems. Question: In the context of a relational database system, what is a transaction? Answer: In the context of a RDBMS, a transaction refers to a sequence of operations that are performed on data that are executed as a single, indivisible unit. Question: Succinctly describe the four components of ACID compliant transactions. Answer: Atomicity - each operation is a single, indivisible unit (like an atom). Consistency - operations take the database from one valid state to another, preserving the integrity constraints. Isolations - transactions execute independently without interfering with one another. If both transactions access the same data, they cannot affect each other\u2019s results. Durability - once a transaction is committed, its changes are permanent, even in the event of a system failure. Question: Why does the CAP principle not make sense when applied to a single-node MongoDB"
    },
    "395": {
        "file": "Copy of Midterm Sample Questions.pdf",
        "page": 0,
        "chunk": "permanent, even in the event of a system failure. Question: Why does the CAP principle not make sense when applied to a single-node MongoDB"
    },
    "396": {
        "file": "Copy of Midterm Sample Questions.pdf",
        "page": 1,
        "chunk": "instance? Answer: The CAP theorem only applies to distributed systems and a single-node MongoDB is just that - a single node. It is not a distributed system. Therefore, all three principles of the CAP theorem (Consistency, Availability, and Partition Tolerance) are fundamentally satisfied by a single MongoDB node. Question: Describe the differences between horizontal and vertical scaling. Answer: Horizontal scaling is adding more servers or nodes to distribute the load across more servers. Vertical scaling is making the current server/node larger to handle more information. The main difference in practice is that horizontal scaling is essentially limitless. You can add as many servers/nodes to achieve a more efficient load balance. Vertical scaling is more limited because you can only add so many CPUs or GBs of RAM for example. Question: Briefly describe how a key/value store can be used as a feature store. Answer: A feature store is a data system used for machine learning, serving as a centralized hub for storing, processing, and accessing commonly used features. Key-value stores can be used as a feature store because retrieving commonly used features is very efficient in key-value stores. Since the underlying data structure resembles a hash table, finding a particular feature can be done in constant time. Question: When was Redis originally released? Answer: 2009. Question: In Redis, what is the difference between the INC and INCR commands? Answer: INC is not a command in Redis. INCR increases a value by 1. Question: What are the benefits of BSON over JSON in MongoDB? Answer: MongoDB supports a wide variety of data types as its fields. Therefore, BSON is required to quickly serialize and deserialize complex field types such as documents. Using JSON may be slower in such complex cases. Question: Write a Mongo query based on the movies data set"
    },
    "397": {
        "file": "Copy of Midterm Sample Questions.pdf",
        "page": 1,
        "chunk": "JSON in MongoDB? Answer: MongoDB supports a wide variety of data types as its fields. Therefore, BSON is required to quickly serialize and deserialize complex field types such as documents. Using JSON may be slower in such complex cases. Question: Write a Mongo query based on the movies data set that returns the titles of all movies released between 2010 and 2015 from the suspense genre? Answer: db.movies.find({ \"year\": { $gte: 2010, $lte: 2015 }, \"genres\": \"suspense\" }, { \"name\": 1 }) Question: What does the $nin operator mean in a Mongo query? Answer: Not in"
    },
    "398": {
        "file": "Mongo_DB_Shell_Cheat_Sheet_1a0e3aa962.pdf",
        "page": 0,
        "chunk": "MongoDB Shell Cheat Sheet To get started, install the MongoDB Shell (mongosh). Basic Commands These basic help commands are available in the MongoDB Shell. mongosh Open a connection to your local MongoDB instance. All other commands will be run within this mongosh connection. db.help() Show help for database methods. db.<collection>.help() db.users.help() Show help on collection methods. The <collection> can be the name of an existing collection or a non-existing collection. Shows help on methods related to the users collection. show dbs Print a list of all databases on the server. use <db> Switch current database to <db>. The mongo shell variable db is set to the current database. show collections Print a list of all collections for the current database. show users Print a list of users for the current database. show roles Print a list of all roles, both user-defined and built-in, for the current database. show profile Print the five most recent operations that took 1 millisecond or more on databases with profiling enabled."
    },
    "399": {
        "file": "Mongo_DB_Shell_Cheat_Sheet_1a0e3aa962.pdf",
        "page": 1,
        "chunk": "show databases Print a list of all existing databases available to the current user. exit Exit the mongosh session. Create Operations Create or insert operations add new documents to a collection. If the collection does not exist, create operations also create the collection. db.collection.insertOne() db.users.insertOne( { name: \"Chris\"} ) Inserts a single document into a collection. Add a new document with the name of Chris into the users collection db.collection.insertMany() db.users.insertMany( { age: \"24\"}, {age: \"38\"} ) Inserts multiple documents into a collection. Add two new documents with the age of 24 and 38 into the users collection Read Operations Read operations retrieve documents from a collection; i.e. query a collection for documents. db.collection.find() db.users.find() Selects documents in a collection or view and returns a cursor to the selected documents. Returns all users. db.collection.find(<filterobjec t>) db.users.find({place: \"NYC\"}) Find all documents that match the filter object Returns all users with the place NYC. db.collection.find({<field>:1,< field>:1}) db.users.find({status:1,item:1}) Returns all documents that match the query after you explicitly include several fields by setting the <field> to 1 in the projection document. Returns matching documents only from state field, item field and, by default, the _id field."
    },
    "400": {
        "file": "Mongo_DB_Shell_Cheat_Sheet_1a0e3aa962.pdf",
        "page": 2,
        "chunk": "db.collection.find({<field>:1,< field>:0, _id:0}) db.users.find({status:1,item:1,_id:0} ) Returns all documents that match the query and removes the _id field from the results by setting it to 0 in the projection. Returns matching documents only from state field and item field. Does not return the _id field. Update Operations Update operations modify existing documents in a collection. db.collection.updateOne() db.users.updateOne({ age: 25 }, { $set: { age: 32 } }) Updates a single document within the collection based on the filter. Updates all users from the age of 25 to 32. db.collection.updateMany() db.users.updateMany({ age: 27 }, { $inc: { age: 3 } }) Updates a single document within the collection based on the filter. Updates all users with an age of 27 with an increase of 3. db.collection.replaceOne() db.users.replaceOne({ name: Kris }, { name: Chris }) Replaces a single document within the collection based on the filter. Replaces the first user with the name Kris with a document that has the name Chris in its name field. Delete Operations Delete operations remove documents from a collection. db.collection.deleteOne() db.users.deleteOne({ age: 37 }) Removes a single document from a collection. Deletes the first user with the age 37. db.collection.deleteMany() db.users.deleteMany({ age: {$lt:18 }) Removes all documents that match the filter from a collection. Deletes all users with the age less than 18.."
    },
    "401": {
        "file": "Mongo_DB_Shell_Cheat_Sheet_1a0e3aa962.pdf",
        "page": 3,
        "chunk": "Comparison Query Operators Use the following inside an filter object to make complex queries $eq db.users.find({ system: { $eq: \"macOS\" } }) Matches values that are equal to a specified value. Finds all users with the operating system macOS. $gt db.users.deleteMany({ age: { $gt: 99} }) Matches values that are greater than a specified value. Deletes all users with an age greater than 99. $gte db.users.updateMany({ age\": {$gte:21 },{access: \"valid\"}) Matches values that are greater than or equal to a specified value. Updates all access to \"valid\" for all users with an age greater than or equal to 21. $in db.users.find( { place: { $in: [ \"NYC\", \"SF\"] } ) Matches any of the values specified in an array. Find all users with the place field that is either NYC or SF. $lt db.users.deleteMany({ \"age\": {$lt:18 }) Matches values that are less than a specified value. Deletes all users with the age less than 18.. $lte db.users.updateMany({ age: { $lte: 17 }, {access: \"invalid\"}) Matches values that are less than or equal to a specified value. Updates all access to \"invalid\" for all users with an age less than or equal to 17. $ne db.users.find({ \"place\": {$ne: \u2018NYC\"}) Matches all values that are not equal to a specified value. Find all users with the place field set to anything other than NYC."
    },
    "402": {
        "file": "Mongo_DB_Shell_Cheat_Sheet_1a0e3aa962.pdf",
        "page": 4,
        "chunk": "$nin db.users.find( { place: { $nin: [ \"NYC\", \"SF\" ] } ) Matches none of the values specified in an array. Find all users with the place field that does not equal NYC or SF. Field Update Operators Use the following inside an update object to make complex updates $inc db.users.updateOne({ age: 22 }, { $inc: { age: 3} }) Increments the value of the field by the specified amount. Adds 3 to the age of the first user with the age of 22. $min db.scores.insertOne( { _id: 1, highScore: 800, lowScore: 200 } ) db.scores.updateOne( { _id: 1 }, { $min: { lowScore: 150 } } ) Only updates the field if the specified value is less than the existing field value. Creates a scoles collection and sets the value of highScore to 800 and lowScore to 200. $min compares 200 (the current value of lowScore) to the specified value of 150. Because 150 is less than 200, $min will update lowScore to 150. $max db.scores.updateOne( { _id: 1 }, { $max: { highScore: 1000 } } ) Only updates the field if the specified value is greater than the existing field value. $max compares 800 (the current value of highScore) to the specified value of 1000. Because 1000 is more than 800, $max will update highScore to 1000. $rename db.scores.updateOne( { $rename: { 'highScore': 'high'} ) Renames a field. Renames the field \u2018highScores\u2019 to \u2018high\u2019,"
    },
    "403": {
        "file": "Mongo_DB_Shell_Cheat_Sheet_1a0e3aa962.pdf",
        "page": 5,
        "chunk": "$set db.users.updateOne({ $set: { name: \"valid user\" } }) Sets the value of a field in a document. Replaces the value of the name field with the specified value valid user. $unset db.users.updateOne({ $unset: { name: \"\" } }) Removes the specified field from a document. Deletes the specified value valid user from the name field. Read Modifiers Add any of the following to the end of any read operation cursor.sort() db.users.find().sort({ name: 1, age: -1 }) Orders the elements of an array during a $push operation. Sorts all users by name in alphabetical order and then if any names are the same sort by age in reverse order cursor.limit() Specifies the maximum number of documents the cursor will return. cursor.skip() Controls where MongoDB begins returning results. cursor.push() db.users.updateMany({}, { $push: { friends: \"Chris\" } }) Appends a specified value to an array. Add Chris to the friends array for all users"
    },
    "404": {
        "file": "Mongo_DB_Shell_Cheat_Sheet_1a0e3aa962.pdf",
        "page": 6,
        "chunk": "Aggregation Operations The Aggregation Framework provides a specific language that can be used to execute a set of aggregation operations (processing & computation) against data held in MongoDB. db.collection.aggregate() db.users.aggregate([ {$match: { access: \"valid\" } }, {$group: { _id: \"$cust_id\", total:{$sum: \"$amount\" } } }, {$sort: { total: -1 } }]) A method that provides access to the aggregation pipeline. Selects documents in the users collection with accdb.orders.estimatedDocumentCount({})_id field from the sum of the amount field, and sorts the results by the total field in descending order: Aggregation Operations Aggregation pipelines consist of one or more stages that process documents and can return results for groups of documents. count Counts the number of documents in a collection or a view. distinct Displays the distinct values found for a specified key in a collection or a view. mapReduce Run map-reduce aggregation operations over a collection Aggregation Operations Single Purpose Aggregation Methods aggregate documents from a single collection. db.collection.estimatedDocument Count() db.users.estimatedDocumentCount({}) Returns an approximate count of the documents in a collection or a view. Retrieves an approximate count of all the documents in the users collection."
    },
    "405": {
        "file": "Mongo_DB_Shell_Cheat_Sheet_1a0e3aa962.pdf",
        "page": 7,
        "chunk": "db.collection.count() db.users.count({}) Returns a count of the number of documents in a collection or a view. Returns the distinct values for the age field from all documents in the users collection. db.collection.distinct() db.users.distinct(\"age\") Returns an array of documents that have distinct values for the specified field. Returns the distinct values for the age field from all documents in the users collection. Indexing Commands Indexes support the efficient execution of queries in MongoDB. Indexes are special data structures that store a small portion of the data set in an easy-to-traverse form. db.collection.createIndex() db.users.createIndex(\"account creation date\") Builds an index on a collection. Creates the account creation date index in the users collection. db.collection.dropIndex() db.users.dropIndex(\"account creation date\") Removes a specified index on a collection. Removes the account creation date index from the users collection. db.collection.dropIndexes() db.users.dropIndexes() db.users.dropIndex(\"account creation date\", \"account termination date\") Removes all indexes but the _id (no parameters) or a specified set of indexes on a collection. Drop all but the _id index from a collection. Removes the account creation date index and the account termination date index from the users collection."
    },
    "406": {
        "file": "Mongo_DB_Shell_Cheat_Sheet_1a0e3aa962.pdf",
        "page": 8,
        "chunk": "db.collection.getIndexes() db.users.getIndexes() Returns an array of documents that describe the existing indexes on a collection. Returns an array of documents that hold index information for the users collection. db.collection.reIndex() db.users.reIndex() Rebuilds all existing indexes on a collection Drops all indexes on the users collection and recreates them. db.collection.totalIndexSize() db.users.totalIndexSize() Reports the total size used by the indexes on a collection. Provides a wrapper around the totalIndexSize field of the collStats output. Returns the total size of all indexes for the users collection. Replication Commands Replication refers to the process of ensuring that the same data is available on more than one MongoDB Server. rs.add() rs.add( \"mongodbd4.example.net:27017\" ) Adds a member to a replica set. Adds a new secondary member, mongodbd4.example.net:27017, with default vote and priority settings to a new replica set rs.conf() Returns a document that contains the current replica set configuration. rs.status() Returns the replica set status from the point of view of the member where the method is run."
    },
    "407": {
        "file": "Mongo_DB_Shell_Cheat_Sheet_1a0e3aa962.pdf",
        "page": 9,
        "chunk": "rs.stepDown() Instructs the primary of the replica set to become a secondary. After the primary steps down, eligible secondaries will hold an election for primary. rs.remove() Removes the member described by the hostname parameter from the current replica set. rs.reconfig() Reconfigures an existing replica set, overwriting the existing replica set configuration. Sharding Commands Sharding is a method for distributing or partitioning data across multiple computers. This is done by partitioning the data by key ranges and distributing the data among two or more database instances. sh.abortReshardCollection() sh.abortReshardCollection(\"users\") Ends a resharding operation Aborts a running reshard operation on the users collection. sh.addShard() sh.addShard(\"cluster\"/mongodb3.exampl e.net:27327\") Adds a shard to a sharded cluster. Adds the cluster replica set and specifies one member of the replica set. sh.commitReshardCollection () sh.commitReshardCollection(\"records.u sers\") Forces a resharding operation to block writes and complete. Forces the resharding operation on the records.users to block writes and complete."
    },
    "408": {
        "file": "Mongo_DB_Shell_Cheat_Sheet_1a0e3aa962.pdf",
        "page": 10,
        "chunk": "sh.disableBalancing() sh.disableBalancing(\"records.users\") Disable balancing on a single collection in a sharded database. Does not affect balancing of other collections in a sharded cluster. Disables the balancer for the specified sharded collection. sh.enableAutoSplit() Enables auto-splitting for the sharded cluster. sh.disableAutoSplit() Disables auto-splitting for the sharded cluster. sh.enableSharding() sh.enablingSharding(\"records\") Creates a database. Creates the records database. sh.help() Returns help text for the sh methods. sh.moveChunk() sh.moveChunk(\"records.users\", { zipcode: \"10003\" }, \"shardexample\") Migrates a chunk in a sharded cluster. Finds the chunk that contains the documents with the zipcode field set to 10003 and then moves that chunk to the shard named shardexample. sh.reshardCollection() sh.reshardCollection(\"records.users\", { order_id: 1 }) Initiates a resharding operation to change the shard key for a collection, changing the distribution of your data. Reshards the users collection with the new shard key { order_id: 1 } sh.shardCollection() sh.shardCollection(\"records.users\", { zipcode: 1 } ) Enables sharding for a collection. Shards the users collection by the zipcode field."
    },
    "409": {
        "file": "Mongo_DB_Shell_Cheat_Sheet_1a0e3aa962.pdf",
        "page": 11,
        "chunk": "sh.splitAt() sh.splitAt( \"records.users\", { x: 70 } ) Divides an existing chunk into two chunks using a specific value of the shard key as the dividing point. Splits a chunk of the records.users collection at the shard key value x: 70 sh.splitFind() sh.splitFind( \"records.users\", { x:70 } ) Divides an existing chunk that contains a document matching a query into two approximately equal chunks. Splits, at the median point, a chunk that contains the shard key value x: 70. sh.status() Reports on the status of a sharded cluster, as db.printShardingStatus(). sh.waitForPingChange() Internal. Waits for a change in ping state from one of the mongos in the sharded cluster. refineCollectionShardKey db.adminCommand( { shardCollection: \"test.orders\", key: { customer_id: 1 } } ) db.getSiblingDB(\"test\").orders.create Index( { customer_id: 1, order_id: 1 } ) db.adminCommand( { refineCollectionShardKey: \"test.orders\", key: { customer_id: 1, order_id: 1 } } ) Modifies the collection's shard key by adding new field(s) as a suffix to the existing key. Shard the orders collection in the test database. The operation uses the customer_id field as the initial shard key. Create the index to support the new shard key if the index does not already exist. Run refineCollectionShardKey command to add the order_id field as a suffix"
    },
    "410": {
        "file": "Mongo_DB_Shell_Cheat_Sheet_1a0e3aa962.pdf",
        "page": 12,
        "chunk": "convertShardKeyToHashed() use test db.orders.createIndex( { _id: \"hashed\" } ) sh.shardCollection( \"test.orders\", { _id : \"hashed\" } ) { _id: ObjectId(\"5b2be413c06d924ab26ff9ca\"), \"item\" : \"Chocolates\", \"qty\" : 25 } convertShardKeyToHashed( ObjectId(\"5b2be413c06d924ab26ff9ca\") ) Returns the hashed value for the input. Consider a sharded collection that uses a hashed shard key. If the following document exists in the collection, the hashed value of the _id field is used to distribute the document: Determine the hashed value of _id field used to distribute the document across the shards, Database Methods db.runCommand() Run a command against the current database db.adminCommand() Provides a helper to run specified database commands against the admin database."
    },
    "411": {
        "file": "Mongo_DB_Shell_Cheat_Sheet_1a0e3aa962.pdf",
        "page": 13,
        "chunk": "User Management Commands Make updates to users in the MongoDB Shell. db.auth() Authenticates a user to a database. db.changeUserPassword() Updates a user's password. db.createUser() Creates a new user for the database on which the method is run. db.dropUser() db.dropAllUsers() Removes user/all users from the current database. db.getUser() db.getUsers() Returns information for a specified user/all users in the database. db.grantRolesToUser() Grants a role and its privileges to a user. db.removeUser() Removes the specified username from the database. db.revokeRolesFromUser() Removes one or more roles from a user on the current database. db.updateUser() Updates the user's profile on the database on which you run the method. passwordPrompt() Prompts for the password in mongosh."
    },
    "412": {
        "file": "Mongo_DB_Shell_Cheat_Sheet_1a0e3aa962.pdf",
        "page": 14,
        "chunk": "Role Management Commands Make updates to roles in the MongoDB Shell. db.createRole() Authenticates a user to a database. db.dropRole() db.dropAllRoles() Deletes a user-defined role/all user-defined roles associated with a database. db.getRole() db.getRoles() Returns information for the specified role/all the user-defined roles in a database. db.grantPrivilegesToRole() Assigns privileges to a user-defined role. db.revokePrivilegesFromRole() Removes the specified privileges from a user-defined role. db.grantRolesToRole() Specifies roles from which a user-defined role inherits privileges. db.revokeRolesFromRole() Removes inherited roles from a role. db.updateRole() Updates a user-defined role. ff"
    },
    "413": {
        "file": "08 - PyMongo.pdf",
        "page": 0,
        "chunk": "DS 4300 MongoDB + PyMongo Mark Fontenot, PhD Northeastern University"
    },
    "414": {
        "file": "08 - PyMongo.pdf",
        "page": 1,
        "chunk": "PyMongo \u25cfPyMongo is a Python library for interfacing with MongoDB instances 2 from pymongo import MongoClient client = MongoClient( \u2018mongodb://user_name:pw@localhost:27017\u2019 )"
    },
    "415": {
        "file": "08 - PyMongo.pdf",
        "page": 2,
        "chunk": "Getting a Database and Collection 3 from pymongo import MongoClient client = MongoClient( \u2018mongodb://user_name:pw@localhost:27017\u2019 ) db = client[\u2018ds4300\u2019] # or client.ds4300 collection = db[\u2018myCollection\u2019] #or db.myCollection"
    },
    "416": {
        "file": "08 - PyMongo.pdf",
        "page": 3,
        "chunk": "Inserting a Single Document 4 db = client[\u2018ds4300\u2019] collection = db[\u2018myCollection\u2019] post = { \u201cauthor\u201d: \u201cMark\u201d, \u201ctext\u201d: \u201cMongoDB is Cool!\u201d, \u201ctags\u201d: [\u201cmongodb\u201d, \u201cpython\u201d] } post_id = collection.insert_one(post).inserted_id print(post_id)"
    },
    "417": {
        "file": "08 - PyMongo.pdf",
        "page": 4,
        "chunk": "Find all Movies from 2000 5 from bson.json_util import dumps # Find all movies released in 2000 movies_2000 = db.movies.find({\"year\": 2000}) # Print results print(dumps(movies_2000, indent = 2))"
    },
    "418": {
        "file": "08 - PyMongo.pdf",
        "page": 5,
        "chunk": "Jupyter Time - Activate your DS4300 conda or venv python environment - Install pymongo with pip install pymongo - Install Jupyter Lab in you python environment - pip install jupyterlab - Download and unzip > this < zip \ufb01le - contains 2 Jupyter Notebooks - In terminal, navigate to the folder where you unzipped the \ufb01les, and run jupyter lab 6"
    },
    "419": {
        "file": "08 - PyMongo.pdf",
        "page": 6,
        "chunk": "?? 7"
    },
    "420": {
        "file": "Class Notes.pdf",
        "page": 0,
        "chunk": "1/8/2025 - Foundations \u25cf\u200b Searching \u25cb\u200b Most common operation performed by a database system \u25cb\u200b Searching is why we have databases \u25cb\u200b SQL Select is most versatile/complex \u25cb\u200b Baseline for efficiency is Linear Search \u25a0\u200b Start at beginning of a list and proceed element by element until: \u25cf\u200b You find what you are looking for or you get the last element in the list \u25cb\u200b Record - a collection of values for attributes of a single entity instance; row in a table \u25cb\u200b Collection - a set of records of the same entity type; table \u25cb\u200b Search key - a value for an attribute from the entity type \u25a0\u200b Could be >= 1 attribute \u25cf\u200b List of records \u25cb\u200b If each record takes up x bytes of memory, then for n records, we need n*x bytes of memory \u25cb\u200b Contiguous allocated list \u25a0\u200b All n*x bytes are allocated as a single \u201cchunk\u201d of memory \u25cb\u200b Linked list \u25a0\u200b Each record needs x bytes + additional space for 1 or 2 memory addresses \u25cf\u200b That quantity times n is how much space it takes uo \u25a0\u200b Individual records are linked together in a type of chain using memory addresses \u25cb\u200b \u25cf\u200b Pros and Cons \u25cb\u200b Arrays are faster for random access, but slow for inserting anywhere but the end \u25a0\u200b"
    },
    "421": {
        "file": "Class Notes.pdf",
        "page": 1,
        "chunk": "\u25cb\u200b Linked lists are faster for interesting anywhere in the list, but slower for random access \u25a0\u200b Insert \u25cb\u200b Numpy is arrays that are contiguously allocated \u25cf\u200b Observations \u25cb\u200b Arrays \u25a0\u200b Fast for random access \u25a0\u200b Slow for random insertions \u25cb\u200b Linked lists \u25a0\u200b Slow for random access \u25a0\u200b Fast for random insertions \u25cf\u200b Binary Search \u25cb\u200b Input: array of values in sorted order, target value \u25cb\u200b Output: the location (index) of where target is located or some value indicating target was not found \u25cb\u200b Go in the middle, can eliminate half if the item is smaller or larger \u25cb\u200b Recursive in nature but can be dangerous in large data \u25a0\u200b Probably over 30k recursive calls \u25cb\u200b \u25cb\u200b Worst case of most searches needing to be done to find the value or determine it isn\u2019t in the set is log base 2 (n) - log2(n) \u25cb\u200b Example in slide is iterative version \u25cf\u200b Time complexity \u25cb\u200b Linear search \u25a0\u200b Best case: target is found at the first element, only 1 comparison \u25a0\u200b Worst case: target is not in the array; n comparisons \u25a0\u200b Therefore, in the worst case, linear search is O(n) time complexity \u25cb\u200b Binary search \u25a0\u200b Best case: target is found at mid; 1 comparison (inside the loop) \u25a0\u200b Worst case: target is not in the array; log2(n) comparisons \u25a0\u200b Therefore, in the worst case, binary search is O(log2(n)) time complexity \u25a0\u200b Can\u2019t perform binary search on an unsorted array \u25cf\u200b Back to database searching"
    },
    "422": {
        "file": "Class Notes.pdf",
        "page": 2,
        "chunk": "\u25cb\u200b Assume data is stored on disk by column id\u2019s value \u25cb\u200b Searching for a specific id = fast \u25cb\u200b But what if we want to search for a specific specialVal? \u25a0\u200b Only option is linear scan of that column \u25a0\u200b Cannot store data on disk sorted by both id and specialVal (at the same time) \u25cf\u200b Data would have to be duplicated \u2013 inefficient \u25a0\u200b We need an external data structure to support faster searching by specialVal than a linear search \u25cf\u200b What do we have in our arsenal? \u25cb\u200b An array of tuples (specialVal, rowNumber) sorted by specialVal \u25a0\u200b We could use Binary Search to quickly locate a particular specialVal and find its corresponding row in the table \u25a0\u200b But, every insert into the table would be like inserting into a sorted array - slow \u25cb\u200b A linked list of tuples (specialVal, rowNumber) sorted by specialVal \u25a0\u200b Searching for [cont] \u25a0\u200b \u25cf\u200b Something with fast insert and fast search? \u25cb\u200b Binary search tree - a binary tree where every node in the left subtree is less than its parent and every node in the right subtree is greater than its parent \u25a0\u200b Recursive data structure which lends itself to recursive formulas"
    },
    "423": {
        "file": "Class Notes.pdf",
        "page": 3,
        "chunk": "1/16/2025 - Practical outline \u25cf\u200b A.json \u25cb\u200b \u2018Preprocessed_text\u2019: [\u2018bank, \u2018finance\u2019, etc.] \u25cf\u200b Finance articles \u25cb\u200b Jan 2018 etc. up to May \u25cb\u200b Each folder has 50k+ json files \u25cf\u200b Give program path to root folder and then have to recursively descend through the folder \u25cf\u200b BST_NODE \u25cb\u200b Key is going to be a word \u25a0\u200b Values will have the files \u25cf\u200b If bank is in a.json and c.json, key would be bank, value would be a.json, c.json \u25cb\u200b Add-values: goes to values list and appends the new value \u25a0\u200b Self.values \u25a0\u200b Consider if bank appears twice in the file \u25cf\u200b Add it twice, add it only once \u25cf\u200b Prob not valuable information to have it twice \u25cb\u200b Put preprocessed texts into a set to to remove duplicates \u25cb\u200b Use string comparison to determine greater or less than \u25cf\u200b BST_index \u25cb\u200b Abstract index is a type of inheritance defined in abstrac_ index.py \u25a0\u200b Abstract is its own class \u25a0\u200b Can change abstract \u25cb\u200b Abstract method (class that implements) have to provide implementation for abstract method \u25cb\u200b Line 145 insert function \u25a0\u200b Takes a key and a single value \u25cf\u200b Value is name of file currently being parsed \u25cb\u200b Line 28 insert recursive \u25a0\u200b Look up recursive \u25cf\u200b AVL_TREE_NODE \u25cb\u200b Has everything BST has \u25cb\u200b Insert recursive \u25a0\u200b Height is starting at 1 \u25cb\u200b If not root: \u25cb\u200b Node = AVLNode(key) \u25cb\u200b node.add_value(value) \u25a0\u200b Return node \u25cb\u200b Elif key < root.key: \u25a0\u200b Root.left = self.insert_recursive(current.left, key value) \u25cb\u200b Elif key > root.key \u25a0\u200b Root.right = self_insert_recurisve(root.left, key, value) \u25cb\u200b Elif key == root,key"
    },
    "424": {
        "file": "Class Notes.pdf",
        "page": 3,
        "chunk": "\u25a0\u200b Root.right = self_insert_recurisve(root.left, key, value) \u25cb\u200b Elif key == root,key"
    },
    "425": {
        "file": "Class Notes.pdf",
        "page": 4,
        "chunk": "\u25cb\u200b Update height \u25a0\u200b Height_left = (0 if not root.left else root.left.height) \u25a0\u200b Height_right = (0 if not root.right else root.right.height) \u25a0\u200b Root.heigh = 1 +(height_left if (height_left > ehgith_right) else height_right) \u25cb\u200b Equivalent of max function \u25a0\u200b Balance = hieght_left - height_right \u25a0\u200b If balance > 1: \u25cf\u200b If key< root.left.key: \u25cb\u200b Return self._rotate_right(root) \u25cf\u200b Else: \u25cb\u200b Root.left = self._rotate_left(root.left) \u25cb\u200b Return self.rotateright(root) \u25a0\u200b If balance < -1: \u25cf\u200b If key > root.right/key: \u25cb\u200b Return self.rortateleft(root) \u25cf\u200b Else: \u25cb\u200b Root.right self.rotateright(root.right) \u25cb\u200b Return self.rotateleft(root) \u25a0\u200b Return root \u25cf\u200b Rotate left \u25cb\u200b Y = x.right \u25cb\u200b Tw = y.left \u25cb\u200b Y.left = x \u25cb\u200b X.right = tw \u25cb\u200b Ht_x_left = (0 if not x.left else x.left_h \u25cb\u200b Should be using functions and not all if checkers \u25a0\u200b If statement is faster than a function call \u25a0\u200b Make sure its not taking too long \u25cf\u200b Use pickle \u25cb\u200b Library in python that serializing objects to disk so that it can read it in much faster 1/27/25 \u25cf\u200b Benefits of the relational model \u25cb\u200b (mostly) standard data model and query language \u25cb\u200b ACID compliance \u25a0\u200b Atomicity, consistency, isolation, durability \u25cb\u200b Works well will highly structured data \u25cb\u200b Can handle large amounts of data \u25cb\u200b Well understood, lots of tooling, lots of experience \u25cb\u200b A transaction is a unit of work for a database \u25cf\u200b Relational database performance \u25cb\u200b Many ways that a RDBMS increases efficiency:"
    },
    "426": {
        "file": "Class Notes.pdf",
        "page": 5,
        "chunk": "\u25a0\u200b Indexing (focus of class) \u25a0\u200b Directly controlling storage \u25a0\u200b Column oriented storage vs row oriented storage \u25a0\u200b Query optimization \u25a0\u200b caching/prefetching \u25a0\u200b materialized views \u25a0\u200b Precompiled stored procedures \u25a0\u200b Data replication and partitioning \u25cf\u200b Transaction processing \u25cb\u200b Transaction - a sequence of one or more of the CRUD operations performed as a single, logical unit of work \u25a0\u200b Either the entire sequence succeeds (COMMIT) \u25a0\u200b OR the entire sequence fails (ROLLBACK or ABORT) \u25cb\u200b Help ensure \u25a0\u200b Data integrity \u25a0\u200b Error recovery \u25a0\u200b Concurrency control \u25a0\u200b Reliable data storage \u25a0\u200b Simplified error handling \u25cf\u200b Acid properties \u25cb\u200b Characteristics or properties of transactions that ensure the safety of database and the integrity of the database that would be detrimental if don't exist \u25cb\u200b Atomicity \u25a0\u200b Transaction is treated as an atomic unit - it is fully executed or no parts of it are executed \u25cb\u200b Consistency \u25a0\u200b A transaction takes a database from one consistent state to another consistent state \u25a0\u200b Consistent state - all data meets integrity constraints \u25cb\u200b Isolation \u25a0\u200b Two transactions T1 and T2 are being executed at the same time but cannot affect each other \u25a0\u200b If both T1 and T2 are reading the data - no problem \u25a0\u200b Ensured through \u201clocking\u201d \u25a0\u200b If T1 is reading the same data that T2 may be writing, can result in: \u25cf\u200b Dirty read \u25cb\u200b A transaction T1 is able to read a row that has been modified by another transaction T2 that hasn\u2019t yet executed a COMMIT"
    },
    "427": {
        "file": "Class Notes.pdf",
        "page": 6,
        "chunk": "\u25cf\u200b Non-repeatable read \u25cb\u200b Two queries in a single transaction T1 execute a SELECT but get different values because another transaction T2 has changed data and COMMITTED \u25cb\u200b \u25cf\u200b Phantom read \u25cb\u200b When a transaction T1 is running and another transaction T2 adds or deletes rows from the set T1 is using \u25cb\u200b \u25cb\u200b Durability \u25a0\u200b Once a transaction is completed and committed successfully, its changes are permanent \u25a0\u200b Even in the event of a system failure, committed transactions are preserved \u25a0\u200b If data disappears, want to be able to get it back \u25cf\u200b Relational Databases may not be the solution to all problems: \u25cb\u200b Sometimes, schema evolve over time \u25cb\u200b Not all apps may need the full strength of ACID compliance \u25cb\u200b Joins can be expensive \u25cb\u200b A lot of data is semi-structured or unstructured (JSON, XML, etc) \u25cb\u200b Horizontal scaling presents challenges \u25cb\u200b Some apps need something more performant (real time, low latency systems) \u25cf\u200b Scalability up or out \u25cb\u200b Conventional wisdom:"
    },
    "428": {
        "file": "Class Notes.pdf",
        "page": 7,
        "chunk": "\u25a0\u200b Scale vertically (up, with bigger, more powerful systems) until the demands of high-availability make it necessary to scale out with some type of distributed computing model \u25cb\u200b But why? \u25a0\u200b Scaling up is easier - no need to really modify your architecture. But there are practical and financial limits \u25cb\u200b However \u25a0\u200b There are modern systems that make horizontal scaling less problematic \u25cb\u200b \u25cb\u200b More power is not always the answer however it is the easiest thing to do \u25cf\u200b So what Distributed data when scaling out \u25cb\u200b A distributed system is \u201ca collection of independent computers that appear to its users as one computer\u201d \u25cb\u200b Characteristics of distributed system \u25a0\u200b Computers operate concurrently \u25a0\u200b Computers fail independently \u25a0\u200b No shared global clock \u25cf\u200b Distributed storage - 2 directions \u25cb\u200b \u25cf\u200b Distributed data stores \u25cb\u200b Data is stored on > 1 node, typically replicated \u25a0\u200b I.e. each block of data is available on N nodes"
    },
    "429": {
        "file": "Class Notes.pdf",
        "page": 8,
        "chunk": "\u25cb\u200b Distributed databases can be relational or non-relational \u25a0\u200b MySQL and PostgreSQL support replication and sharding \u25a0\u200b CockroachDB - new player on the scene \u25a0\u200b Many NoSQL systems support one or both models \u25cb\u200b But remember: Network partitioning is inevitable \u25a0\u200b Network failures, system failures \u25a0\u200b Overall system needs to be a partition tolerant \u25cf\u200b System can keep running even with network partition \u25cf\u200b The CAP Theorem \u25cb\u200b \u25cb\u200b Can always have two of these but can never have all three \u25cb\u200b Consistency here is not necessarily same consistency as ACID \u25cb\u200b Availability is can i get to it all the time \u25cb\u200b If something breaks up system, can i still read and write from the partitions \u25cb\u200b CAP Theorem states that it is impossible for a distributed data store to simultaneously provide more than two out of the following three guarantees: \u25a0\u200b Consistency: every read receives the most recent write or error thrown \u25cf\u200b Every user of the DB has an identical view of the data at any given instant \u25a0\u200b Availability: every request receives a (non-error) response - but no guarantee that the response contains the most recent write \u25cf\u200b In the event of a failure, the database remains operational \u25a0\u200b Partition tolerance - the system can continue to operate despite arbitrary network issues \u25cf\u200b The database can maintain operations in the event of the network\u2019s failing between two segments of the distributed system"
    },
    "430": {
        "file": "Class Notes.pdf",
        "page": 9,
        "chunk": "\u25cb\u200b \u25cb\u200b Consistency + Availability: System always responds with the latest data and every request gets a response, but may not be able to deal with network issues (network partitions) \u25a0\u200b If you make a request and network has been partitioned, don\u2019t know what you will get back or could get an error \u25cb\u200b Consistency + Partition tolerance: If system responds with data from a distributed store, it is always the latest, else data request is dropped \u25a0\u200b \u201cElse..dropped\u201d is lack of availability \u25a0\u200b If gets partitioned, data may not be consistent and can\u2019t always access it \u25cb\u200b Availability + Partition Tolerance: system always sends responses based on distributed data store, but may not be the absolute latest data \u25a0\u200b \u201cStale data\u201d \u25cf\u200b Cap in reality \u25cb\u200b What it is really saying: \u25a0\u200b If you cannot limit the number of faults, requests can be directed to any server, and you insist on serving every request, then you cannot possibly be consistent \u25cb\u200b But it is interpreted as: \u25a0\u200b You must always give up something: availability, consistency, tolerance to failure 2/3/25 \u25cf\u200b Distributed DBs and ACID - Pessimistic Concurrency \u25cb\u200b ACID transaction"
    },
    "431": {
        "file": "Class Notes.pdf",
        "page": 10,
        "chunk": "\u25cb\u200b If something bad could happen, it will, and attempts to prevent a conflict from happening \u25cb\u200b Focuses on \u201cdata safety\u201d \u25cb\u200b Considered a pessimistic concurrency model because it assumes one transaction has to protect itself from other transactions \u25a0\u200b IOW, it assumes if something can go wrong it will \u25cb\u200b Conflicts are prevented by locking resources until a transaction is complete (there are both read and write locks) \u25cb\u200b Write Lock Analogy \u2192 borrowing a book from a library\u2026If you have it, no one else can \u25cb\u200b Typically when you start a transaction, transaction processing unit can look and see what needs to be locked if anything \u25cf\u200b Optimistic concurrency \u25cb\u200b Transactions don\u2019t obtain locks on data when they read or write \u25cb\u200b Optimistic because it assumes conflicts are unlikely to occur \u25a0\u200b Even if there is a conflict, everything will be ok \u25cb\u200b But how? \u25a0\u200b Add last update timestamp and version number to columns to every table\u2026 read them when changing. THEN, check at the end of transaction to see if any other transaction has caused them to be modified \u25cb\u200b Low conflict systems (backups, analytical dbs, etc) \u25a0\u200b Read heavy systems \u25a0\u200b The conflicts that arise can be handled by rolling back and re-running a transaction that notices a conflict \u25a0\u200b So optimistic concurrency works well - allows for higher concurrency \u25cb\u200b High conflict Systems \u25a0\u200b Rolling back and rerunning transactions that encounter a conflict \u2192 less efficient \u25a0\u200b So, a locking scheme (pessimistic model) might be preferable \u25cb\u200b When lots of people involved, may not want to use this model \u25cf\u200b NOSQL \u25cb\u200b First used to describe a relational database system that did not use SQL \u25a0\u200b 1998 - Carlo Strozzi \u25cb\u200b More common, modern meaning is \u201cNot Only SQL\u201d \u25cb\u200b But, sometimes thought of"
    },
    "432": {
        "file": "Class Notes.pdf",
        "page": 10,
        "chunk": "be preferable \u25cb\u200b When lots of people involved, may not want to use this model \u25cf\u200b NOSQL \u25cb\u200b First used to describe a relational database system that did not use SQL \u25a0\u200b 1998 - Carlo Strozzi \u25cb\u200b More common, modern meaning is \u201cNot Only SQL\u201d \u25cb\u200b But, sometimes thought of as \u201cnon-relational databases\u201d \u25cb\u200b Idea originally developed, in part, as a response to processing unstructured web-based data \u25cf\u200b ACID Alternative for Distribution Systems - BASE \u25cb\u200b Basically Available \u25a0\u200b Guarantees the availability of the data (per CAP), but response can be \u201cfailure\u201d/\u201dunreliable\u201d because the data is an inconsistent or changing state \u25a0\u200b System appears to work most of the time \u25cb\u200b Soft State"
    },
    "433": {
        "file": "Class Notes.pdf",
        "page": 11,
        "chunk": "\u25a0\u200b The state of the system could change over time, even without input. Changes could be the result of eventual consistency \u25cf\u200b Data stores don\u2019t have to be write-consistent \u25cf\u200b Replicas don\u2019t have to be mutually consistent \u25cb\u200b Eventual Consistency \u25a0\u200b The system will eventually become consistent \u25cf\u200b All writes will eventually stop so all nodes/replicas can be updated \u25cb\u200b Many DB systems that support replication of nodes will support the three things of BASE to some degree \u25cf\u200b Categories of NoSQL DBs \u25cb\u200b Document databases \u25a0\u200b Like json \u25cb\u200b Graph databases \u25a0\u200b Things with lots of relationships between the different points?? \u25cb\u200b Key-value databases \u25cb\u200b Columnar databases \u25cb\u200b Vector databases \u25cf\u200b Key Value Stores \u25cb\u200b Or key value databases \u25cb\u200b Feel similar to operating a hash table in python \u25cb\u200b key = value \u25cb\u200b Designed around 3 things \u25a0\u200b Simplicity \u25cf\u200b The data model is extremely simple \u25cf\u200b Comparatively, tables in a RDBMS are very complex \u25cf\u200b Lends itself to simple CRUD ops and API creation \u25a0\u200b Speed \u25cf\u200b Usually developed as in-memory DB \u25cf\u200b Retrieving a value given its key is typically a O(1) op b/c hash tables or similar data structures used under the hood \u25cf\u200b No concept of complex queries or joins\u2026they slow things down \u25a0\u200b Scalability \u25cf\u200b Horizontal scaling is simple - add more nodes \u25cf\u200b Typically concerned with eventual consistency, meaning in a distributed environment, the only guarantee is that all nodes will eventually converge on the same value \u25cf\u200b KV DS Use Cases \u25cb\u200b EDA/Experimentation Results Store \u25a0\u200b Store intermediate results from data preprocessing and EDA \u25a0\u200b Store experiment or testing (A/B) results w/o prod db \u25cb\u200b Feature Store \u25a0\u200b Store frequently accessed feature \u2192 low-latency retrieval for model training and prediction \u25cb\u200b Model Monitoring"
    },
    "434": {
        "file": "Class Notes.pdf",
        "page": 11,
        "chunk": "\u25cb\u200b EDA/Experimentation Results Store \u25a0\u200b Store intermediate results from data preprocessing and EDA \u25a0\u200b Store experiment or testing (A/B) results w/o prod db \u25cb\u200b Feature Store \u25a0\u200b Store frequently accessed feature \u2192 low-latency retrieval for model training and prediction \u25cb\u200b Model Monitoring"
    },
    "435": {
        "file": "Class Notes.pdf",
        "page": 12,
        "chunk": "\u25a0\u200b Store key metrics about performance of model, for example, in real-time inferencing \u25cf\u200b KV SWE Use Cases \u25cb\u200b Caching scenarios \u25cb\u200b Storing Session Information \u25a0\u200b Everything about the current session can be stored via a single PUT or POST and retrieved with a single GET \u2026 very fast \u25cb\u200b User Profiles & Preferences \u25a0\u200b User info could be obtained with a single GET operation \u2026 language, TZ, product or UI preferences \u25cb\u200b Shopping Cart Data \u25a0\u200b Cart data is tied to the user \u25a0\u200b Needs to be available across browser, machines, session \u25cb\u200b Caching Layer \u25a0\u200b In front of a disk-based database \u25cf\u200b Consider efficiency of KV because we can keep everything memory resident \u25cf\u200b Redis DB \u25cb\u200b Remote Directory Server \u25a0\u200b Open source, in-memory database \u25a0\u200b Sometimes called a data structure store \u25a0\u200b Primarily a KV store, but can be used with other models: Graph, Spatial, Full Text Search, Vector, Time Series \u25cb\u200b It is considered an in-memory database system but\u2026. \u25a0\u200b Supports durability of data by \u25cf\u200b Essentially saving snapshots to disk at specific intervals OR \u25cf\u200b Append-only file chick is a journal of changes that can be used for roll-forward if there is af failure \u25cf\u200b Can be very fast .. >100,000 SET ops/second \u25cf\u200b Rich collection of commands \u25cf\u200b Does NOT handle complex data. No secondary indexes. Only supports lookup by Key \u25cf\u200b Redis Data Types \u25cb\u200b Keys: \u25a0\u200b Usually strings but can be binary sequence \u25cb\u200b Values: \u25a0\u200b Strings \u25a0\u200b Lists (linked lists) \u25a0\u200b Sets (unique unsorted string elements) \u25a0\u200b Sorted sets \u25a0\u200b Hashes (string \u2192 string) \u25a0\u200b Geospatial data \u25cf\u200b Redis Databases and Interaction \u25cb\u200b Redis provides 16 databases by default \u25a0\u200b They are numbered 0 to 15"
    },
    "436": {
        "file": "Class Notes.pdf",
        "page": 12,
        "chunk": "Sets (unique unsorted string elements) \u25a0\u200b Sorted sets \u25a0\u200b Hashes (string \u2192 string) \u25a0\u200b Geospatial data \u25cf\u200b Redis Databases and Interaction \u25cb\u200b Redis provides 16 databases by default \u25a0\u200b They are numbered 0 to 15"
    },
    "437": {
        "file": "Class Notes.pdf",
        "page": 13,
        "chunk": "\u25a0\u200b There is no other name associated \u25cb\u200b DIrect interaction with Redis is through a set of commands related to setting and getting k/v pairs (and variations) \u25cb\u200b Many language libraries available as well \u25cf\u200b Foundation Data Type - String \u25cb\u200b Sequence of bytes - text, serialized objects, bin arrays \u25cb\u200b Simplest data type \u25cb\u200b Maps a string to another string \u25cb\u200b Use cases: \u25a0\u200b Caching frequently accessed HTML/CSS/JS fragments \u25a0\u200b Config. Settings, user settings info, token management \u25a0\u200b Counting web page/app screen views or rate limiting \u25cf\u200b Basic command \u25cb\u200b SETNX will only set a key if it doesn\u2019t already exist in the table \u25cb\u200b SET someValue 0 \u25cb\u200b INRC someValue \u25a0\u200b Considered atomic operations, won\u2019t end up have two transactions trying to increment same number at same time \u25cb\u200b INCRBY someValue 10 \u25cb\u200b DECR someValue \u25cb\u200b DECRBY someValue 5 \u25a0\u200b INCR parses the value as an int and increments (or adds to value) \u25cb\u200b \u25cf\u200b Hash Type\u200b \u25cb\u200b Value of KV entry is a collection of field-value pairs \u25cb\u200b Use cases: \u25a0\u200b Can be used to represent basic objects/structures \u25cf\u200b Number of field/value pairs per has is 2^31-1 \u25cf\u200b Practical limit: available system resource (e.g. memory) \u25a0\u200b Session information management \u25a0\u200b User/Event tracking (could include TTL) \u25a0\u200b Active Session Tracking (all sessions under one hash key) \u25cf\u200b Hash Commands \u25cb\u200b"
    },
    "438": {
        "file": "Class Notes.pdf",
        "page": 14,
        "chunk": "\u25cf\u200b As long as for data in table, only searching for primary key, this can become close to what a relational database is like \u25cb\u200b Search only by username \u25cb\u200b But if ever want to search by email, can\u2019t do it in redis \u25cf\u200b List Type\u200b \u25cb\u200b Value of KV Pair is linked lists of string values \u25cb\u200b Use cases: \u25a0\u200b Implementation of stacks and queues \u25a0\u200b Queue management and message passing queues (producer/consumer model) \u25a0\u200b Logging systems (easy to keep in chronological order) \u25a0\u200b Build social media streams/feeds \u25a0\u200b Message history in a chat application \u25a0\u200b Batch processing by queueing up a set of tasks to be executed sequentially at a later time \u25cf\u200b Linked Lists crash course \u25cb\u200b Sequential data structure of linked nodes (instead of contiguously allocated memory) \u25cb\u200b Each node points to the next element of the list (except the last points to nil/null) \u25cb\u200b O(1) to insert new value at front or insert new value at the end \u25cb\u200b \u25cb\u200b push/pop from same side operates like a stack \u25cb\u200b push/pop from alternate sides operates like a queue \u25cf\u200b JSON Type \u25cb\u200b Full support of the JSON standard \u25cb\u200b Uses JSONPath syntax for parsing/navigating a JSON document \u25cb\u200b Internally, stored in binary in a tree-structure \u2192 fast access to sub elements \u25cf\u200b Set Type \u25cb\u200b Unordered collection of unique strings (members) \u25cb\u200b Use cases: \u25a0\u200b Track unique items \u25a0\u200b Primitive relation \u25a0\u200b Access control list for users and permission structures \u25a0\u200b Social network friends list \u25cb\u200b Supports set operations \u25cf\u200b Write a python script that will parallelize the parsing of json files to maximize the throughput that redis can handle MongoDB Lecture \u25cf\u200b Document Database"
    },
    "439": {
        "file": "Class Notes.pdf",
        "page": 14,
        "chunk": "Supports set operations \u25cf\u200b Write a python script that will parallelize the parsing of json files to maximize the throughput that redis can handle MongoDB Lecture \u25cf\u200b Document Database"
    },
    "440": {
        "file": "Class Notes.pdf",
        "page": 15,
        "chunk": "\u25cb\u200b A non-relatinal database that stores data as structured documents usually in json \u25cb\u200b JSON = JavaScript Object Notation \u25a0\u200b A lightweight data-interchange format \u25a0\u200b It is easy for humans to read and write \u25a0\u200b Its easy for machines to parse and generate \u25cb\u200b JSON is built on two structure \u25a0\u200b A collection of name/value pairs. In various languages, this is operationalized as an object, record, struct, dictionary, hash table, keyed list, or associative array \u25a0\u200b An ordered list of values. In most languages, this is operationalized as an array, vector, list, or sequence \u25cb\u200b These are two universal data structures supported by virtually all modern programming languages \u25a0\u200b Thus, json makes a great data interchange \u25cf\u200b Binary Json, BSON \u25cb\u200b BSON \u2192 Binary Json \u25a0\u200b Binary encoded serialization of a JSON-like document structure \u25a0\u200b Supports extended types not part of basic JSON (e.g. data, BinaryDAte, etc.) \u25a0\u200b Lose the human readability aspect \u25a0\u200b Lightweight - keep space overhead to a minimum \u25a0\u200b Traversable - designd to be easily traversed, which is vitially important tot a document DB \u25cf\u200b XML (extensible markup language) \u25cb\u200b Precursor to JSON as data exchange format \u25cb\u200b XML + CSS \u2192 web pages that separated content and formatting \u25cb\u200b Structurally similar to HTML, but tag set is extensible \u25cf\u200b Why Document Databases \u25cb\u200b Document databases address the impedance mismatch problem between object persistence in OO systems and how relational databases structure data \u25a0\u200b OO programming \u2192 inheritance and composition of types \u25a0\u200b How do we save a complex object to a relational database? \u25cf\u200b We basically have to deconstruct it \u25cb\u200b The structure of a document is self-describing \u25cb\u200b They are well-aligned with apps that use JSON/XML as a transport layer Introduction to the Graph Data Model \u25cf\u200b What is a graph database \u25cb\u200b Data"
    },
    "441": {
        "file": "Class Notes.pdf",
        "page": 15,
        "chunk": "save a complex object to a relational database? \u25cf\u200b We basically have to deconstruct it \u25cb\u200b The structure of a document is self-describing \u25cb\u200b They are well-aligned with apps that use JSON/XML as a transport layer Introduction to the Graph Data Model \u25cf\u200b What is a graph database \u25cb\u200b Data model based on the graph data structure \u25cb\u200b Composed of nodes and edges \u25a0\u200b Edges connect nodes \u25a0\u200b Each is uniquely identified \u25a0\u200b Each can contain properties (e.g. name, occupation, etc)"
    },
    "442": {
        "file": "Class Notes.pdf",
        "page": 16,
        "chunk": "\u25a0\u200b Supports queries based on graph-oriented operations \u25cf\u200b Traversals \u25cf\u200b Shortest path \u25cf\u200b Lots of others \u25cf\u200b Where do graphs show up? \u25cb\u200b Social networks \u25a0\u200b Modeling social interactions in fields like psychology and sociology \u25a0\u200b Social media as well \u25cb\u200b The web \u25a0\u200b It is just a big graph of \u201cpages\u201d nodes connected by hyperlinks (edges) \u25cb\u200b Chemical and biological data \u25a0\u200b Systems biology, genetics, etc. \u25a0\u200b Interaction relationships in chemistry \u25cf\u200b What is a graph? \u25cb\u200b Labeled property graph \u25a0\u200b Composed of a set of node (vertex) objects and relationship (edge) objects \u25a0\u200b Labels are used to mark a node as part of a group \u25a0\u200b Properties are attributes (think KV pairs) and can exist on nodes and relationships \u25a0\u200b Nodes with no associated relationships are OK. \u25a0\u200b Edges not connected to nodes are not permitted \u25cf\u200b Example \u25cb\u200b \u25cf\u200b Paths \u25cb\u200b A path is an ordered sequence of nodes connected by edges in which no nodes or edges are repeated"
    },
    "443": {
        "file": "Class Notes.pdf",
        "page": 17,
        "chunk": "\u25a0\u200b \u25cf\u200b Flavors of Graphs \u25cb\u200b Connected (vs Disconnected): there is a path between any two nodes in the graph \u25a0\u200b N choose 2 \u25a0\u200b No representation of how long or direct but can follow lines from one node to any other nodes \u25a0\u200b \u25cb\u200b Weighted (vs unweighted) - edge has a weight property (important for some algorithms) \u25a0\u200b If an unweighted graph, can consider it a weighted graph where they all have even weights \u25a0\u200b \u25cb\u200b Directed ( vs Undirected) - relationships (edges) define a start and end node"
    },
    "444": {
        "file": "Class Notes.pdf",
        "page": 18,
        "chunk": "\u25a0\u200b \u25cb\u200b Acyclic (vs Cyclic) - graph contains no cycles \u25a0\u200b \u25cb\u200b Sparse vs Dense \u25a0\u200b \u25cb\u200b Trees \u25a0\u200b \u25cf\u200b Types of Graph Algorithms - Pathfinding \u25cb\u200b Pathfinding"
    },
    "445": {
        "file": "Class Notes.pdf",
        "page": 19,
        "chunk": "\u25a0\u200b Finding the shortest path between two nodes, if one exists, is probably the most common operation \u25a0\u200b \u201cShortest\u201d means fewest edges or lowest weight \u25a0\u200b Average shortest path can be used to monitor efficiency and resiliency of networks \u25a0\u200b Minimum spanning tree, cycle detection, max/min flow\u2026 are other types of pathfinding \u25cf\u200b BFS vs DFS \u25cb\u200b \u25a0\u200b Moves from light to dark \u25cf\u200b Shortest path \u25cb\u200b \u25cf\u200b Types of Graph Algorithms - Centrality & Community Detection \u25cb\u200b Centrality \u25a0\u200b Determining which nodes are \u201cmore important\u201d in a network compared to other nodes \u25a0\u200b EX: Social Network Influencers?"
    },
    "446": {
        "file": "Class Notes.pdf",
        "page": 20,
        "chunk": "\u25a0\u200b \u25cb\u200b Community Detection \u25a0\u200b Evaluate clustering or partitioning of nodes of a graph and tendency to 7strengthen or break apart \u25cf\u200b Some Famous Graph Algorithms \u25cb\u200b Dijkstra\u2019s Algorithm - single-source shortest path algo for positively weighted graphs \u25cb\u200b A* Algorithm - similar to Dijkstra\u2019s with added feature of using a heuristic to guide traversal \u25cb\u200b PageRank - measures the importance of each node within a graph based on the number of incoming relationships and the importance of the nodes from those incoming relationships \u25cf\u200b Neo4j \u25cb\u200b A graph database system that supports both transactional and analytical processing of graph-based data \u25cb\u200b Relatively new class of no-sql DBs \u25cb\u200b Considered schema optional (one can be imposed) \u25cb\u200b Supports various types of indexing \u25cb\u200b ACID compliant \u25cb\u200b Supports distributed computing \u25cb\u200b Similar: Microsoft CosmoDB, Amazon Neptune \u25cf\u200b Maximum port number is 65535 \u25cf\u200b 0-1023 are ports reserved for root access \u25cf\u200b Docker compose commands must be run in the same folder where the docker yml file is In Class 2/24 \u25cf\u200b Take class notes \u25cf\u200b Generate embeddings \u25cb\u200b Chunk: file name, pdf page, vector \u25cf\u200b Store in a redis-stack \u25cf\u200b On other side with person:"
    },
    "447": {
        "file": "Class Notes.pdf",
        "page": 21,
        "chunk": "\u25cb\u200b Questio embedding \u25a0\u200b Get 10 most similar embeddings \u25cf\u200b Retrieval Augmented Generation (RAG) AWS Introduction \u25cf\u200b Amazon Web Services \u25cb\u200b Leading Cloud Platform with over 200 different services available \u25cb\u200b Globally available via its massive networks of regions and availability zones with their massive data centers \u25a0\u200b Breaks everything into regions and then into availability zones \u25cf\u200b Availability zone think of like a giant data center \u25cb\u200b Based on a pay-as-you-use cost model \u25a0\u200b Theoretically cheaper than renting rackspace/servers in a data center\u2026Theoretically \u25cf\u200b History of AWS \u25cb\u200b Originally launched in 2006 with only 2 servies: S3 and EC2 \u25cb\u200b By 2010, services had expanded to include SimpleDB, Elastic Block Store, Relational Database Service, DynamoDB, CloudWatch, Simple Workflow, CloudFront, Availability Zones, and others \u25cb\u200b CVN = content delivery network \u25cb\u200b Amazon had competitions with big prizes to spur the adoption of AWS in its early days \u25cb\u200b They\u2019ve continuously innovated, always introducing new services for ops, dev, analytics, etc \u25cf\u200b Cloud Models \u25cb\u200b IaaS - Infrastructure as a Service \u25a0\u200b Contains the basic services that are needed to build an IT infrastructure \u25cb\u200b PaaS - Platform as a Service \u25a0\u200b Remove the need for having to manage infrastructure \u25a0\u200b You can get right to deploying your app \u25cb\u200b SaaS - Software as a System \u25a0\u200b Provide full software apps that are run and managed by another party/vendor"
    },
    "448": {
        "file": "Class Notes.pdf",
        "page": 22,
        "chunk": "\u25cb\u200b \u25cf\u200b The Shared Responsibility Model - AWS \u25cb\u200b AWS Responsibilities (Security OF the cloud) \u25a0\u200b Security of physical infrastructure (infra) and network \u25cf\u200b Keep the data centers secure, control access to them \u25cf\u200b Maintain power availability, HVAC, etc. \u25cf\u200b Monitor and maintain physical networking equipment and global infra/connectivity \u25a0\u200b Hypervisor & Host OSs \u25cf\u200b Manage the virtualization layer used in AWS compute services \u25cf\u200b Maintaining underlying host OSs for other services \u25a0\u200b Maintaining managed services \u25cf\u200b Keep infra up to date and functional \u25cf\u200b Maintain server software (patching etc.) \u25cb\u200b Client responsibilities (Security IN the cloud) \u25a0\u200b Control of Data/Content \u25cf\u200b Client controls how its data is classified, encrypted, and shared \u25cf\u200b Implement and enforce appropriate data-handling policies \u25a0\u200b Access management and IAM \u25cf\u200b Properly configure IAM users, roles, and policies \u25cf\u200b Enforce the Principle of Least Privilege \u25a0\u200b Manage self-hosted APps and associated oSs \u25a0\u200b Ensure networks security to its VPC \u25a0\u200b Handle compliance and governance policies and procedures \u25cf\u200b The AWS Global Infrastructure"
    },
    "449": {
        "file": "Class Notes.pdf",
        "page": 23,
        "chunk": "\u25cb\u200b Regions - distinct geographical areas \u25a0\u200b Us-east-1, us-west-1, etc \u25cb\u200b Availability Zones (AZs) \u25a0\u200b Each region has multiple AZs \u25a0\u200b Roughly equivalent to isolated data centers \u25cb\u200b Edge Locations \u25a0\u200b Locations for CDN and other types of caching services \u25a0\u200b Allows content to be closer to end user \u25cb\u200b Currently 36 regions totalling 114 availability zones with 700+ POPs (points of presence) with cloudFront \u25cf\u200b Compute services \u25cb\u200b Compute resources \u25a0\u200b Resources where you create them and have them available \u25a0\u200b All the way to on-demand serverless instances \u25cb\u200b VM-based \u25a0\u200b EC2 and EC2 Spot - Elastic Cloud Compute \u25cb\u200b Container Based \u25a0\u200b ECS - Elastic container service \u25a0\u200b ECR - Elastic container registry \u25a0\u200b EKS - Elastic Kubernetes Service \u25a0\u200b Fargate - Serverless container service \u25cb\u200b Serverless: AWLS Lambda \u25cf\u200b Storage Services \u25cb\u200b Each has its own unique icon \u25cb\u200b Amazon S3 - Simple Storage Service \u25a0\u200b Object storage in buckets; highly scalable; different storage classes \u25cb\u200b Amazon EFS - Elastic File System \u25a0\u200b Simple, serverless, elastic, \u201cset and forget\u201d file system \u25cb\u200b Amazon EBS - Elastic Block Storage \u25a0\u200b High-performance block storage service \u25cb\u200b Amazon File Cache \u25a0\u200b High-speed cache for datasets stored anywhere \u25cb\u200b AWS Backup \u25a0\u200b Fully managed, policy-based service to automate data protection and compliance of apps on AWS \u25cf\u200b Database Services \u25cb\u200b Relational - Amazon RDS, Amazon Aurora \u25cb\u200b Key-value - Amazon DynamoDB \u25cb\u200b In-memory - Amazon MemoryDK, Amazon ElastiCache \u25cb\u200b Document - Amazon DocumentDB (compatible with MongoDB) \u25cb\u200b Graph - Amazon Neptune \u25cf\u200b Analytics Services \u25cb\u200b Amazon Athena - Analyze petabyte scale data wher it lives (S3, for example) \u25cb\u200b Amazon EMR - Elastic MapReduce - Access APache Spark, Hive, Presto etc."
    },
    "450": {
        "file": "Class Notes.pdf",
        "page": 23,
        "chunk": "\u25cf\u200b Analytics Services \u25cb\u200b Amazon Athena - Analyze petabyte scale data wher it lives (S3, for example) \u25cb\u200b Amazon EMR - Elastic MapReduce - Access APache Spark, Hive, Presto etc."
    },
    "451": {
        "file": "Class Notes.pdf",
        "page": 24,
        "chunk": "\u25cb\u200b AWS Glue - Discover, prepare, and integrate all your data \u25cb\u200b Amazon Redshift - Data warehousing service \u25cb\u200b Amazon Kinesis - real-time data streaming \u25cb\u200b Amazon QuickSight - cloud-native BI/reporting tool \u25cf\u200b ML and AI Services \u25cb\u200b Amazon SageMaker \u25a0\u200b Fully-managed ML platform, including Jupyter NBs \u25a0\u200b Build, train, deploy ML models \u25cb\u200b AWS AI Services w/ Pre-trained Models \u25a0\u200b Amazon comprehend - NLP \u25a0\u200b Amazon Rekognition - Image/Video analysis \u25a0\u200b Amazon Textract - text extraction \u25a0\u200b Amazon translate - machine translation \u25cf\u200b Important Services for Data Analytics/Engineering \u25cb\u200b EC2 and Lambda \u25a0\u200b Elastic Cloud Compute \u25a0\u200b Scalable Virtual Computing in the cloud \u25a0\u200b Pay as you go \u25a0\u200b Many instance types available \u25a0\u200b Multiple different operating systems \u25cb\u200b Amazon S3 \u25cb\u200b Amazon RDS and DynamoDB \u25cb\u200b AWS Glue \u25cb\u200b Amazon Athena \u25cb\u200b Amazon EMR \u25cb\u200b Amazon Redshift \u25cf\u200b Features of EC2 \u25cb\u200b Elasticity - easily (and programmatically) scale instances up or down as needed \u25cb\u200b You can use one of the standard AMIs or provide your own AMI if pre-config is needed \u25cb\u200b Easily integrates with many other services such as S3, RDS, etc. \u25cf\u200b EC2 Lifestyle \u25cb\u200b Launch - when starting an instance for the first time with a chosen configuration \u25cb\u200b start/stop - temporarily suspend usage without deleting the instance \u25cb\u200b Terminate - permanently delete the instance \u25cb\u200b Reboot - restart an instance without sling the data on the root volume \u25cf\u200b Where can you store data in EC2 \u25cb\u200b Instance Store: Temporary high-speed storage tied to the instance lifecycle \u25cb\u200b EFS (Elastic File System) Support - shared file storage \u25a0\u200b Like a thumb drive \u25cb\u200b EBS (elastic block storage) - persistent block-level storage \u25cb\u200b S3 - large data set storage or EC2 backups even \u25cf\u200b Common EC2 Use Cases \u25cb\u200b Web Hosting - Run a"
    },
    "452": {
        "file": "Class Notes.pdf",
        "page": 24,
        "chunk": "to the instance lifecycle \u25cb\u200b EFS (Elastic File System) Support - shared file storage \u25a0\u200b Like a thumb drive \u25cb\u200b EBS (elastic block storage) - persistent block-level storage \u25cb\u200b S3 - large data set storage or EC2 backups even \u25cf\u200b Common EC2 Use Cases \u25cb\u200b Web Hosting - Run a website/web server and associated apps"
    },
    "453": {
        "file": "Class Notes.pdf",
        "page": 25,
        "chunk": "\u25cb\u200b Data processing - It\u2019s a VM\u2026 you can do anything to data possible with a programming language \u25cb\u200b Machine Learning - Train models using GPU instances \u25cb\u200b Disaster Recovery - Backup critical workloads or infrastructure in the cloud Lambdas 3/17/25 \u25cf\u200b Lambdas provide serverless computing \u25cf\u200b Automatically run code in response to events \u25cf\u200b Relieves you from having to manage servers - only worry about the code \u25cf\u200b You only pay for execution time, not for idle compute time (different from EC2) EXAM: \u25cf\u200b Mongo query \u25cf\u200b How to rotate/insert a tree \u25cf\u200b Support mapping"
    },
    "454": {
        "file": "Extended Notes - Cornell B+ Trees.pdf",
        "page": 0,
        "chunk": "B+ Trees The idea we saw earlier of putting multiple set (list, hash table) elements together into large chunks that exploit locality can also be applied to trees. Binary search trees are not good for locality because a given node of the binary tree probably occupies only a fraction of any cache line. B+ trees are a way to get better locality by putting multiple elements into each tree node. B+ trees were originally invented for storing data structures on disk, where locality is even more crucial than with memory. Accessing a disk location takes about 5ms = 5,000,000ns. Therefore, if you are storing a tree on disk, you want to make sure that a given disk read is as effective as possible. B+ trees have a high branching factor, much larger than 2, which ensures that few disk reads are needed to navigate to the place where data is stored. B+ trees may also useful for in-memory data structures because these days main memory is almost as slow relative to the processor as disk drives were to main memory when B+ trees were first introduced! A B+ tree of order m is a search tree in which each nonleaf node has up to m children. The actual elements of the collection are stored in the leaves of the tree, and the nonleaf nodes contain only keys. Each leaf stores some number of elements; the maximum number may be greater or (typically) less than m. The data structure satisfies several invariants: 1.\u200b Every path from the root to a leaf has the same length 2.\u200b If a node has n children, it contains n\u22121 keys. 3.\u200b Every node (except the root) is at least half full 4.\u200b The elements stored in a given subtree all have keys that are between"
    },
    "455": {
        "file": "Extended Notes - Cornell B+ Trees.pdf",
        "page": 0,
        "chunk": "invariants: 1.\u200b Every path from the root to a leaf has the same length 2.\u200b If a node has n children, it contains n\u22121 keys. 3.\u200b Every node (except the root) is at least half full 4.\u200b The elements stored in a given subtree all have keys that are between the keys in the parent node on either side of the subtree pointer. (This generalizes the BST invariant.) 5.\u200b The root has at least two children if it is not a leaf. For example, the following is an order-5 B+ tree (m=5) where the leaves have enough space to store up to 3 data records:"
    },
    "456": {
        "file": "Extended Notes - Cornell B+ Trees.pdf",
        "page": 1,
        "chunk": "Because the height of the tree is uniformly the same and every node is at least half full, we are guaranteed that the asymptotic performance is O(lg n) where n is the size of the collection. The real win is in the constant factors, of course. We can choose m so that the pointers to the m children plus the m\u22121 elements fill out a cache line at the highest level of the memory hierarchy where we can expect to get cache hits. For example, if we are accessing a large disk database then our \"cache lines\" are memory blocks of the size that is read from disk. Lookup in a B+ tree is straightforward. Given a node to start from, we use a simple linear or binary search to find whether the desired element is in the node, or if not, which child pointer to follow from the current node. Insertion and deletion from a B+ tree are more complicated; in fact, they are notoriously difficult to implement correctly. For insertion, we first find the appropriate leaf node into which the inserted element falls (assuming it is not already in the tree). If there is already room in the node, the new element can be inserted simply. Otherwise the current leaf is already full and must be split into two leaves, one of which acquires the new element. The parent is then updated to contain a new key and child pointer. If the parent is already full, the process ripples upwards, eventually possibly reaching the root. If the root is split into two, then a new root is created with just two children, increasing the height of the tree by one. For example, here is the effect of a series of insertions. The first insertion (13) merely affects a"
    },
    "457": {
        "file": "Extended Notes - Cornell B+ Trees.pdf",
        "page": 1,
        "chunk": "ripples upwards, eventually possibly reaching the root. If the root is split into two, then a new root is created with just two children, increasing the height of the tree by one. For example, here is the effect of a series of insertions. The first insertion (13) merely affects a leaf. The second insertion (14) overflows the leaf and adds a key to an internal node. The third insertion propagates all the way to the root."
    },
    "458": {
        "file": "Extended Notes - Cornell B+ Trees.pdf",
        "page": 2,
        "chunk": "Deletion works in the opposite way: the element is removed from the leaf. If the leaf becomes empty, a key is removed from the parent node. If that breaks invariant 3, the keys of the parent node and its immediate right (or left) sibling are reapportioned among them so that invariant 3 is satisfied. If this is not possible, the parent node can be combined with that sibling, removing a key another level up in the tree and possible causing a ripple all the way to the root. If the root has just two children, and they are combined, then the root is deleted and the new combined node becomes the root of the tree, reducing the height of the tree by one. Further reading: Aho, Hopcroft, and Ullman, Data Structures and Algorithms, Chapter 11."
    },
    "459": {
        "file": "Extended Notes - Fontenot B+ Tree Walkthrough.pdf",
        "page": 0,
        "chunk": "Example B+ Tree: m = 4 Step 1 - Insert: 42, 21, 63, 89 -\u200b Initially, the first node is a leaf node AND root node. -\u200b 21, 42, \u2026 represent keys of some set of K:V pairs -\u200b Leaf nodes store keys and data, although data not shown -\u200b Inserting another key will cause the node to split. Step 2 - Insert: 35 -\u200b Leaf node needs to split to accommodate 35. New leaf node allocated to the right of existing node -\u200b 5/2 values stay in original node; remaining values moved to new node -\u200b Smallest value from new leaf node (42) is copied up to the parent, which needs to be created in this case. It will be an internal node. Step 3 - Insert: 10, 27, 96 -\u200b The insert process starts at the root node. The keys of the root node are searched to find out which child node we need to descend to. -\u200b EX: 10. Since 10 < 42, we follow the pointer to the left of 42 -\u200b Note - none of these new values cause a node to split"
    },
    "460": {
        "file": "Extended Notes - Fontenot B+ Tree Walkthrough.pdf",
        "page": 1,
        "chunk": "Step 4 - Insert 30 -\u200b Starting at root, we descend to the left-most child (we\u2019ll call curr). -\u200b curr is a leaf node. Thus, we insert 30 into curr. -\u200b BUT curr is full. So we have to split. -\u200b Create a new node to the right of curr, temporarily called newNode. -\u200b Insert newNode into the doubly linked list of leaf nodes. -\u200b re-distribute the keys -\u200b copy the smallest key (27 in this case) from newNode to parent; rearrange keys and pointers in parent node. -\u200b Parent of newNode is also root. So, nothing else to do Observation: The root node is full. -\u200b The next insertion that splits a leaf will cause the root to split, and thus the tree will get 1 level deeper."
    },
    "461": {
        "file": "Extended Notes - Fontenot B+ Tree Walkthrough.pdf",
        "page": 2,
        "chunk": "Step 4 - Insert 37 -\u200b When splitting an internal node, we move the middle element to the parent (instead of copying it). -\u200b In this particular tree, that means we have to create a new internal node which is also now the root."
    },
    "462": {
        "file": "Extended Notes - Moving Beyond the Relational Model Slide Deck.pdf",
        "page": 0,
        "chunk": "Benefits of Relational Model: -\u200b (Mostly) Standard Data Model and Query Language -\u200b The relational model is widely adopted across industries, providing a structured way to store and manage data in tables with well-defined relationships. -\u200b SQL (Structured Query Language) is the standard query language for relational databases, making it easier for developers, analysts, and data scientists to work with data across different database systems. -\u200b Most relational databases follow common conventions, allowing for easier migration between different database management systems (e.g., MySQL, PostgreSQL, SQL Server). -\u200b ACID Compliance (Atomicity, Consistency, Isolation, Durability) - Ensures reliable transactions by enforcing four key properties: -\u200b Atomicity: A transaction is either fully completed or fully rolled back\u2014no partial updates (aka transaction is treated as an atomic unit - it is fully executed or no parts of it are executed) -\u200b Consistency: The database remains in a valid state before and after a transaction (aka a transaction takes a database from one consistent state to another consistent state consistent state - all data meets integrity constraints) -\u200b Isolation: Transactions execute independently without interfering with each other -\u200b Two transactions T1 and T2 are being executed at the same time but cannot affect each other -\u200b If both T1 and T2 are reading the data - no problem -\u200b If T1 is reading the same data that T2 may be writing, can result in -\u200b Dirty Read: a transaction T1 is able to read a row that has been modified by another transaction T2 that hasn\u2019t yet executed a COMMIT -\u200b Non-repeatable Read: two queries in a single transaction T1 execute a SELECT but get different values because another transaction T2 has changed data and COMMITTED -\u200b Phantom Reads: when a transaction T1 is running and another transaction T2 adds or deletes rows from the"
    },
    "463": {
        "file": "Extended Notes - Moving Beyond the Relational Model Slide Deck.pdf",
        "page": 0,
        "chunk": "hasn\u2019t yet executed a COMMIT -\u200b Non-repeatable Read: two queries in a single transaction T1 execute a SELECT but get different values because another transaction T2 has changed data and COMMITTED -\u200b Phantom Reads: when a transaction T1 is running and another transaction T2 adds or deletes rows from the set T1 is using -\u200b Durability: Once a transaction is committed, its changes are permanently stored, even in the case of a system failure. -\u200b Once a transaction is completed and committed successfully, its changes are permanent. -\u200b Even in the event of a system failure, committed transactions are preserved -\u200b Works Well with Highly Structured Data: -\u200b Best suited for scenarios where data can be organized into predefined schemas with fixed relationships, such as financial records, inventory management, and customer databases."
    },
    "464": {
        "file": "Extended Notes - Moving Beyond the Relational Model Slide Deck.pdf",
        "page": 1,
        "chunk": "-\u200b Ensures data integrity through constraints like primary keys, foreign keys, and unique constraints. -\u200b Enables efficient joins and aggregations, which are crucial for analytical and transactional workloads. -\u200b Can Handle Large Amounts of Data -\u200b Optimized indexing, partitioning, and query optimization techniques allow relational databases to scale efficiently. -\u200b Many enterprise-grade relational databases support horizontal and vertical scaling, handling millions or even billions of rows. -\u200b Used in large-scale applications like banking systems, airline reservation systems, and enterprise resource planning (ERP) software. -\u200b Well Understood, Lots of Tooling, Lots of Experience -\u200b Relational databases have been around for decades, leading to a wealth of knowledge, best practices, and mature ecosystem support. -\u200b A large number of tools exist for database administration, performance tuning, data migration, and backup management. -\u200b Many database professionals, from developers to DBAs, have deep experience in relational databases, making hiring and knowledge transfer easier for organizations. Relational Database Performance -\u200b Many techniques help relational database management systems (RDBMS) optimize efficiency, ensuring fast queries, reduced latency, and scalable performance. 1.\u200b Indexing -\u200b Indexes improve query performance by allowing the database to locate data quickly rather than scanning entire tables. -\u200b Common types of indexes: -\u200b B-Tree Indexes: Used for range and equality searches. -\u200b Hash Indexes: Ideal for exact-match lookups. -\u200b Bitmap Indexes: Efficient for low-cardinality columns (e.g., gender, Boolean values). -\u200b Proper indexing speeds up read operations but can slightly slow down writes due to index maintenance. 2.\u200b Directly Controlling Storage -\u200b RDBMSs manage how data is physically stored on disk, optimizing storage structures for performance. -\u200b Techniques include: -\u200b Data compression to reduce disk I/O. -\u200b Page-level optimizations for fast retrieval. -\u200b Efficient allocation of memory buffers and disk space."
    },
    "465": {
        "file": "Extended Notes - Moving Beyond the Relational Model Slide Deck.pdf",
        "page": 1,
        "chunk": "data is physically stored on disk, optimizing storage structures for performance. -\u200b Techniques include: -\u200b Data compression to reduce disk I/O. -\u200b Page-level optimizations for fast retrieval. -\u200b Efficient allocation of memory buffers and disk space."
    },
    "466": {
        "file": "Extended Notes - Moving Beyond the Relational Model Slide Deck.pdf",
        "page": 2,
        "chunk": "3.\u200b Column-Oriented Storage vs. Row-Oriented Storage -\u200b Row-Oriented Storage: Stores entire rows together; efficient for transactional (OLTP) workloads where frequent inserts, updates, and deletes occur. -\u200b Column-Oriented Storage: Stores data by columns instead of rows, improving performance for analytical (OLAP) queries that aggregate large amounts of data. Reduces disk I/O by only retrieving necessary columns. 4.\u200b Query Optimization -\u200b RDBMSs analyze SQL queries to find the most efficient execution plan. -\u200b Techniques include: Using cost-based optimization to determine the lowest-cost query plan; Reordering joins to minimize processing time; Pushdown filtering to reduce the amount of processed data. 5.\u200b Caching/Prefetching -\u200b Caching: Frequently accessed query results are stored in memory to reduce redundant computations. -\u200b Prefetching: Anticipates future queries and loads relevant data into memory ahead of time. -\u200b Reduces disk I/O bottlenecks and speeds up query execution. 6.\u200b Materialized Views -\u200b A materialized view is a precomputed result of a query stored as a table. -\u200b Unlike regular views, which recompute results each time they are queried, materialized views store results persistently. -\u200b Commonly used in reporting and analytics to speed up expensive aggregations and joins. 7.\u200b Precompiled Stored Procedures -\u200b Stored procedures are precompiled SQL queries that execute faster because they don\u2019t need to be re-parsed every time. -\u200b Benefits: Reduces query execution time, Improves security by limiting direct access to raw tables, Allows complex logic to be executed at the database level. 8.\u200b Data Replication and Partitioning -\u200b Replication: Copies data across multiple database instances to improve availability and fault tolerance -\u200b Common types: Master-slave replication (one primary, multiple read replicas); Multi-master replication (multiple writable nodes). -\u200b Partitioning: Splits large tables into smaller, more manageable chunks to improve query performance. -\u200b Horizontal partitioning: Divides rows across different tables (e.g., by region, time range). -\u200b Vertical partitioning: Stores specific"
    },
    "467": {
        "file": "Extended Notes - Moving Beyond the Relational Model Slide Deck.pdf",
        "page": 2,
        "chunk": "and fault tolerance -\u200b Common types: Master-slave replication (one primary, multiple read replicas); Multi-master replication (multiple writable nodes). -\u200b Partitioning: Splits large tables into smaller, more manageable chunks to improve query performance. -\u200b Horizontal partitioning: Divides rows across different tables (e.g., by region, time range). -\u200b Vertical partitioning: Stores specific columns in separate tables to optimize read performance. Transaction Processing -\u200b Transaction: a sequence of one or more of the CRUD operations performed as a single, logical unit of work -\u200b Either the entire sequence succeeds (COMMIT)"
    },
    "468": {
        "file": "Extended Notes - Moving Beyond the Relational Model Slide Deck.pdf",
        "page": 3,
        "chunk": "-\u200b OR the entire sequence fails (ROLLBACK or ABORT) -\u200b Help ensure: Data Integrity, Error Recovery, Concurrency Control, Reliable Data Storage, Simplified Error Handling Transaction Processing What is a Transaction? -\u200b A transaction is a sequence of one or more CRUD operations (Create, Read, Update, Delete) performed as a single, logical unit of work. Transactions ensure data consistency and reliability in database systems. -\u200b All operations must either complete successfully together (COMMIT) or fail together (ROLLBACK or ABORT). -\u200b If any part of the transaction encounters an error, the entire sequence is reversed to maintain data integrity. How Transactions Work 1.\u200b Begin Transaction \u2013 The system starts a new transaction. 2.\u200b Perform CRUD Operations \u2013 Multiple database operations occur (e.g., inserting a record, updating a balance). 3.\u200b Validation & Checks \u2013 The system ensures constraints and consistency rules are met. 4.\u200b Commit or Rollback -\u200b COMMIT: If all operations succeed, changes are saved permanently. -\u200b ROLLBACK (ABORT): If any operation fails, all changes are undone, ensuring the database remains in a consistent state. Why Are Transactions Important? Transactions play a crucial role in maintaining a reliable and robust database system by ensuring: 1.\u200b Data Integrity: Transactions maintain database correctness by ensuring that only valid and complete changes are saved. a.\u200b Prevents issues such as half-completed updates or lost data during failures. 2.\u200b Error Recovery: If a system crashes or a query fails, the database can roll back to a stable state. a.\u200b Ensures that no partial or inconsistent data is left behind. 3.\u200b Concurrency Control: Multiple users can safely perform operations on the database at the same time. a.\u200b Prevents issues like lost updates, dirty reads, and inconsistent data retrieval. 4.\u200b Reliable Data Storage: Transactions ensure that once data is committed, it remains stored even in the event of a"
    },
    "469": {
        "file": "Extended Notes - Moving Beyond the Relational Model Slide Deck.pdf",
        "page": 3,
        "chunk": "left behind. 3.\u200b Concurrency Control: Multiple users can safely perform operations on the database at the same time. a.\u200b Prevents issues like lost updates, dirty reads, and inconsistent data retrieval. 4.\u200b Reliable Data Storage: Transactions ensure that once data is committed, it remains stored even in the event of a power failure or system crash. a.\u200b Guarantees Durability, one of the key ACID properties. 5.\u200b Simplified Error Handling: Developers can focus on application logic without worrying about data corruption. a.\u200b If an error occurs, rolling back ensures a clean database state."
    },
    "470": {
        "file": "Extended Notes - Moving Beyond the Relational Model Slide Deck.pdf",
        "page": 4,
        "chunk": "Example Transaction - Transfer $$ DELIMITER // CREATE PROCEDURE transfer( IN sender_id INT, IN receiver_id INT, IN amount DECIMAL(10,2) ) BEGIN DECLARE rollback_message VARCHAR(255) \u200b DEFAULT 'Transaction rolled back: Insufficient funds'; DECLARE commit_message VARCHAR(255) DEFAULT 'Transaction committed successfully'; -- Start the transaction START TRANSACTION; -- Attempt to debit money from account 1 UPDATE accounts SET balance = balance - amount WHERE account_id = sender_id; -- Attempt to credit money to account 2 UPDATE accounts SET balance = balance + amount WHERE account_id = receiver_id; -- Check if there are sufficient funds in account 1 -- Simulate a condition where there are insufficient funds IF (SELECT balance FROM accounts WHERE account_id = sender_id) < 0 THEN -- Roll back the transaction if there are insufficient funds ROLLBACK; SIGNAL SQLSTATE '45000' \u200b -- 45000 is unhandled, user-defined error SET MESSAGE_TEXT = rollback_message; ELSE -- Log the transactions if there are sufficient funds INSERT INTO transactions (account_id, amount, transaction_type) \u200b \u200b VALUES (sender_id, -amount, 'WITHDRAWAL'); INSERT INTO transactions (account_id, amount, transaction_type) VALUES (receiver_id, amount, 'DEPOSIT'); -- Commit the transaction COMMIT; SELECT commit_message AS 'Result'; END IF; END // DELIMITER ;"
    },
    "471": {
        "file": "Extended Notes - Moving Beyond the Relational Model Slide Deck.pdf",
        "page": 5,
        "chunk": "Relational Database Problems While relational databases are powerful and widely used, they may not be the best choice for every use case. Some challenges and limitations include: 1.\u200b Schemas Evolve Over Time -\u200b Rigid Schema Structure: -\u200b Relational databases require predefined schemas, meaning tables and relationships must be designed before data is stored. -\u200b As applications grow and change, modifying the schema (e.g., adding new columns, changing relationships) can be cumbersome and may require schema migrations, which can be slow and risky in large databases. -\u200b Alternative Solutions: -\u200b NoSQL databases (e.g., MongoDB, Cassandra) provide schema flexibility, allowing fields to be added dynamically without disrupting existing data. 2.\u200b Not All Applications Need Full ACID Compliance -\u200b ACID compliance ensures strong consistency, but at a cost: -\u200b Transactions in relational databases follow Atomicity, Consistency, Isolation, and Durability, which can introduce performance overhead, especially in distributed systems. -\u200b Some applications prioritize availability and speed over strong consistency. -\u200b Alternative Solutions: -\u200b Eventual consistency models in NoSQL databases (e.g., Amazon DynamoDB, Apache Cassandra) are often preferred for applications where absolute consistency is not critical (e.g., social media feeds, recommendation engines). 3.\u200b Joins Can Be Expensive -\u200b Performance Bottlenecks: -\u200b SQL queries involving multiple joins can become computationally expensive, especially on large datasets. -\u200b Each join operation requires scanning and matching rows from multiple tables, which can lead to high CPU and memory usage. -\u200b Alternative Solutions: -\u200b Denormalization (storing related data together) is sometimes preferred in NoSQL databases to avoid expensive join operations. -\u200b Columnar databases (e.g., Apache Parquet, ClickHouse) optimize analytical workloads by reducing the need for joins. 4.\u200b A Lot of Data Is Semi-Structured or Unstructured (JSON, XML, etc.) -\u200b Relational databases are designed for structured data: -\u200b They work best when data fits neatly into rows and columns. -\u200b However,"
    },
    "472": {
        "file": "Extended Notes - Moving Beyond the Relational Model Slide Deck.pdf",
        "page": 5,
        "chunk": "-\u200b Columnar databases (e.g., Apache Parquet, ClickHouse) optimize analytical workloads by reducing the need for joins. 4.\u200b A Lot of Data Is Semi-Structured or Unstructured (JSON, XML, etc.) -\u200b Relational databases are designed for structured data: -\u200b They work best when data fits neatly into rows and columns. -\u200b However, modern applications generate semi-structured (JSON, XML) or unstructured (text, images, videos) data that doesn\u2019t always fit a tabular format. -\u200b Alternative Solutions: -\u200b NoSQL document stores (e.g., MongoDB, Firebase) allow for flexible JSON-like storage. -\u200b Search engines like Elasticsearch and OpenSearch are optimized for handling unstructured text and logs. 5.\u200b Horizontal Scaling Presents Challenges -\u200b Scaling relational databases across multiple servers is complex: -\u200b Relational databases traditionally scale vertically (by adding more CPU, RAM, or storage to a single server)."
    },
    "473": {
        "file": "Extended Notes - Moving Beyond the Relational Model Slide Deck.pdf",
        "page": 6,
        "chunk": "-\u200b Scaling horizontally (distributing data across multiple servers) requires techniques like sharding, replication, and distributed transactions, which add complexity. -\u200b Alternative Solutions: -\u200b NoSQL databases like Cassandra and DynamoDB are natively designed for horizontal scaling, handling massive amounts of data across distributed nodes with ease. 6.\u200b Some Applications Require More Performance (Real-Time, Low Latency Systems) -\u200b Relational databases can be too slow for real-time applications: -\u200b High-throughput, low-latency applications (e.g., stock trading platforms, real-time analytics, gaming leaderboards) need sub-millisecond response times. -\u200b RDBMS architectures, with their strict consistency and transaction management, can introduce delays. -\u200b Alternative Solutions: -\u200b In-memory databases (e.g., Redis, Memcached) provide ultra-fast key-value lookups. -\u200b Time-series databases (e.g., InfluxDB, TimescaleDB) are optimized for time-based data analysis. Scalability \u2013 Up or Out? Conventional Wisdom: Scale Vertically (Up) Until High Availability Demands Horizontal Scaling (Out) -\u200b Vertical Scaling (Scaling Up): -\u200b Definition: Scaling up refers to adding more resources (CPU, RAM, storage) to a single server to handle increased workload. -\u200b Why it\u2019s common: -\u200b It\u2019s simple and relatively cost-effective in the short term. -\u200b No architecture changes are needed; you just upgrade the existing server hardware. -\u200b Often sufficient for many small to medium-sized applications. -\u200b Limitations: -\u200b Physical limits: At some point, hardware upgrades become prohibitively expensive or ineffective. You can\u2019t keep adding more CPUs, RAM, or storage indefinitely. -\u200b Single point of failure: With a vertically scaled system, if the server goes down, the entire application can become unavailable. This poses availability and redundancy challenges, especially in mission-critical systems. -\u200b Cost efficiency: The larger the system, the more expensive it is to upgrade. There's a diminishing return on investment with each incremental upgrade. -\u200b Horizontal Scaling (Scaling Out): -\u200b Definition: Scaling out means adding more servers or nodes to distribute the load, effectively splitting the workload"
    },
    "474": {
        "file": "Extended Notes - Moving Beyond the Relational Model Slide Deck.pdf",
        "page": 6,
        "chunk": "in mission-critical systems. -\u200b Cost efficiency: The larger the system, the more expensive it is to upgrade. There's a diminishing return on investment with each incremental upgrade. -\u200b Horizontal Scaling (Scaling Out): -\u200b Definition: Scaling out means adding more servers or nodes to distribute the load, effectively splitting the workload across multiple machines. -\u200b Why it\u2019s challenging: -\u200b Requires distributed computing models (e.g., sharding, replication), which can introduce complexity in managing consistency, coordination, and fault tolerance. -\u200b Managing a large number of machines introduces complexity in system design (network communication, load balancing, etc.), and data consistency must be carefully handled."
    },
    "475": {
        "file": "Extended Notes - Moving Beyond the Relational Model Slide Deck.pdf",
        "page": 7,
        "chunk": "-\u200b Why it\u2019s necessary: -\u200b For high-traffic applications or applications requiring high availability and fault tolerance, scaling vertically alone eventually becomes insufficient. -\u200b High-demand apps (e.g., social media platforms, e-commerce, cloud services) often need to serve millions or billions of users, requiring distributed systems that can horizontally scale. -\u200b Why Scaling Up (Vertical Scaling) is Often Easier -\u200b Simple Upgrades: -\u200b It\u2019s often easier to just increase the resources of an existing machine than to architect a distributed system. -\u200b No need to redesign the application: Vertical scaling doesn\u2019t require significant changes in the system architecture or application code. -\u200b Less Complex Infrastructure: -\u200b No need to worry about network communication, data sharding, or distributed transactions. -\u200b Fewer Management Overheads: -\u200b Fewer machines mean fewer maintenance tasks, backups, and monitoring systems. -\u200b Practical and Financial Limits to Scaling Up -\u200b Physical Hardware Constraints: -\u200b Eventually, you\u2019ll hit hardware limits. For example, there\u2019s a limit to the number of CPUs, RAM, and storage you can add to a single server. -\u200b As machines grow more powerful, the cost of upgrading them grows exponentially. High-end servers can be very expensive compared to commodity servers. -\u200b Single Point of Failure: -\u200b If your vertically scaled system crashes, everything crashes. There's no inherent redundancy unless you add complex failover mechanisms. -\u200b High availability and fault tolerance are limited unless you implement replication or clustering, which pushes the system toward horizontal scaling. Modern Systems That Make Horizontal Scaling Less Problematic -\u200b Distributed Databases: -\u200b Modern distributed databases (e.g., Cassandra, Google Spanner) are designed to scale horizontally with built-in mechanisms for data partitioning (sharding), replication, and eventual consistency. -\u200b They handle much of the complexity of scaling, making horizontal scaling much more accessible for modern systems. -\u200b Cloud Computing and Serverless Architectures: -\u200b Cloud platforms (e.g.,"
    },
    "476": {
        "file": "Extended Notes - Moving Beyond the Relational Model Slide Deck.pdf",
        "page": 7,
        "chunk": "distributed databases (e.g., Cassandra, Google Spanner) are designed to scale horizontally with built-in mechanisms for data partitioning (sharding), replication, and eventual consistency. -\u200b They handle much of the complexity of scaling, making horizontal scaling much more accessible for modern systems. -\u200b Cloud Computing and Serverless Architectures: -\u200b Cloud platforms (e.g., AWS, Google Cloud, Azure) allow dynamic horizontal scaling, where resources (such as virtual machines or containers) can be added automatically based on demand. -\u200b Serverless frameworks (e.g., AWS Lambda, Azure Functions) enable scaling on-demand without worrying about managing infrastructure. -\u200b These platforms provide auto-scaling capabilities that automatically spin up new resources as traffic increases and scale them down when demand drops."
    },
    "477": {
        "file": "Extended Notes - Moving Beyond the Relational Model Slide Deck.pdf",
        "page": 8,
        "chunk": "-\u200b Microservices: -\u200b Microservices architectures decompose applications into smaller, more manageable components, which can be scaled independently. -\u200b This allows individual services to be scaled out independently, making it easier to handle varying levels of load across different parts of an application. -\u200b Containerization (Docker, Kubernetes): -\u200b Containers allow for easier deployment and scaling of applications across distributed environments. -\u200b Kubernetes, for example, simplifies the process of managing containers and automatically scaling services as needed. So What? Distributed Data when Scaling Out What is a Distributed System? -\u200b A distributed system is a network of independent computers that work together to provide a unified service, appearing to users as a single system. As Andrew Tannenbaum puts it: -\u200b \u201cA distributed system is a collection of independent computers that appear to its users as one computer.\u201d -\u200b When scaling out in a distributed system, multiple machines (or nodes) handle different pieces of the workload, allowing the system to scale horizontally. This enables systems to handle larger amounts of data and more requests concurrently, but it also introduces a unique set of challenges. Characteristics of Distributed Systems Distributed systems have several key characteristics that differentiate them from traditional, monolithic systems: 1.\u200b Computers Operate Concurrently: -\u200b Concurrency means that multiple processes or threads can be executed in parallel across different machines. -\u200b Each machine in a distributed system may be working on different tasks at the same time, contributing to the overall processing power of the system. -\u200b This concurrent processing allows distributed systems to handle a higher volume of requests and tasks efficiently. For example, one node may be processing user requests, another may be updating a database, while a third node handles background jobs. -\u200b Challenges: -\u200b Synchronization between machines can be tricky, especially when they must share or access common"
    },
    "478": {
        "file": "Extended Notes - Moving Beyond the Relational Model Slide Deck.pdf",
        "page": 8,
        "chunk": "systems to handle a higher volume of requests and tasks efficiently. For example, one node may be processing user requests, another may be updating a database, while a third node handles background jobs. -\u200b Challenges: -\u200b Synchronization between machines can be tricky, especially when they must share or access common data. -\u200b This requires effective communication mechanisms and often a distributed consensus to ensure that all parts of the system are working in harmony. 2.\u200b Computers Fail Independently -\u200b Independent Failures: In a distributed system, each machine can fail independently of the others. This means that even if one node goes down, the others can continue to function. -\u200b This fault tolerance is one of the primary benefits of distributed systems, as failures do not bring down the entire system. -\u200b Challenges: -\u200b Fault detection and recovery become complex when systems are distributed. How do we know a machine has failed, and how do we ensure it recovers or that its tasks are redistributed to healthy machines?"
    },
    "479": {
        "file": "Extended Notes - Moving Beyond the Relational Model Slide Deck.pdf",
        "page": 9,
        "chunk": "-\u200b Data consistency and availability can be impacted by node failures, which must be handled carefully through techniques like replication and partitioning. -\u200b Example: If a server in a database cluster fails, replicas of the data stored on other machines can still serve requests, but the system must know which copies are most up-to-date and reliable. 3.\u200b No Shared Global Clock -\u200b No Global Clock means there is no single time source or synchronized clock across all machines in a distributed system. Each machine operates based on its local time, leading to discrepancies across the system. -\u200b Without a shared clock, it\u2019s impossible to guarantee that all events in the system happen in a perfectly synchronized manner. -\u200b This has important implications for tasks like ordering operations (e.g., in transaction processing or event logs) and event coordination across nodes. -\u200b Challenges: -\u200b Clock skew: Differences in time across nodes can cause issues with event ordering. For example, determining the exact sequence of operations in a distributed database might be difficult. -\u200b Distributed systems often use techniques like logical clocks (e.g., Lamport timestamps) to maintain an order of events across machines without relying on synchronized physical clocks. Challenges in Scaling Out with Distributed Data While distributed systems offer scalability and fault tolerance, they come with inherent challenges: 1.\u200b Data Partitioning (Sharding) -\u200b Sharding is the process of dividing a large dataset into smaller, more manageable parts (shards), which can be distributed across multiple machines. -\u200b Challenges: -\u200b Deciding on the best strategy for partitioning data. For example, which data should go on which machine, and how do we ensure balanced distribution of the load? -\u200b Handling cross-shard queries can be complex and inefficient. If a query needs data from multiple shards, it must be coordinated and consolidated efficiently. 2.\u200b Data Replication"
    },
    "480": {
        "file": "Extended Notes - Moving Beyond the Relational Model Slide Deck.pdf",
        "page": 9,
        "chunk": "strategy for partitioning data. For example, which data should go on which machine, and how do we ensure balanced distribution of the load? -\u200b Handling cross-shard queries can be complex and inefficient. If a query needs data from multiple shards, it must be coordinated and consolidated efficiently. 2.\u200b Data Replication -\u200b Replication involves creating multiple copies of data on different machines to ensure high availability and fault tolerance. -\u200b Challenges: -\u200b Managing consistency across replicas: How do we make sure all replicas are up-to-date after a change is made? -\u200b Handling replication lag: When updates are made to one replica, it might take time for the changes to propagate to others, potentially leading to inconsistent reads. -\u200b Distributed databases often employ eventual consistency or use techniques like Quorum-based replication to strike a balance between consistency and availability. 3.\u200b Communication and Latency -\u200b In a distributed system, nodes must communicate over a network, which can introduce latency and potential bottlenecks. -\u200b Challenges:"
    },
    "481": {
        "file": "Extended Notes - Moving Beyond the Relational Model Slide Deck.pdf",
        "page": 10,
        "chunk": "-\u200b Ensuring low-latency communication between nodes while scaling out to a large number of machines. -\u200b Handling network partitions, where communication between certain nodes is temporarily broken. This requires sophisticated strategies for maintaining availability and consistency during these partitions (e.g., CAP theorem considerations). 4.\u200b Consistency and Consensus -\u200b Achieving consistency in a distributed system can be difficult, especially when multiple nodes are involved. -\u200b Challenges: -\u200b Distributed consensus protocols (e.g., Paxos, Raft) are often needed to agree on the state of the system across nodes. These protocols can be complex and costly in terms of performance but are crucial for maintaining consistency. -\u200b CAP theorem: It\u2019s impossible to guarantee consistency, availability, and partition tolerance all at once in a distributed system, leading to trade-offs. Distributed Data Stores What Are Distributed Data Stores? -\u200b A distributed data store is a system where data is stored across multiple machines or nodes rather than on a single server. The goal is to achieve high availability, scalability, and fault tolerance by spreading data across different locations. -\u200b Replication: A key feature of distributed data stores is data replication, where each block of data is typically replicated across multiple nodes to ensure redundancy and fault tolerance. For example, a database might store the same data on N nodes, ensuring that even if one or more nodes fail, the data can still be accessed from the other nodes. -\u200b Sharding: Data is partitioned into smaller, more manageable pieces (called shards) that are distributed across nodes. Each shard contains a subset of the overall data, helping to improve performance and scalability. Types of Distributed Databases Distributed databases can either be relational or non-relational (NoSQL), with both types supporting distribution, replication, and sharding in different ways. 1.\u200b Relational Distributed Databases: Some traditional relational databases have evolved to support"
    },
    "482": {
        "file": "Extended Notes - Moving Beyond the Relational Model Slide Deck.pdf",
        "page": 10,
        "chunk": "shard contains a subset of the overall data, helping to improve performance and scalability. Types of Distributed Databases Distributed databases can either be relational or non-relational (NoSQL), with both types supporting distribution, replication, and sharding in different ways. 1.\u200b Relational Distributed Databases: Some traditional relational databases have evolved to support distributed configurations: -\u200b MySQL and PostgreSQL are examples of relational databases that offer replication (the process of maintaining copies of the same data on different nodes) and sharding (splitting data into subsets that reside on different machines). -\u200b These systems typically require additional tools or setups (like MySQL Cluster, PgSQL's logical replication, or PostgreSQL partitioning) to scale horizontally and achieve distribution across multiple nodes. -\u200b Replication ensures data availability and durability in case of failures. -\u200b Sharding helps distribute the load across multiple nodes to improve scalability, though managing distributed transactions and maintaining consistency becomes more complex in these cases. 2.\u200b NoSQL Distributed Databases: Many NoSQL databases were designed with distributed systems in mind, providing more inherent support for sharding and replication: -\u200b Cassandra, MongoDB, and Couchbase are some examples of distributed NoSQL systems that automatically handle sharding and replication."
    },
    "483": {
        "file": "Extended Notes - Moving Beyond the Relational Model Slide Deck.pdf",
        "page": 11,
        "chunk": "-\u200b NoSQL databases often favor eventual consistency over strict ACID compliance, meaning that while data is eventually consistent across all nodes, it may not be immediately consistent after updates. This allows for more flexibility in distributed configurations but requires careful attention to potential inconsistencies. -\u200b Cassandra uses a peer-to-peer model and supports replication and partitioning, with the ability to scale horizontally across many nodes. -\u200b MongoDB offers replication and sharding out of the box, making it suitable for horizontally scalable applications. 3.\u200b Newer Players -\u200b CockroachDB: -\u200b A newer distributed relational database, CockroachDB was designed to provide horizontal scalability and high availability with strong consistency (supporting the ACID properties of transactions). It automatically handles sharding and replication, and its architecture is similar to that of Google Spanner, focusing on global distribution. -\u200b It\u2019s a distributed SQL database that offers strong consistency, allowing users to scale applications globally while maintaining the guarantees typically associated with relational databases. Key Characteristics of Distributed Data Stores Replication: Replication ensures that data is available even when some nodes or machines fail. Typically, each data block (or row) is replicated across multiple nodes (often N nodes, with N being a configurable number). -\u200b Synchronous replication ensures that data is written to all nodes before acknowledging the write. -\u200b Asynchronous replication writes data to one node first and later synchronizes it to other replicas. This is often faster but can lead to eventual consistency. -\u200b Challenges: Managing consistency between replicas can be complex. There is always a trade-off between performance and consistency, particularly when nodes are located in different regions (latency issues can arise). Sharding: Sharding involves splitting data into smaller chunks called shards, each of which resides on a separate node. This partitioning is usually done based on a shard key (e.g., user ID or region)"
    },
    "484": {
        "file": "Extended Notes - Moving Beyond the Relational Model Slide Deck.pdf",
        "page": 11,
        "chunk": "trade-off between performance and consistency, particularly when nodes are located in different regions (latency issues can arise). Sharding: Sharding involves splitting data into smaller chunks called shards, each of which resides on a separate node. This partitioning is usually done based on a shard key (e.g., user ID or region) to ensure that data is distributed evenly across nodes. -\u200b Advantages: Sharding allows for horizontal scalability, meaning you can add more nodes to the system to distribute the load as demand increases. -\u200b Challenges: -\u200b Managing cross-shard queries can be complex and potentially slower. If data is spread across different shards, queries that need data from multiple shards may incur additional overhead. -\u200b Rebalancing shards across nodes can be difficult, especially when data grows unevenly, requiring manual intervention or complex algorithms to ensure even distribution. Network Partitioning is Inevitable One of the most important principles in distributed systems is that network partitioning is inevitable, meaning that at some point, the network may experience temporary failures, leading to split-brain situations where some nodes can no longer communicate with others."
    },
    "485": {
        "file": "Extended Notes - Moving Beyond the Relational Model Slide Deck.pdf",
        "page": 12,
        "chunk": "-\u200b What is Network Partitioning? -\u200b Network partitioning occurs when there\u2019s a disruption in the communication between parts of the system, resulting in some nodes being cut off from others. -\u200b For example, if a failure occurs in the network between two data centers, the systems on each side of the partition may be isolated and unable to communicate with each other. -\u200b System Needs to Be Partition Tolerant -\u200b Partition Tolerance refers to the ability of the system to continue functioning even when parts of the network are unavailable. This is crucial in distributed systems because network failures are inevitable and cannot always be prevented. -\u200b According to the CAP Theorem (states that it is impossible for a distributed data store to simultaneously provide more than two out of the following three guarantees), a distributed system can only guarantee two of the following three properties: -\u200b Consistency: Every read operation sees the most recent write. -\u200b Every user of the DB has an identical view of the data at any given instant -\u200b Consistency + Availability: System always responds with the latest data and every request gets a response, but may not be able to deal with network issues -\u200b Availability: Every request to the system gets a response (either success or failure). -\u200b In the event of a failure, the database remains operational -\u200b Availability + Partition Tolerance: System always sends are responds based on distributed store, but may not be the absolute latest data. -\u200b Partition Tolerance: The system continues to operate even if there\u2019s a network partition. -\u200b The database can maintain operations in the event of the network\u2019s failing between two segments of the distributed system -\u200b Consistency + Partition Tolerance: If system responds with data from a distributed store, it is always the latest,"
    },
    "486": {
        "file": "Extended Notes - Moving Beyond the Relational Model Slide Deck.pdf",
        "page": 12,
        "chunk": "The system continues to operate even if there\u2019s a network partition. -\u200b The database can maintain operations in the event of the network\u2019s failing between two segments of the distributed system -\u200b Consistency + Partition Tolerance: If system responds with data from a distributed store, it is always the latest, else data request is dropped. -\u200b What CAP is really saying: If you cannot limit the number of faults, requests can be directed to any server, and you insist on serving every request, then you cannot possibly be consistent (But it is interpreted as: You must always give up something: consistency, availability, or tolerance to failure) -\u200b In practice, most distributed systems opt for Partition Tolerance and Availability, as network failures are common. This often leads to eventual consistency (e.g., Cassandra, MongoDB)."
    },
    "487": {
        "file": "AVL Trees.pdf",
        "page": 0,
        "chunk": "AVL Tree \u25cf\u200b Problems with BST: The order in which elements are inserted affects the height of the tree. If elements are inserted in either ascending or descending order, then the tree is completely imbalanced. The more imbalanced a tree is, the more comparisons we have to make. Therefore, our goal is to create a BST with minimum height. \u25cb\u200b This can be achieved by a complete BST \u25a0\u200b A complete BST is a BST where every level is full except possibly the last one. \u25cb\u200b This ensures at most log(n) comparisons. \u25cb\u200b However, it is cumbersome to build such a tree from a list of numbers \u25cb\u200b Here come AVL Trees! \u25cf\u200b An AVL tree is an approximately balanced BST tree \u25cb\u200b Maintains a balance factor in each node \u25cb\u200b The balance property of a given node states that the maximum difference between the height of the left subtree and the height of the right subtree is 1. \u25cb\u200b Each time a node is inserted, the balance property is checked. If it is not satisfied, the tree must be rotated. \u25cb\u200b There are four cases that warrant rotation \u25a0\u200b Case 1: Left-Left Insertion \u25cf\u200b If Z is the node of imbalance, left-left rotation is caused when the inserted node falls in the left subtree of the left child of Z \u25cf\u200b Identify the unbalanced node (let\u2019s call it Z). \u25cf\u200b Let Y be the left child of Z, and X be the left child of Y. \u25cf\u200b Perform a right rotation: \u25cb\u200b Make Y the new root. \u25cb\u200b Move Z to be the right child of Y. \u25cb\u200b Assign Y's right child as Z's left child. \u25a0\u200b Case 2: Left-Right Insertion \u25cf\u200b Occurs when you insert the new node into the right subtree of the left child of Z"
    },
    "488": {
        "file": "AVL Trees.pdf",
        "page": 0,
        "chunk": "right rotation: \u25cb\u200b Make Y the new root. \u25cb\u200b Move Z to be the right child of Y. \u25cb\u200b Assign Y's right child as Z's left child. \u25a0\u200b Case 2: Left-Right Insertion \u25cf\u200b Occurs when you insert the new node into the right subtree of the left child of Z \u25cf\u200b Identify the unbalanced node (Z). \u25cf\u200b Look at Z's left child (Y), which has a right child (X). \u25cf\u200b Step 1: Move X up and Y down \u25cb\u200b X takes Y's position. \u25cb\u200b Y becomes X's left child. \u25cb\u200b If X had a left subtree, it becomes Y's right subtree. \u25cf\u200b Step 2: Move X up and Z down"
    },
    "489": {
        "file": "AVL Trees.pdf",
        "page": 1,
        "chunk": "\u25cb\u200b X takes Z's position. \u25cb\u200b Z becomes X's right child. \u25cb\u200b If X had a right subtree, it becomes Z's left subtree. \u25a0\u200b Case 3: Right-Left Insertion \u25cf\u200b Occurs when you insert the new node into the left subtree of the right child of Z \u25cf\u200b Identify the unbalanced node (Z). \u25cf\u200b Look at Z's right child (Y), which has a left child (X). \u25cf\u200b Step 1: Move X up and Y down \u25cb\u200b X takes Y's position. \u25cb\u200b Y becomes X's right child. \u25cb\u200b If X had a right subtree, it becomes Y's left subtree. \u25cf\u200b Step 2: Move X up and Z down \u25cb\u200b X takes Z's position. \u25cb\u200b Z becomes X's left child. \u25cb\u200b If X had a left subtree, it becomes Z's right subtree. \u25a0\u200b Case 4: Right-Right Insertion \u25cf\u200b Occurs when the new node is inserted into the right subtree of the right child of Z \u25cf\u200b Identify the unbalanced node (Z). \u25cf\u200b Let Y be the right child of Z, and X be the right child of Y. \u25cf\u200b Perform a left rotation: \u25cb\u200b Make Y the new root. \u25cb\u200b Move Z to be the left child of Y. \u25cb\u200b Assign Y's left child as Z's right child."
    },
    "490": {
        "file": "DS4300 Notes.pdf",
        "page": 0,
        "chunk": "Binary Search Tree (BST) \u2022 A BST is a binary tree where: o The left subtree contains nodes with values less than the parent. o The right subtree contains nodes with values greater than the parent. \u2022 The order in which values are inserted affects the shape of the tree. AVL Tree (Adelson-Velsky and Landis Tree) \u2022 A self-balancing BST that maintains the AVL balance property: o The balance factor of a node is calculated as: \u2022 Balance Factor = height(left subtree) - height(right subtree) o A node is balanced if the absolute value of its balance factor is \u2264 1. \u2022 AVL trees ensure a balanced height, keeping operations efficient. AVL Tree Rotations (Rebalancing) An insertion can cause an imbalance in one of the following four ways. In each case, alpha (\u03b1) is the first unbalanced node. Left-Left (LL) Case \u2022 Occurs when a node is inserted into the left subtree of the left child of \u03b1. \u2022 Structure before imbalance: \u03b1 / y / z \u2022 Fix: Single Right Rotation around \u03b1. o Make y the new root. o Move \u03b1 to the right of y. Left-Right (LR) Case \u2022 Occurs when a node is inserted into the right subtree of the left child of \u03b1. \u2022 Structure before imbalance: \u03b1 / y \\ z \u2022 Fix: Double Rotation (Left Rotation + Right Rotation) o First, Left Rotate around y, converting it into an LL case. o Then, Right Rotate around \u03b1, making z the new root of the affected subtree. Right-Left (RL) Case \u2022 Occurs when a node is inserted into the left subtree of the right child of \u03b1. \u2022 Structure before imbalance: \u03b1 \\ y / z"
    },
    "491": {
        "file": "DS4300 Notes.pdf",
        "page": 0,
        "chunk": "the affected subtree. Right-Left (RL) Case \u2022 Occurs when a node is inserted into the left subtree of the right child of \u03b1. \u2022 Structure before imbalance: \u03b1 \\ y / z"
    },
    "492": {
        "file": "DS4300 Notes.pdf",
        "page": 1,
        "chunk": "\u2022 Fix: Double Rotation (Right Rotation + Left Rotation) o First, Right Rotate around y, converting it into an RR case. o Then, Left Rotate around \u03b1, making z the new root of the affected subtree. Right-Right (RR) Case \u2022 Occurs when a node is inserted into the right subtree of the right child of \u03b1. \u2022 Structure before imbalance: \u03b1 \\ y \\ z \u2022 Fix: Single Left Rotation around \u03b1. o Make y the new root. o Move \u03b1 to the left of y. Key Notes on AVL Rotations \u2022 LL and RR cases require a single rotation. \u2022 LR and RL cases require a double rotation. \u2022 The case type refers to the position of the inserted node relative to the unbalanced node (\u03b1). \u2022 AVL trees do not stand for anything specific \u2013 they are named after the inventors Adelson-Velsky and Landis. Time Complexity of AVL Tree Operations \u2022 Search: O(log n) \u2022 Insertion: O(log n) (due to rebalancing) \u2022 Deletion: O(log n) (may require rotations) Hash Tables Definition \u2022 A hash table (also called a hash map) is a data structure that stores key-value pairs. \u2022 It uses a hash function to map keys to indices in an array. \u2022 Provides fast insert, delete, and lookup operations. \u2022 Dispersion: spread of hash values (we want good dispersion) Key Operations & Time Complexity \u2022 Insertion: O(1) on average, O(n) in the worst case (due to collisions). \u2022 Search: O(1) on average, O(n) in the worst case. \u2022 Deletion: O(1) on average, O(n) in the worst case. Hash Function \u2022 A hash function converts a key into an index. \u2022 It should: o Be fast to compute. o Distribute keys uniformly across the table (create dispersion). o Minimize collisions (different keys mapping to the same index). o"
    },
    "493": {
        "file": "DS4300 Notes.pdf",
        "page": 1,
        "chunk": "\u2022 Deletion: O(1) on average, O(n) in the worst case. Hash Function \u2022 A hash function converts a key into an index. \u2022 It should: o Be fast to compute. o Distribute keys uniformly across the table (create dispersion). o Minimize collisions (different keys mapping to the same index). o Should include modulo division with the table size to ensure it fits within the bounds Collisions"
    },
    "494": {
        "file": "DS4300 Notes.pdf",
        "page": 2,
        "chunk": "\u2022 A collision occurs when two keys produce the same index. \u2022 Collision Resolution Strategies: 1. Chaining (Separate Chaining) \u2022 Each index in the array stores a linked list (or another data structure) of key-value pairs. \u2022 When a collision occurs, the new key-value pair is added to the list at that index. \u2022 Pros: Simple, handles many collisions well. \u2022 Cons: Increased memory usage due to linked lists. 2. Open Addressing \u2022 Instead of using a linked list, all elements stay within the array itself. \u2022 If a collision occurs, the algorithm searches for an open slot. \u2022 Types of Open Addressing: o Linear Probing: If a slot is occupied, check the next slot (index + 1, wrap around if necessary). o Quadratic Probing: Check slots in a quadratic sequence (index + 1\u00b2, index + 2\u00b2, etc.). o Double Hashing: Use a second hash function to determine the step size for probing. \u2022 Pros: More cache-efficient, no extra memory for linked lists. \u2022 Cons: Can lead to clustering (many elements in the same region), reducing efficiency. Load Factor (\u03b1) \u2022 Load Factor = (number of elements) / (size of hash table). \u2022 Determines when to resize the table (typically when \u03b1 > 0.7). \u2022 Higher load factor \u2192 More collisions. \u2022 Lower load factor \u2192 More wasted space. Resizing & Rehashing \u2022 When the load factor exceeds a threshold (e.g., 0.7), the table is resized: 1. A new array (typically twice as large) is allocated. 2. Each key is rehashed into the new array. 3. The old array is discarded. \u2022 Rehashing is costly (O(n)), but reduces future collisions. Advantages of Hash Tables \u2022 Fast lookups, insertions, and deletions (O(1) average case). \u2022 Efficient memory usage for large datasets. \u2022 Used in many applications (caching, databases, sets, dictionaries). Disadvantages"
    },
    "495": {
        "file": "DS4300 Notes.pdf",
        "page": 2,
        "chunk": "rehashed into the new array. 3. The old array is discarded. \u2022 Rehashing is costly (O(n)), but reduces future collisions. Advantages of Hash Tables \u2022 Fast lookups, insertions, and deletions (O(1) average case). \u2022 Efficient memory usage for large datasets. \u2022 Used in many applications (caching, databases, sets, dictionaries). Disadvantages of Hash Tables \u2022 Worst-case O(n) time complexity if too many collisions occur. \u2022 Requires a good hash function to avoid inefficiency. \u2022 Rehashing is expensive when resizing. B+ Trees \u2022 Optimized for disk-based indexing, reducing disk access operations. \u2022 A B+ Tree is an m-way tree of order m, meaning: o Each node can have at most m keys. o Each node (except leaves) can have at most m+1 children. \u2022 Internal nodes only store keys and pointers to children (no data). \u2022 Leaf nodes store all data records and are linked as a doubly linked list for fast range queries. B+ Tree Properties \u2022 All nodes (except root) must be at least half full (at least \u2308m/2\u2309 keys)."
    },
    "496": {
        "file": "DS4300 Notes.pdf",
        "page": 3,
        "chunk": "\u2022 Root node does not need to be half full (can have fewer keys initially). \u2022 Insertion always happens at the leaf level. \u2022 Leaf nodes are connected as a doubly linked list, enabling efficient range queries. \u2022 Keys are always kept sorted within nodes. \u2022 Splitting nodes: o If a leaf node splits, the smallest key in the right half is copied up to the parent. o If an internal node splits, the middle key is moved up to the parent. B+ Tree Insertions Case 1: Inserting Without Splitting If the target leaf has space, insert the new key while keeping the order. Before inserting 25: Leaf Level: | 10 20 30 40 | After inserting 25: Leaf Level: | 10 20 25 30 40 | Case 2: Leaf Node Split If a leaf node is full, it must split into two. \u2022 The smallest key in the right half is copied to the parent. \u2022 The parent may also need to split if it becomes full. Before inserting 35 (Leaf Node Full): [ 20 40 ] / | \\ [10] [20 30] [40 50] After inserting 35 (Leaf Split): [ 20 30 40 ] / | \\ [10] [20 25] [30 35] [40 50] Case 3: Internal Node Split If an internal node is full after an insertion, it splits, and the middle key moves up. Before inserting 45 (Internal Node Full): [ 30 ] / \\ [10 20] [30 40 50] After inserting 45 (Internal Split):"
    },
    "497": {
        "file": "DS4300 Notes.pdf",
        "page": 4,
        "chunk": "[ 30 40 ] / | \\ [10 20] [30 35] [40 45 50] B+ Tree Deletions \u2022 Deletion happens at the leaf level. \u2022 If a node has too few keys, it may borrow from a sibling. \u2022 If borrowing is not possible, the node merges with a sibling, and the parent key is deleted. \u2022 The root is the only node allowed to have fewer than \u2308m/2\u2309 keys. Advantages of B+ Trees \u2022 Better disk efficiency: Nodes are designed to fit in memory pages. \u2022 Fast range queries: Leaves are linked, enabling fast sequential access. \u2022 More keys per node: Fewer levels compared to a binary search tree. \u2022 Efficient insertions & deletions: Keeps balance with minimal restructuring. AWS (Amazon Web Services) \u2022 Leading cloud platform with 200+ services for computing, storage, databases, networking, AI, and more. \u2022 Globally available through a network of regions (geographic areas) and availability zones (isolated data centers within regions). \u2022 Uses a pay-as-you-use model, meaning you only pay for the resources you consume. o Cost-effective compared to traditional on-premise data centers, but costs can scale quickly if not managed properly. AWS History & Growth \u2022 Launched in 2006 with only S3 (Simple Storage Service) and EC2 (Elastic Compute Cloud). \u2022 By 2010, AWS expanded with services like SimpleDB, EBS, RDS, DynamoDB, CloudWatch, CloudFront, and more. \u2022 Early adoption incentives: Amazon ran competitions with large prizes to encourage developers to use AWS. \u2022 Continuous innovation has led to 200+ services spanning operations, development, analytics, security, and AI. AWS Global Infrastructure \u2022 Regions: Geographic areas containing multiple data centers. \u2022 Availability Zones (AZs): Isolated data centers within a region, offering redundancy and fault tolerance. \u2022 POP (Points of Presence): Locations for Content Delivery Networks (CDN) to serve users faster (CloudFront uses these). Cloud Computing"
    },
    "498": {
        "file": "DS4300 Notes.pdf",
        "page": 4,
        "chunk": "analytics, security, and AI. AWS Global Infrastructure \u2022 Regions: Geographic areas containing multiple data centers. \u2022 Availability Zones (AZs): Isolated data centers within a region, offering redundancy and fault tolerance. \u2022 POP (Points of Presence): Locations for Content Delivery Networks (CDN) to serve users faster (CloudFront uses these). Cloud Computing Models"
    },
    "499": {
        "file": "DS4300 Notes.pdf",
        "page": 5,
        "chunk": "AWS supports the three main cloud service models: 1. Infrastructure as a Service (IaaS) o Provides virtualized computing resources (e.g., EC2, VPC, EBS). o User has the most control over configurations, networking, and OS. 2. Platform as a Service (PaaS) o Provides a managed platform for deploying applications (e.g., AWS Elastic Beanstalk, RDS). o AWS handles infrastructure, users focus on app development. 3. Software as a Service (SaaS) o Fully managed services where AWS handles everything (e.g., AWS Lambda, Amazon RDS, AI Services). o Users just interact with the software without worrying about infrastructure. AWS Shared Responsibility Model AWS operates under a shared security model, meaning security responsibilities are divided: AWS Responsibilities (Security OF the Cloud) \u2022 Protects physical infrastructure (data centers, power, networking). \u2022 Manages host operating systems & hypervisors. \u2022 Secures AWS-managed services (e.g., S3, RDS). Customer Responsibilities (Security IN the Cloud) \u2022 Controls data, encryption, and access management. \u2022 Manages Identity & Access Management (IAM) roles and policies. \u2022 Configures network security (e.g., security groups, VPN, firewalls). \u2022 Ensures compliance with governance policies. Core AWS Services Compute Services AWS provides multiple compute options: 1. VM-Based (Virtual Machines) o EC2 (Elastic Compute Cloud) \u2013 Virtual machines (VMs) with various instance types for different workloads. 2. Container-Based (Managed Containers) o ECS (Elastic Container Service) \u2013 Managed container orchestration for Docker. o EKS (Elastic Kubernetes Service) \u2013 Managed Kubernetes. o ECR (Elastic Container Registry) \u2013 Container image storage. o Fargate \u2013 Serverless container execution (no VM management). 3. Serverless Compute o AWS Lambda \u2013 Event-driven, serverless functions that scale automatically. Storage Services AWS offers various storage solutions based on needs: 1. Object Storage o Amazon S3 (Simple Storage Service) \u2013 Scalable, high-durability storage. \u2022 Supports time-limited upload links for secure file sharing."
    },
    "500": {
        "file": "DS4300 Notes.pdf",
        "page": 5,
        "chunk": "o AWS Lambda \u2013 Event-driven, serverless functions that scale automatically. Storage Services AWS offers various storage solutions based on needs: 1. Object Storage o Amazon S3 (Simple Storage Service) \u2013 Scalable, high-durability storage. \u2022 Supports time-limited upload links for secure file sharing."
    },
    "501": {
        "file": "DS4300 Notes.pdf",
        "page": 6,
        "chunk": "\u2022 Maximum file size: 5 TB. o Amazon Glacier \u2013 Low-cost archival storage. 2. Block Storage o Amazon EBS (Elastic Block Store) \u2013 Attachable disk storage for EC2 instances. 3. File Storage o Amazon EFS (Elastic File System) \u2013 Scalable, managed file storage for multiple EC2 instances. o Amazon File Cache \u2013 Cache storage for high-speed data access. 4. Backup & Recovery o AWS Backup \u2013 Centralized backup management. Database Services AWS provides both SQL and NoSQL database services: 1. Relational Databases (SQL-based) o Amazon RDS (Relational Database Service) \u2013 Managed SQL databases (MySQL, PostgreSQL, MariaDB, SQL Server, Oracle). o Amazon Aurora \u2013 MySQL/PostgreSQL-compatible, high-performance relational database. 2. NoSQL Databases o Amazon DynamoDB \u2013 Managed key-value database (similar to Redis). o Amazon DocumentDB \u2013 Managed NoSQL document database (MongoDB- compatible). o Amazon Neptune \u2013 Graph database service for complex relationships. 3. In-Memory Databases o Amazon ElastiCache \u2013 Managed Redis/Memcached caching for fast data retrieval. o Amazon MemoryDB \u2013 Fully managed Redis-compatible database. Networking & CDN \u2022 Amazon VPC (Virtual Private Cloud) \u2013 Isolated network for AWS resources. \u2022 AWS CloudFront \u2013 Amazon\u2019s Content Delivery Network (CDN) (competes with Cloudflare). Analytics Services AWS provides data analytics tools for big data processing, real-time streaming, and business intelligence: \u2022 Amazon Athena \u2013 SQL-based queries on S3 data. \u2022 Amazon EMR (Elastic MapReduce) \u2013 Managed Hadoop & Spark for big data. \u2022 AWS Glue \u2013 Serverless ETL (Extract, Transform, Load) service. \u2022 Amazon Redshift \u2013 Cloud-based data warehouse. \u2022 Amazon Kinesis \u2013 Real-time data streaming service. \u2022 Amazon QuickSight \u2013 Business intelligence & visualization tool. Machine Learning (ML) & AI Services"
    },
    "502": {
        "file": "DS4300 Notes.pdf",
        "page": 6,
        "chunk": "streaming service. \u2022 Amazon QuickSight \u2013 Business intelligence & visualization tool. Machine Learning (ML) & AI Services"
    },
    "503": {
        "file": "DS4300 Notes.pdf",
        "page": 7,
        "chunk": "AWS provides managed AI/ML services: 1. Amazon SageMaker \u2013 End-to-end machine learning platform for building, training, and deploying ML models. 2. AWS AI Services \u2013 Pre-built AI models: o Amazon Comprehend \u2013 Natural language processing (NLP). o Amazon Rekognition \u2013 Image and facial recognition. o Amazon Textract \u2013 Extracts text from scanned documents. o Amazon Translate \u2013 Language translation service. Why Use AWS? \u2022 Scalability \u2013 Auto-scaling and load balancing allow seamless growth. \u2022 Cost-Effectiveness \u2013 Pay only for what you use. \u2022 Security \u2013 AWS follows the highest security standards. \u2022 Flexibility \u2013 Supports multiple architectures (VMs, containers, serverless). \u2022 Global Reach \u2013 Available across the world with high availability. Final Thoughts AWS is the most widely adopted cloud platform due to its scalability, flexibility, and rich set of services. Understanding core AWS services, pricing, security, and deployment models is essential for cloud computing professionals."
    },
    "504": {
        "file": "04 - Data Replication.pdf",
        "page": 0,
        "chunk": "DS 4300 Replicating Data Mark Fontenot, PhD Northeastern University Some material used with permission from Dr. Rachlin, with thanks!"
    },
    "505": {
        "file": "04 - Data Replication.pdf",
        "page": 1,
        "chunk": "Distributing Data - Bene\ufb01ts 2 - Scalability / High throughput: Data volume or Read/Write load grows beyond the capacity of a single machine - Fault Tolerance / High Availability: Your application needs to continue working even if one or more machines goes down. - Latency: When you have users in different parts of the world you want to give them fast performance too"
    },
    "506": {
        "file": "04 - Data Replication.pdf",
        "page": 2,
        "chunk": "Distributed Data - Challenges - Consistency: Updates must be propagated across the network. - Application Complexity: Responsibility for reading and writing data in a distributed environment often falls to the application. 3"
    },
    "507": {
        "file": "04 - Data Replication.pdf",
        "page": 3,
        "chunk": "Vertical Scaling - Shared Memory Architectures - Geographically Centralized server - Some fault tolerance (via hot-swappable components) 4"
    },
    "508": {
        "file": "04 - Data Replication.pdf",
        "page": 4,
        "chunk": "Vertical Scaling - Shared Disk Architectures - Machines are connected via a fast network - Contention and the overhead of locking limit scalability (high-write volumes) \u2026 BUT ok for Data Warehouse applications (high read volumes) 5"
    },
    "509": {
        "file": "04 - Data Replication.pdf",
        "page": 5,
        "chunk": "AWS EC2 Pricing - Oct 2024 6 > $78,000/month https://aws.amazon.com/ec2/pricing/on-demand/"
    },
    "510": {
        "file": "04 - Data Replication.pdf",
        "page": 6,
        "chunk": "Horizontal Scaling - Shared Nothing Architectures \u25cfEach node has its own CPU, memory, and disk \u25cfCoordination via application layer using conventional network \u25cfGeographically distributed \u25cfCommodity hardware 7"
    },
    "511": {
        "file": "04 - Data Replication.pdf",
        "page": 7,
        "chunk": "Data - Replication vs Partitioning 8 Replicates have same data as Main Partitions have a subset of the data"
    },
    "512": {
        "file": "04 - Data Replication.pdf",
        "page": 8,
        "chunk": "Replication 9"
    },
    "513": {
        "file": "04 - Data Replication.pdf",
        "page": 9,
        "chunk": "Common Strategies for Replication - Single leader model - Multiple leader model - Leaderless model Distributed databases usually adopt one of these strategies. 10"
    },
    "514": {
        "file": "04 - Data Replication.pdf",
        "page": 10,
        "chunk": "Leader-Based Replication - All writes from clients go to the leader - Leader sends replication info to the followers - Followers process the instructions from the leader - Clients can read from either the leader or followers 11"
    },
    "515": {
        "file": "04 - Data Replication.pdf",
        "page": 11,
        "chunk": "Leader-Based Replication 12 This write could NOT be sent to one of the followers\u2026 only the leader."
    },
    "516": {
        "file": "04 - Data Replication.pdf",
        "page": 12,
        "chunk": "Leader-Based Replication - Very Common Strategy Relational: \u25cf MySQL, \u25cf Oracle, \u25cf SQL Server, \u25cf PostgreSQL NoSQL: \u25cf MongoDB, \u25cf RethinkDB (realtime web apps), \u25cf Espresso (LinkedIn) Messaging Brokers: Kafka, RabbitMQ 13"
    },
    "517": {
        "file": "04 - Data Replication.pdf",
        "page": 13,
        "chunk": "How Is Replication Info Transmitted to Followers? 14 Replication Method Description Statement-based Send INSERT, UPDATE, DELETEs to replica. Simple but error-prone due to non-deterministic functions like now(), trigger side-effects, and dif\ufb01culty in handling concurrent transactions. Write-ahead Log (WAL) A byte-level speci\ufb01c log of every change to the database. Leader and all followers must implement the same storage engine and makes upgrades dif\ufb01cult. Logical (row-based) Log For relational DBs: Inserted rows, modi\ufb01ed rows (before and after), deleted rows. A transaction log will identify all the rows that changed in each transaction and how they changed. Logical logs are decoupled from the storage engine and easier to parse. Trigger-based Changes are logged to a separate table whenever a trigger \ufb01res in response to an insert, update, or delete. Flexible because you can have application speci\ufb01c replication, but also more error prone."
    },
    "518": {
        "file": "04 - Data Replication.pdf",
        "page": 14,
        "chunk": "Synchronous vs Asynchronous Replication Synchronous: Leader waits for a response from the follower Asynchronous: Leader doesn\u2019t wait for con\ufb01rmation. 15 Synchronous: Asynchronous:"
    },
    "519": {
        "file": "04 - Data Replication.pdf",
        "page": 15,
        "chunk": "What Happens When the Leader Fails? Challenges: How do we pick a new Leader Node? \u25cfConsensus strategy \u2013 perhaps based on who has the most updates? \u25cfUse a controller node to appoint new leader? AND\u2026 how do we con\ufb01gure clients to start writing to the new leader? 16"
    },
    "520": {
        "file": "04 - Data Replication.pdf",
        "page": 16,
        "chunk": "What Happens When the Leader Fails? More Challenges: \u25cf If asynchronous replication is used, new leader may not have all the writes How do we recover the lost writes? Or do we simply discard? \u25cf After (if?) the old leader recovers, how do we avoid having multiple leaders receiving con\ufb02icting data? (Split brain: no way to resolve con\ufb02icting requests. \u25cf Leader failure detection. Optimal timeout is tricky. 17"
    },
    "521": {
        "file": "04 - Data Replication.pdf",
        "page": 17,
        "chunk": "Replication Lag refers to the time it takes for writes on the leader to be re\ufb02ected on all of the followers. \u25cfSynchronous replication: Replication lag causes writes to be slower and the system to be more brittle as num followers increases. \u25cfAsynchronous replication: We maintain availability but at the cost of delayed or eventual consistency. This delay is called the inconsistency window. Replication Lag 18"
    },
    "522": {
        "file": "04 - Data Replication.pdf",
        "page": 18,
        "chunk": "Read-after-Write Consistency Scenario - you\u2019re adding a comment to a Reddit post\u2026 after you click Submit and are back at the main post, your comment should show up for you. - Less important for other users to see your comment as immediately. 19"
    },
    "523": {
        "file": "04 - Data Replication.pdf",
        "page": 19,
        "chunk": "Implementing Read-After-Write Consistency Method 1: Modi\ufb01able data (from the client\u2019s perspective) is always read from the leader. 20"
    },
    "524": {
        "file": "04 - Data Replication.pdf",
        "page": 20,
        "chunk": "Implementing Read-After-Write Consistency Method 2: Dynamically switch to reading from leader for \u201crecently updated\u201d data. - For example, have a policy that all requests within one minute of last update come from leader. 21"
    },
    "525": {
        "file": "04 - Data Replication.pdf",
        "page": 21,
        "chunk": "But\u2026 This Can Create Its Own Challenges 22 We created followers so they would be proximal to users. BUT\u2026 now we have to route requests to distant leaders when reading modi\ufb01able data?? :("
    },
    "526": {
        "file": "04 - Data Replication.pdf",
        "page": 22,
        "chunk": "Monotonic Read Consistency Monotonic read anomalies: occur when a user reads values out of order from multiple followers. Monotonic read consistency: ensures that when a user makes multiple reads, they will not read older data after previously reading newer data. 23"
    },
    "527": {
        "file": "04 - Data Replication.pdf",
        "page": 23,
        "chunk": "Consistent Pre\ufb01x Reads Reading data out of order can occur if different partitions replicate data at different rates. There is no global write consistency. Consistent Pre\ufb01x Read Guarantee - ensures that if a sequence of writes happens in a certain order, anyone reading those writes will see them appear in the same order. 24 A B How far into the future can you see, Ms. B? About 10 seconds usually, Mr A."
    },
    "528": {
        "file": "04 - Data Replication.pdf",
        "page": 24,
        "chunk": "?? 25"
    },
    "529": {
        "file": "09 - Introduction to Graph Data Model.pdf",
        "page": 0,
        "chunk": "DS 4300 Introduction to the Graph Data Model Mark Fontenot, PhD Northeastern University Material referenced from Graph Algorithms - Practical Examples in Apache Spark and Neo4j by Needham and Hodler (O\u2019Reilly Press, 2019)"
    },
    "530": {
        "file": "09 - Introduction to Graph Data Model.pdf",
        "page": 1,
        "chunk": "What is a Graph Database - Data model based on the graph data structure - Composed of nodes and edges - edges connect nodes - each is uniquely identi\ufb01ed - each can contain properties (e.g. name, occupation, etc) - supports queries based on graph-oriented operations - traversals - shortest path - lots of others 2"
    },
    "531": {
        "file": "09 - Introduction to Graph Data Model.pdf",
        "page": 2,
        "chunk": "Where do Graphs Show up? - Social Networks - yes\u2026 things like Instagram, - but also\u2026 modeling social interactions in \ufb01elds like psychology and sociology - The Web - it is just a big graph of \u201cpages\u201d (nodes) connected by hyperlinks (edges) - Chemical and biological data - systems biology, genetics, etc. - interaction relationships in chemistry 3"
    },
    "532": {
        "file": "09 - Introduction to Graph Data Model.pdf",
        "page": 3,
        "chunk": "Basics of Graphs and Graph Theory 4"
    },
    "533": {
        "file": "09 - Introduction to Graph Data Model.pdf",
        "page": 4,
        "chunk": "What is a graph? Labeled Property Graph - Composed of a set of node (vertex) objects and relationship (edge) objects - Labels are used to mark a node as part of a group - Properties are attributes (think KV pairs) and can exist on nodes and relationships - Nodes with no associated relationships are OK. Edges not connected to nodes are not permitted. 5"
    },
    "534": {
        "file": "09 - Introduction to Graph Data Model.pdf",
        "page": 5,
        "chunk": "Example 2 Labels: - person - car 4 relationship types: - Drives - Owns - Lives_with - Married_to Properties 6"
    },
    "535": {
        "file": "09 - Introduction to Graph Data Model.pdf",
        "page": 6,
        "chunk": "Paths A path is an ordered sequence of nodes connected by edges in which no nodes or edges are repeated. 7 1 2 3 6 5 4 Ex: 1 \u2192 2 \u2192 6 \u2192 5 Not a path: 1 \u2192 2 \u2192 6 \u2192 2 \u2192 3"
    },
    "536": {
        "file": "09 - Introduction to Graph Data Model.pdf",
        "page": 7,
        "chunk": "Flavors of Graphs Connected (vs. Disconnected) \u2013 there is a path between any two nodes in the graph Weighted (vs. Unweighted) \u2013 edge has a weight property (important for some algorithms) Directed (vs. Undirected) \u2013 relationships (edges) de\ufb01ne a start and end node Acyclic (vs. Cyclic) \u2013 Graph contains no cycles 8"
    },
    "537": {
        "file": "09 - Introduction to Graph Data Model.pdf",
        "page": 8,
        "chunk": "Connected vs. Disconnected 9"
    },
    "538": {
        "file": "09 - Introduction to Graph Data Model.pdf",
        "page": 9,
        "chunk": "Weighted vs. Unweighted 10"
    },
    "539": {
        "file": "09 - Introduction to Graph Data Model.pdf",
        "page": 10,
        "chunk": "Directed vs. Undirected 11"
    },
    "540": {
        "file": "09 - Introduction to Graph Data Model.pdf",
        "page": 11,
        "chunk": "Cyclic vs Acyclic 12"
    },
    "541": {
        "file": "09 - Introduction to Graph Data Model.pdf",
        "page": 12,
        "chunk": "Sparse vs. Dense 13"
    },
    "542": {
        "file": "09 - Introduction to Graph Data Model.pdf",
        "page": 13,
        "chunk": "Trees 14"
    },
    "543": {
        "file": "09 - Introduction to Graph Data Model.pdf",
        "page": 14,
        "chunk": "Types of Graph Algorithms - Path\ufb01nding - Path\ufb01nding - \ufb01nding the shortest path between two nodes, if one exists, is probably the most common operation - \u201cshortest\u201d means fewest edges or lowest weight - Average Shortest Path can be used to monitor ef\ufb01ciency and resiliency of networks. - Minimum spanning tree, cycle detection, max/min \ufb02ow\u2026 are other types of path\ufb01nding 15"
    },
    "544": {
        "file": "09 - Introduction to Graph Data Model.pdf",
        "page": 15,
        "chunk": "BFS vs DFS 16"
    },
    "545": {
        "file": "09 - Introduction to Graph Data Model.pdf",
        "page": 16,
        "chunk": "Shortest Path 17"
    },
    "546": {
        "file": "09 - Introduction to Graph Data Model.pdf",
        "page": 17,
        "chunk": "Types of Graph Algorithms - Centrality & Community Detection - Centrality - determining which nodes are \u201cmore important\u201d in a network compared to other nodes - EX: Social Network In\ufb02uencers? - Community Detection - evaluate clustering or partitioning of nodes of a graph and tendency to strengthen or break apart 18"
    },
    "547": {
        "file": "09 - Introduction to Graph Data Model.pdf",
        "page": 18,
        "chunk": "Centrality 19"
    },
    "548": {
        "file": "09 - Introduction to Graph Data Model.pdf",
        "page": 19,
        "chunk": "Some Famous Graph Algorithms - Dijkstra\u2019s Algorithm - single-source shortest path algo for positively weighted graphs - A* Algorithm - Similar to Dijkstra\u2019s with added feature of using a heuristic to guide traversal - PageRank - measures the importance of each node within a graph based on the number of incoming relationships and the importance of the nodes from those incoming relationships 20"
    },
    "549": {
        "file": "09 - Introduction to Graph Data Model.pdf",
        "page": 20,
        "chunk": "Neo4j - A Graph Database System that supports both transactional and analytical processing of graph-based data - Relatively new class of no-sql DBs - Considered schema optional (one can be imposed) - Supports various types of indexing - ACID compliant - Supports distributed computing - Similar: Microsoft CosmoDB, Amazon Neptune 21"
    },
    "550": {
        "file": "09 - Introduction to Graph Data Model.pdf",
        "page": 21,
        "chunk": "?? 22"
    },
    "551": {
        "file": "02 - Foundations.pdf",
        "page": 0,
        "chunk": "DS 4300 Large Scale Information Storage and Retrieval Foundations Mark Fontenot, PhD Northeastern University"
    },
    "552": {
        "file": "02 - Foundations.pdf",
        "page": 1,
        "chunk": "Searching \u25cfSearching is the most common operation performed by a database system \u25cfIn SQL, the SELECT statement is arguably the most versatile / complex. \u25cfBaseline for ef\ufb01ciency is Linear Search \u25cb Start at the beginning of a list and proceed element by element until: \u25a0 You \ufb01nd what you\u2019re looking for \u25a0 You get to the last element and haven\u2019t found it 2"
    },
    "553": {
        "file": "02 - Foundations.pdf",
        "page": 2,
        "chunk": "Searching \u25cfRecord - A collection of values for attributes of a single entity instance; a row of a table \u25cfCollection - a set of records of the same entity type; a table \u25cb Trivially, stored in some sequential order like a list \u25cfSearch Key - A value for an attribute from the entity type \u25cb Could be >= 1 attribute 3"
    },
    "554": {
        "file": "02 - Foundations.pdf",
        "page": 3,
        "chunk": "Lists of Records \u25cfIf each record takes up x bytes of memory, then for n records, we need n*x bytes of memory. \u25cfContiguously Allocated List \u25cb All n*x bytes are allocated as a single \u201cchunk\u201d of memory \u25cfLinked List \u25cb Each record needs x bytes + additional space for 1 or 2 memory addresses \u25cb Individual records are linked together in a type of chain using memory addresses 4"
    },
    "555": {
        "file": "02 - Foundations.pdf",
        "page": 4,
        "chunk": "Contiguous vs Linked 5 6 Records Contiguously Allocated - Array front back 6 Records Linked by memory addresses - Linked List Extra storage for a memory address"
    },
    "556": {
        "file": "02 - Foundations.pdf",
        "page": 5,
        "chunk": "Pros and Cons \u25cfArrays are faster for random access, but slow for inserting anywhere but the end \u25cfLinked Lists are faster for inserting anywhere in the list, but slower for random access 6 Insert after 2nd record records: records: 5 records had to be moved to make space Insert after 2nd record"
    },
    "557": {
        "file": "02 - Foundations.pdf",
        "page": 6,
        "chunk": "Observations: - Arrays - fast for random access - slow for random insertions - Linked Lists - slow for random access - fast for random insertions 7"
    },
    "558": {
        "file": "02 - Foundations.pdf",
        "page": 7,
        "chunk": "Binary Search \u25cf Input: array of values in sorted order, target value \u25cf Output: the location (index) of where target is located or some value indicating target was not found def binary_search(arr, target) left, right = 0, len(arr) - 1 while left <= right: mid = (left + right) // 2 if arr[mid] == target: return mid elif arr[mid] < target: left = mid + 1 else: right = mid - 1 return -1 8 A C G M P R Z target = A mid Since target < arr[mid], we reset right to mid - 1. left right A C G M P R Z target = A mid left right"
    },
    "559": {
        "file": "02 - Foundations.pdf",
        "page": 8,
        "chunk": "Time Complexity \u25cfLinear Search \u25cb Best case: target is found at the \ufb01rst element; only 1 comparison \u25cb Worst case: target is not in the array; n comparisons \u25cb Therefore, in the worst case, linear search is O(n) time complexity. \u25cfBinary Search \u25cb Best case: target is found at mid; 1 comparison (inside the loop) \u25cb Worst case: target is not in the array; log2 n comparisons \u25cb Therefore, in the worst case, binary search is O(log2n) time complexity. 9"
    },
    "560": {
        "file": "02 - Foundations.pdf",
        "page": 9,
        "chunk": "Back to Database Searching \u25cf Assume data is stored on disk by column id\u2019s value \u25cf Searching for a speci\ufb01c id = fast. \u25cf But what if we want to search for a speci\ufb01c specialVal? \u25cb Only option is linear scan of that column \u25cf Can\u2019t store data on disk sorted by both id and specialVal (at the same time) \u25cb data would have to be duplicated \u2192 space inef\ufb01cient 10"
    },
    "561": {
        "file": "02 - Foundations.pdf",
        "page": 10,
        "chunk": "Back to Database Searching \u25cf Assume data is stored on disk by column id\u2019s value \u25cf Searching for a speci\ufb01c id = fast. \u25cf But what if we want to search for a speci\ufb01c specialVal? \u25cb Only option is linear scan of that column \u25cf Can\u2019t store data on disk sorted by both id and specialVal (at the same time) \u25cb data would have to be duplicated \u2192 space inef\ufb01cient 11 We need an external data structure to support faster searching by specialVal than a linear scan."
    },
    "562": {
        "file": "02 - Foundations.pdf",
        "page": 11,
        "chunk": "What do we have in our arsenal? 1) An array of tuples (specialVal, rowNumber) sorted by specialVal a) We could use Binary Search to quickly locate a particular specialVal and \ufb01nd its corresponding row in the table b) But, every insert into the table would be like inserting into a sorted array - slow\u2026 2) A linked list of tuples (specialVal, rowNumber) sorted by specialVal a) searching for a specialVal would be slow - linear scan required b) But inserting into the table would theoretically be quick to also add to the list. 12"
    },
    "563": {
        "file": "02 - Foundations.pdf",
        "page": 12,
        "chunk": "Something with Fast Insert and Fast Search? - Binary Search Tree - a binary tree where every node in the left subtree is less than its parent and every node in the right subtree is greater than its parent. 13 Image from: https://courses.grainger.illinois.edu/cs225/sp2019/notes/bst/"
    },
    "564": {
        "file": "02 - Foundations.pdf",
        "page": 13,
        "chunk": "To the Board! 14"
    },
    "565": {
        "file": "10 - Neo4j.pdf",
        "page": 0,
        "chunk": "DS 4300 Neo4j Mark Fontenot, PhD Northeastern University Material referenced from Graph Algorithms - Practical Examples in Apache Spark and Neo4j by Needham and Hodler (O\u2019Reilly Press, 2019)"
    },
    "566": {
        "file": "10 - Neo4j.pdf",
        "page": 1,
        "chunk": "Neo4j - A Graph Database System that supports both transactional and analytical processing of graph-based data - Relatively new class of no-sql DBs - Considered schema optional (one can be imposed) - Supports various types of indexing - ACID compliant - Supports distributed computing - Similar: Microsoft CosmoDB, Amazon Neptune 2"
    },
    "567": {
        "file": "10 - Neo4j.pdf",
        "page": 2,
        "chunk": "Neo4j - Query Language and Plugins - Cypher - Neo4j\u2019s graph query language created in 2011 - Goal: SQL-equivalent language for graph databases - Provides a visual way of matching patterns and relationships (nodes)-[:CONNECT_TO]->(otherNodes) - APOC Plugin - Awesome Procedures on Cypher - Add-on library that provides hundreds of procedures and functions - Graph Data Science Plugin - provides ef\ufb01cient implementations of common graph algorithms (like the ones we talked about yesterday) 3"
    },
    "568": {
        "file": "10 - Neo4j.pdf",
        "page": 3,
        "chunk": "Neo4j in Docker Compose 4"
    },
    "569": {
        "file": "10 - Neo4j.pdf",
        "page": 4,
        "chunk": "Docker Compose 5 \u25cfSupports multi-container management. \u25cfSet-up is declarative - using YAML docker-compose.yaml \ufb01le \u25cb services \u25cb volumes \u25cb networks, etc. \u25cf1 command can be used to start, stop, or scale a number of services at one time. \u25cfProvides a consistent method for producing an identical environment (no more \u201cwell\u2026 it works on my machine!) \u25cfInteraction is mostly via command line"
    },
    "570": {
        "file": "10 - Neo4j.pdf",
        "page": 5,
        "chunk": "docker-compose.yaml 6 services: neo4j: container_name: neo4j image: neo4j:latest ports: - 7474:7474 - 7687:7687 environment: - NEO4J_AUTH=neo4j/${NEO4J_PASSWORD} - NEO4J_apoc_export_file_enabled=true - NEO4J_apoc_import_file_enabled=true - NEO4J_apoc_import_file_use__neo4j__config=true - NEO4J_PLUGINS=[\"apoc\", \"graph-data-science\"] volumes: - ./neo4j_db/data:/data - ./neo4j_db/logs:/logs - ./neo4j_db/import:/var/lib/neo4j/import - ./neo4j_db/plugins:/plugins Never put \u201csecrets\u201d in a docker compose \ufb01le. Use .env \ufb01les."
    },
    "571": {
        "file": "10 - Neo4j.pdf",
        "page": 6,
        "chunk": ".env Files - .env \ufb01les - stores a collection of environment variables - good way to keep environment variables for different platforms separate - .env.local - .env.dev - .env.prod 7 NEO4J_PASSWORD=abc123!!! .env file"
    },
    "572": {
        "file": "10 - Neo4j.pdf",
        "page": 7,
        "chunk": "Docker Compose Commands \u25cfTo test if you have Docker CLI properly installed, run: docker --version \u25cfMajor Docker Commands \u25cb docker compose up \u25cb docker compose up -d \u25cb docker compose down \u25cb docker compose start \u25cb docker compose stop \u25cb docker compose build \u25cb docker compose build --no-cache 8"
    },
    "573": {
        "file": "10 - Neo4j.pdf",
        "page": 8,
        "chunk": "localhost:7474 9"
    },
    "574": {
        "file": "10 - Neo4j.pdf",
        "page": 9,
        "chunk": "Neo4j Browser 10 https://neo4j.com/docs/browser-manual/current/visual-tour/ localhost:7474 Then login."
    },
    "575": {
        "file": "10 - Neo4j.pdf",
        "page": 10,
        "chunk": "Inserting Data by Creating Nodes CREATE (:User {name: \"Alice\", birthPlace: \"Paris\"}) CREATE (:User {name: \"Bob\", birthPlace: \"London\"}) CREATE (:User {name: \"Carol\", birthPlace: \"London\"}) CREATE (:User {name: \"Dave\", birthPlace: \"London\"}) CREATE (:User {name: \"Eve\", birthPlace: \"Rome\"}) 11"
    },
    "576": {
        "file": "10 - Neo4j.pdf",
        "page": 11,
        "chunk": "Adding an Edge with No Variable Names CREATE (:User {name: \"Alice\", birthPlace: \"Paris\"}) CREATE (:User {name: \"Bob\", birthPlace: \"London\"}) MATCH (alice:User {name:\u201dAlice\u201d}) MATCH (bob:User {name: \u201cBob\u201d}) CREATE (alice)-[:KNOWS {since: \u201c2022-12-01\u201d}]->(bob) 12 Note: Relationships are directed in neo4j."
    },
    "577": {
        "file": "10 - Neo4j.pdf",
        "page": 12,
        "chunk": "Matching Which users were born in London? MATCH (usr:User {birthPlace: \u201cLondon\u201d}) RETURN usr.name, usr.birthPlace 13"
    },
    "578": {
        "file": "10 - Neo4j.pdf",
        "page": 13,
        "chunk": "Download Dataset and Move to Import Folder Clone this repo: https://github.com/PacktPublishing/Graph-Data-Science-with-Neo4j In Chapter02/data of data repo, unzip the net\ufb02ix.zip \ufb01le Copy net\ufb02ix_titles.csv into the following folder where you put your docker compose \ufb01le neo4j_db/neo4j_db/import 14"
    },
    "579": {
        "file": "10 - Neo4j.pdf",
        "page": 14,
        "chunk": "Importing Data 15"
    },
    "580": {
        "file": "10 - Neo4j.pdf",
        "page": 15,
        "chunk": "Basic Data Importing LOAD CSV WITH HEADERS FROM 'file:///netflix_titles.csv' AS line CREATE(:Movie { id: line.show_id, title: line.title, releaseYear: line.release_year } ) 16 Type the following into the Cypher Editor in Neo4j Browser"
    },
    "581": {
        "file": "10 - Neo4j.pdf",
        "page": 16,
        "chunk": "Loading CSVs - General Syntax LOAD CSV [WITH HEADERS] FROM 'file:///file_in_import_folder.csv' AS line [FIELDTERMINATOR ','] // do stuffs with 'line' 17"
    },
    "582": {
        "file": "10 - Neo4j.pdf",
        "page": 17,
        "chunk": "Importing with Directors this Time LOAD CSV WITH HEADERS FROM 'file:///netflix_titles.csv' AS line WITH split(line.director, \",\") as directors_list UNWIND directors_list AS director_name CREATE (:Person {name: trim(director_name)}) But this generates duplicate Person nodes (a director can direct more than 1 movie) 18"
    },
    "583": {
        "file": "10 - Neo4j.pdf",
        "page": 18,
        "chunk": "Importing with Directors Merged MATCH (p:Person) DELETE p LOAD CSV WITH HEADERS FROM 'file:///netflix_titles.csv' AS line WITH split(line.director, \",\") as directors_list UNWIND directors_list AS director_name MERGE (:Person {name: director_name}) 19"
    },
    "584": {
        "file": "10 - Neo4j.pdf",
        "page": 19,
        "chunk": "Adding Edges LOAD CSV WITH HEADERS FROM 'file:///netflix_titles.csv' AS line MATCH (m:Movie {id: line.show_id}) WITH m, split(line.director, \",\") as directors_list UNWIND directors_list AS director_name MATCH (p:Person {name: director_name}) CREATE (p)-[:DIRECTED]->(m) 20"
    },
    "585": {
        "file": "10 - Neo4j.pdf",
        "page": 20,
        "chunk": "Gut Check Let\u2019s check the movie titled Ray: MATCH (m:Movie {title: \"Ray\"})<-[:DIRECTED]-(p:Person) RETURN m, p 21"
    },
    "586": {
        "file": "10 - Neo4j.pdf",
        "page": 21,
        "chunk": "?? 22"
    },
    "587": {
        "file": "BST Trees.pdf",
        "page": 0,
        "chunk": "Binary Search Trees \u25cf\u200b Insertion - A new key is always inserted at the leaf by maintaining the property of the binary search tree. We start searching for a key from the root until we hit a leaf node. Once a leaf node is found, the new node is added as a child of the leaf node. The below steps are followed while we try to insert a node into a binary search tree: \u25cb\u200b Initialize the current node (say, currNode or node) with root node \u25cb\u200b Compare the key with the current node. \u25cb\u200b Move left if the key is less than or equal to the current node value. \u25cb\u200b Move right if the key is greater than the current node value. \u25cb\u200b Repeat steps 2 and 3 until you reach a leaf node. \u25cb\u200b Attach the new key as a left or right child based on the comparison with the leaf node\u2019s value. \u25cf\u200b Traversal \u25cb\u200b Preorder Traversal \u25a0\u200b The root node of the subtree is visited first. \u25a0\u200b Then the left subtree is traversed. \u25a0\u200b At last, the right subtree is traversed. \u25cb\u200b Postorder Traversal \u25a0\u200b The root node of the subtree is visited first. \u25a0\u200b Then the left subtree is traversed. \u25a0\u200b At last, the right subtree is traversed. \u25cb\u200b Inorder Traversal \u25a0\u200b The left subtree is traversed first \u25a0\u200b Then the root node for that subtree is traversed \u25a0\u200b Finally, the right subtree is traversed \u25cb\u200b Level Order Traversal \u25a0\u200b Given the root of a binary tree, return the level order traversal of its nodes' values. (i.e., from left to right, level by level)."
    },
    "588": {
        "file": "BST Trees.pdf",
        "page": 0,
        "chunk": "binary tree, return the level order traversal of its nodes' values. (i.e., from left to right, level by level)."
    },
    "589": {
        "file": "Extended Notes - Foundations Slide Deck.pdf",
        "page": 0,
        "chunk": "Searching: -\u200b Searching is the most common operation performed by a database system -\u200b In SQL, the SELECT statement is arguably the most versatile / complex (they can be recursive and there can even be select statements in select statements) General Vocab: -\u200b Record: a collection of values for attributes of a single entity instance; a row of a table -\u200b Collection: a set of records of the same entity type; a table (trivially stored in some sequential order like a list) -\u200b Search Key: a value for an attribute from the entity type (could be >= 1 attribute) id specVal 1 55 2 87 3 50 4 108 -\u200b Assume data is stored on disk by column id\u2019s value, searching for a specific id is fast -\u200b But searching for specific specialVal since data is unsorted, the only option is linear scan the column -\u200b Can\u2019t store data on disk sorted by both id and specialVal at the same time so the data would have to be duplicated and there\u2019d be inefficient space -\u200b Therefore we need an external data structure to support faster searching by specialVal than a linear search -\u200b What to do? -\u200b An array of tuples (specialVal, rowNumber) sorted by specialVal -\u200b We could use Binary Search to quickly locate a particular specialVal and find its corresponding row in the table -\u200b But, every insert into the table would be like inserting into a sorted array - slow\u2026 -\u200b OR A linked list of tuples (specialVal, rowNumber) sorted by specialVal -\u200b searching for a specialVal would be slow - linear scan required -\u200b But inserting into the table would theoretically be quick to also add to the list\u2026 INSTEAD USE BINARY SEARCH TREE Linear Search: -\u200b Baseline for efficiency where you start at the beginning"
    },
    "590": {
        "file": "Extended Notes - Foundations Slide Deck.pdf",
        "page": 0,
        "chunk": "(specialVal, rowNumber) sorted by specialVal -\u200b searching for a specialVal would be slow - linear scan required -\u200b But inserting into the table would theoretically be quick to also add to the list\u2026 INSTEAD USE BINARY SEARCH TREE Linear Search: -\u200b Baseline for efficiency where you start at the beginning of a list and proceed element element by element until you either find what you\u2019re looking for or get to the last element and haven\u2019t found it aka O(n) -\u200b If each record takes up x bytes of memory, then for n records, we need n*x bytes of memory -\u200b There are 2 different Data Structures for Linear Search: 1.\u200b Contiguously Allocated List (aka Array): all n*x are allocated as a single \u201cchunk\u201d of memory"
    },
    "591": {
        "file": "Extended Notes - Foundations Slide Deck.pdf",
        "page": 1,
        "chunk": "-\u200b Pro: Faster for random access -\u200b Con: Slow for inserting anywhere but the end 2.\u200b Linked List: each record needs x bytes and additional space for 1 or 2 memory addresses - individual records are linked together in a type of chain using memory addresses -\u200b Pro: Faster for inserting anywhere in the list -\u200b Con: Slower for random access -\u200b Best Case: target is found at the first element where only 1 comparison is needed -\u200b Worst Case: target is not in the array; n comparisons - O(n) time complexity is for worst case Binary Search: -\u200b Input: array of values in sorted order, target value -\u200b Output: location (index) of where target is located or some value indicating target was not found -\u200b Best Case: target is found at mid; 1 comparison (inside the loop) -\u200b Worst Case: target is not in the array; log2n comparisons - O(log2n) time complexity is for worse case Binary Search Tree: -\u200b A binary tree where every node in the left subtree is less than its parent and every node in the right subtree is greater than its parent"
    },
    "592": {
        "file": "Extended Notes - NoSQL Intro + KV DBs.pdf",
        "page": 0,
        "chunk": "Distributed DBs and ACID - Pessimistic Concurrency: ACID Transactions -\u200b ACID is a set of properties that guarantee reliable processing of database transactions: -\u200b Atomicity: Each transaction is treated as a single, indivisible unit. Either all operations in the transaction succeed, or none of them do. -\u200b Consistency: Transactions take the database from one valid state to another, preserving the integrity constraints. -\u200b Isolation: Transactions are isolated from each other, ensuring that concurrently executing transactions do not interfere. -\u200b Durability: Once a transaction is committed, it\u2019s permanent and will survive system crashes. Pessimistic Concurrency Control -\u200b Pessimistic concurrency control is a strategy for managing concurrent access to database resources in a way that prevents conflicts. -\u200b The key idea behind pessimistic concurrency is that conflicts between transactions are likely to happen, so the system assumes that if something can go wrong, it probably will. As a result, it proactively prevents conflicts from happening during the execution of transactions. -\u200b In other words, it operates on the assumption that other transactions will interfere, so it uses techniques to lock resources to avoid conflicts. How Does Pessimistic Concurrency Work? -\u200b Locking Resources: -\u200b To avoid concurrent transactions from interfering with each other, the database will lock resources (e.g., rows, tables, or even entire databases) until the transaction is complete. -\u200b This ensures that no other transaction can modify the same data at the same time. -\u200b There are two types of locks used: 1.\u200b Read Locks: Prevent other transactions from writing to the resource but allow them to read it. 2.\u200b Write Locks: Prevent other transactions from reading or writing to the resource until the lock is released. -\u200b This creates a serialized execution of transactions, ensuring consistency and isolation but at the cost of performance and concurrency. Write Lock Analogy Analogy:"
    },
    "593": {
        "file": "Extended Notes - NoSQL Intro + KV DBs.pdf",
        "page": 0,
        "chunk": "the resource but allow them to read it. 2.\u200b Write Locks: Prevent other transactions from reading or writing to the resource until the lock is released. -\u200b This creates a serialized execution of transactions, ensuring consistency and isolation but at the cost of performance and concurrency. Write Lock Analogy Analogy: Borrowing a Book from a Library -\u200b Imagine you want to borrow a book from the library. When you take the book, no one else can borrow it until you return it. -\u200b This is similar to a write lock: when a transaction locks a resource (e.g., a row in a database), no other transaction can access it (either for reading or writing) until the transaction is completed and the lock is released."
    },
    "594": {
        "file": "Extended Notes - NoSQL Intro + KV DBs.pdf",
        "page": 1,
        "chunk": "-\u200b Example: -\u200b Transaction A locks \"Book A\" (read/write lock). -\u200b Transaction B wants to read \"Book A\" but cannot until Transaction A finishes and releases the lock. Why Pessimistic Concurrency? -\u200b Data Safety: -\u200b The focus of pessimistic concurrency is on data safety and ensuring that transactions are executed correctly without interference. -\u200b By using locks, the system prevents conflicts and ensures that only one transaction can alter data at any given time, preventing inconsistencies or corruption. -\u200b Use Cases: -\u200b Pessimistic concurrency control is ideal for environments where conflicts are highly likely or when data integrity is critical (e.g., financial systems, inventory systems, or applications with high-value transactions). Challenges of Pessimistic Concurrency -\u200b Deadlock: Since transactions hold locks, they can sometimes end up in a situation where two transactions are waiting on each other to release a resource, leading to a deadlock. The system must have a mechanism for detecting and resolving deadlocks, often by aborting one of the transactions to break the cycle. -\u200b Performance Impact: Locking can significantly reduce throughput and increase response times, especially when there are many concurrent transactions. Since each transaction has to wait for others to release locks, this leads to inefficiencies in highly concurrent environments. -\u200b Additionally, the more data that is locked, the greater the chance of contention between transactions, further slowing down performance. Optimistic Concurrency: Overview -\u200b In optimistic concurrency control, transactions do not acquire locks on data when reading or writing. Instead, the system assumes that conflicts will be rare, and thus transactions proceed under the assumption that no other transaction will interfere with their work. -\u200b Optimistic because it works under the assumption that conflicts between transactions are unlikely to happen, and even if they do, it\u2019s not a problem. This approach minimizes contention for resources and"
    },
    "595": {
        "file": "Extended Notes - NoSQL Intro + KV DBs.pdf",
        "page": 1,
        "chunk": "be rare, and thus transactions proceed under the assumption that no other transaction will interfere with their work. -\u200b Optimistic because it works under the assumption that conflicts between transactions are unlikely to happen, and even if they do, it\u2019s not a problem. This approach minimizes contention for resources and allows for more concurrent transactions. How Does Optimistic Concurrency Work? 1.\u200b Reading Data: -\u200b A transaction reads the data it needs to work with and also fetches a timestamp or version number associated with the data. -\u200b These are used to track when the data was last modified and are attached to each row in the database. 2.\u200b Making Changes:"
    },
    "596": {
        "file": "Extended Notes - NoSQL Intro + KV DBs.pdf",
        "page": 2,
        "chunk": "-\u200b The transaction proceeds with making changes to the data without locking it, assuming no one else is modifying the same data at the same time. -\u200b During this phase, other transactions can read and write to the same data without interference (because no locks are in place). 3.\u200b Checking for Conflicts: -\u200b Before committing the transaction, it checks if any of the data it worked with has been modified by another transaction since it was first read. This is done by comparing the timestamp or version number that was initially retrieved with the current state of the data. -\u200b If the data has been modified (i.e., if the timestamp/version number doesn\u2019t match), a conflict is detected, and the transaction may be rolled back and retried. 4.\u200b Commit: -\u200b If no conflicts are found, the transaction proceeds to commit the changes, and the new timestamp/version number is updated in the database. Why Is It Optimistic? -\u200b Optimism comes from the assumption that conflicts are rare and that most transactions can proceed without interference. Rather than preemptively locking resources (as in pessimistic concurrency), the system relies on validation at commit time to detect conflicts. -\u200b The idea is that, by not locking data during the transaction, you can achieve higher throughput and better concurrency, especially in systems with lower conflict rates. Types of Systems Where Optimistic Concurrency Works Well 1.\u200b Low Conflict Systems -\u200b Backups: Systems performing operations like backups where few transactions occur concurrently. -\u200b Analytical Databases: In environments where data is mostly read-heavy and transactions involve more querying than updating, conflicts are less likely, and optimistic concurrency works effectively. -\u200b In these systems, conflicts are rare, and when they do occur, it\u2019s often acceptable to roll back and retry the transaction. -\u200b Benefits: -\u200b Higher throughput: Without the need"
    },
    "597": {
        "file": "Extended Notes - NoSQL Intro + KV DBs.pdf",
        "page": 2,
        "chunk": "data is mostly read-heavy and transactions involve more querying than updating, conflicts are less likely, and optimistic concurrency works effectively. -\u200b In these systems, conflicts are rare, and when they do occur, it\u2019s often acceptable to roll back and retry the transaction. -\u200b Benefits: -\u200b Higher throughput: Without the need for locks, more transactions can run concurrently, improving the performance of read-heavy systems. -\u200b Less contention: Since no locks are being placed, there is no waiting for locks to be released, so overall system performance can improve. 2.\u200b Read-heavy Systems -\u200b In read-heavy systems, most operations involve retrieving data rather than modifying it. Optimistic concurrency is a good fit for these environments because the likelihood of data contention (two transactions trying to modify the same data at the same time) is minimal."
    },
    "598": {
        "file": "Extended Notes - NoSQL Intro + KV DBs.pdf",
        "page": 3,
        "chunk": "-\u200b Example: A news website where users are reading articles, but only a few are submitting comments or editing content. Most transactions are reads, so the likelihood of conflicts is low, and optimistic concurrency can ensure that the system scales efficiently. Handling Conflicts in Optimistic Concurrency -\u200b If two transactions try to modify the same data at the same time, optimistic concurrency detects this at the commit phase. -\u200b If a conflict is detected (i.e., the data has been modified by another transaction after it was read), the transaction is rolled back and the process is retried. -\u200b This rollback and retry mechanism means the system can handle conflicts without locking data, but it introduces an overhead because transactions need to be retried if conflicts are detected. High Conflict Systems -\u200b Challenges: -\u200b In high conflict systems, where many transactions are competing for the same data, the rollback and retry approach becomes less efficient. -\u200b If conflicts happen frequently, the system might end up spending a lot of time rolling back and retrying transactions, which can reduce performance. -\u200b In such cases, pessimistic concurrency (using locks to prevent conflicts before they happen) may be preferable, even though it might limit concurrency. -\u200b High conflict systems may require a more predictable and controlled approach to ensure consistency. Comparison with Pessimistic Concurrency -\u200b Pessimistic Concurrency (locks data to prevent conflicts before they happen) is more appropriate for high-conflict systems where data contention is frequent, and ensuring consistency is paramount. -\u200b Optimistic Concurrency (checks for conflicts at commit time) is more suitable for low-conflict systems, where the overhead of managing locks is unnecessary and higher concurrency is desirable. No SQL: Origin of the Term \"NoSQL\" -\u200b The term \"NoSQL\" was first used in 1998 by Carlo Strozzi to describe his lightweight, open-source relational"
    },
    "599": {
        "file": "Extended Notes - NoSQL Intro + KV DBs.pdf",
        "page": 3,
        "chunk": "(checks for conflicts at commit time) is more suitable for low-conflict systems, where the overhead of managing locks is unnecessary and higher concurrency is desirable. No SQL: Origin of the Term \"NoSQL\" -\u200b The term \"NoSQL\" was first used in 1998 by Carlo Strozzi to describe his lightweight, open-source relational database system that did not use SQL for querying. -\u200b However, the modern understanding of NoSQL databases is quite different from Strozzi\u2019s original use of the term. Modern Meaning: \"Not Only SQL\" -\u200b Today, NoSQL is generally understood to mean \"Not Only SQL\", rather than strictly \"No SQL\" at all."
    },
    "600": {
        "file": "Extended Notes - NoSQL Intro + KV DBs.pdf",
        "page": 4,
        "chunk": "-\u200b While NoSQL databases often do not follow the traditional relational database model, some still allow for SQL-like querying or structured querying mechanisms. Is NoSQL Always Non-Relational? -\u200b NoSQL databases are often thought of as non-relational databases because they do not use the traditional table-based structure with strict schemas like relational databases (RDBMS). -\u200b Instead, they allow for more flexible data models, including: -\u200b Key-Value Stores (e.g., Redis, DynamoDB) -\u200b Document Stores (e.g., MongoDB, CouchDB) -\u200b Column-Family Stores (e.g., Apache Cassandra, HBase) -\u200b Graph Databases (e.g., Neo4j, ArangoDB) Why NoSQL? -\u200b NoSQL databases were originally developed in response to the need for handling large-scale, web-based, and unstructured data that relational databases struggled with. -\u200b The rise of Big Data, social media, and cloud computing created new challenges that required scalable, flexible, and high-performance database solutions that could handle: -\u200b Massive amounts of unstructured or semi-structured data (e.g., JSON, XML, multimedia content). -\u200b High-speed reads and writes (e.g., caching systems, real-time analytics). -\u200b Distributed and horizontally scalable architectures (e.g., global applications, cloud-based services). Key Characteristics of NoSQL Databases -\u200b Schema Flexibility: Unlike relational databases, NoSQL databases do not require predefined schemas. This makes it easier to evolve data models over time. -\u200b Horizontal Scalability: Many NoSQL databases are designed to scale horizontally (adding more machines) rather than vertically (adding more power to a single machine). -\u200b High Availability & Partition Tolerance: NoSQL databases often favor availability and partition tolerance over strict consistency, as per the CAP theorem. -\u200b Optimized for Specific Use Cases: Rather than a one-size-fits-all approach, NoSQL databases specialize in different types of workloads (e.g., key-value stores for caching, document stores for flexible data representation, graph databases for connected data)."
    },
    "601": {
        "file": "Extended Notes - NoSQL Intro + KV DBs.pdf",
        "page": 4,
        "chunk": "Use Cases: Rather than a one-size-fits-all approach, NoSQL databases specialize in different types of workloads (e.g., key-value stores for caching, document stores for flexible data representation, graph databases for connected data)."
    },
    "602": {
        "file": "Extended Notes - NoSQL Intro + KV DBs.pdf",
        "page": 5,
        "chunk": "CAP Theorem Review -\u200b The CAP theorem, proposed by Eric Brewer in 2000, states that a distributed database system can only guarantee two out of three of the following properties at any given time: 1.\u200b Consistency (C) \u2013 Every user of the database has an identical view of the data at any given instant. -\u200b All nodes return the most recent version of the data. -\u200b No stale or conflicting versions exist. -\u200b If a write occurs, all subsequent reads must reflect that change immediately. 2.\u200b Availability (A) \u2013 The database remains operational even in the event of failures. -\u200b Every request always receives a response (though it may not be the latest data). -\u200b The system does not go down even if some parts fail. 3.\u200b Partition Tolerance (P) \u2013 The database can continue to function even if network failures create partitions that temporarily prevent some nodes from communicating. -\u200b Even if messages between nodes are delayed or lost, the system remains operational. -\u200b Network partitions are inevitable in distributed systems, so databases must decide how to handle them. -\u200b Trade-Offs: Choosing Two Out of Three -\u200b Since it is impossible to achieve all three properties simultaneously, distributed databases must prioritize two based on their use case: 1.\u200b Consistency + Availability (CA) \u2192 No Partition Tolerance -\u200b The system always returns the most up-to-date data and never serves stale reads. -\u200b It remains available under normal conditions but cannot function correctly if the network is partitioned (i.e., when nodes cannot communicate). -\u200b If a partition occurs, the system may refuse requests or enter a failure mode until connectivity is restored."
    },
    "603": {
        "file": "Extended Notes - NoSQL Intro + KV DBs.pdf",
        "page": 5,
        "chunk": "cannot communicate). -\u200b If a partition occurs, the system may refuse requests or enter a failure mode until connectivity is restored."
    },
    "604": {
        "file": "Extended Notes - NoSQL Intro + KV DBs.pdf",
        "page": 6,
        "chunk": "-\u200b Example: Traditional relational databases (e.g., PostgreSQL, MySQL running on a single server). -\u200b A single-node database maintains consistency and availability but fails under network partitions. 2.\u200b Consistency + Partition Tolerance (CP) \u2192 Reduced Availability -\u200b The system always returns the most up-to-date data, even in the event of network failures. -\u200b However, if a partition occurs, some requests may be dropped or the system may become unavailable in order to maintain consistency. -\u200b This ensures data integrity at the cost of availability. -\u200b Example: HBase, Google Bigtable, MongoDB (with strong consistency settings) -\u200b Many banking systems prioritize CP because ensuring correct balances is more important than availability. 3.\u200b Availability + Partition Tolerance (AP) \u2192 Eventual Consistency -\u200b The system remains operational and responsive even when network partitions occur. -\u200b However, users may see slightly stale data due to eventual consistency. -\u200b Over time, updates propagate, and all nodes eventually reach a consistent state. -\u200b Example: DynamoDB, Cassandra, CouchDB, Riak -\u200b Content delivery networks (CDNs) prioritize availability and partition tolerance so that users always get content, even if it\u2019s not the absolute latest version. ACID Alternative for Distrib Systems - BASE -\u200b In distributed systems, strict ACID (Atomicity, Consistency, Isolation, Durability) guarantees can be difficult to maintain due to scalability constraints and the realities of network partitions (per the CAP theorem). To address this, many modern distributed databases adopt a more flexible approach known as BASE: -\u200b BASE is an alternative consistency model designed to prioritize availability and scalability over strict consistency. It consists of three key principles: 1.\u200b Basically Available: -\u200b The system guarantees availability (per the CAP theorem). -\u200b However, responses might indicate that data is incomplete, temporarily inconsistent, or even in a failure state due to ongoing updates. -\u200b The system \"works most of the time\", but"
    },
    "605": {
        "file": "Extended Notes - NoSQL Intro + KV DBs.pdf",
        "page": 6,
        "chunk": "strict consistency. It consists of three key principles: 1.\u200b Basically Available: -\u200b The system guarantees availability (per the CAP theorem). -\u200b However, responses might indicate that data is incomplete, temporarily inconsistent, or even in a failure state due to ongoing updates. -\u200b The system \"works most of the time\", but data accuracy is not always immediate. 2.\u200b Soft State: -\u200b The system's state can change over time, even without additional input. -\u200b This occurs due to eventual consistency mechanisms, such as background synchronization or replication processes. -\u200b Unlike ACID databases, BASE does not require immediate consistency across all replicas."
    },
    "606": {
        "file": "Extended Notes - NoSQL Intro + KV DBs.pdf",
        "page": 7,
        "chunk": "3.\u200b Eventual Consistency: -\u200b While data updates may not be immediately reflected across all nodes, the system guarantees that given enough time and no new updates, all replicas will converge to the same state. -\u200b All writes will eventually propagate across the distributed system, ensuring consistency in the long run. -\u200b This is a fundamental property of AP (Availability + Partition Tolerance) systems from the CAP theorem. -\u200b Examples of BASE-Oriented Databases -\u200b NoSQL Databases (e.g., Apache Cassandra, DynamoDB, Riak) -\u200b Key-Value Stores (e.g., Redis, Amazon S3) -\u200b Document Stores (e.g., MongoDB, CouchDB) -\u200b Eventually Consistent Storage Systems (e.g., Amazon SimpleDB, Cosmos DB) -\u200b When to Choose BASE over ACID - BASE is ideal for applications where: -\u200b High availability and scalability are more important than strict consistency -\u200b Data inconsistency is acceptable for short periods (e.g., social media feeds, recommendation systems) -\u200b The system can tolerate eventual consistency delays (e.g., analytics platforms, logging systems) -\u200b Horizontal scaling is needed to support massive workloads (e.g., global applications with distributed users) -\u200b Example: Imagine a social media platform like Twitter. If a user posts a tweet, their followers might not all see it immediately due to replication lag. However, within seconds or minutes, all servers will eventually update and reflect the post. This trade-off allows for high availability and global scalability, even if it means minor delays in data consistency. Key-Value Stores (3 Key Principles) -\u200b A key-value store is a non-relational database that follows a simple key = value structure. Each piece of data is stored as a unique key (identifier) paired with its corresponding value, making these databases highly efficient and scalable. 1.\u200b Simplicity -\u200b Minimalist Data Model: The key-value model is extremely simple compared to traditional relational databases (RDBMS), where data is stored in structured tables with predefined"
    },
    "607": {
        "file": "Extended Notes - NoSQL Intro + KV DBs.pdf",
        "page": 7,
        "chunk": "Each piece of data is stored as a unique key (identifier) paired with its corresponding value, making these databases highly efficient and scalable. 1.\u200b Simplicity -\u200b Minimalist Data Model: The key-value model is extremely simple compared to traditional relational databases (RDBMS), where data is stored in structured tables with predefined schemas. -\u200b Flat Data Structure: Unlike SQL databases that require tables, rows, and columns, key-value stores use a flat structure, making CRUD (Create, Read, Update, Delete) operations straightforward. -\u200b Schema-less: There are no strict rules on data format, allowing flexible data storage (e.g., JSON, XML, text, binary, etc.)."
    },
    "608": {
        "file": "Extended Notes - NoSQL Intro + KV DBs.pdf",
        "page": 8,
        "chunk": "-\u200b Ideal Use Cases: Caching (storing frequently accessed data for quick retrieval), Session Management (storing user sessions in web apps), Configuration Storage (storing app settings and feature flags) 2.\u200b Speed -\u200b Optimized for Fast Lookups: -\u200b Key-value stores use hash tables or similar data structures under the hood. -\u200b This allows them to retrieve a value by its key in O(1) time complexity \u2192 extremely fast. -\u200b Unlike relational databases that require complex indexing and query optimization, key-value stores provide instant access to values. -\u200b In-Memory Performance: -\u200b Many key-value stores (e.g., Redis, Memcached) run entirely in memory, making reads and writes blazing fast. -\u200b Eliminates the overhead of disk-based storage systems. -\u200b No Complex Queries or Joins: -\u200b Key-value stores do not support SQL-like queries, foreign keys, or joins. -\u200b Why? Because these operations slow down performance, making the system less efficient for its primary use cases. -\u200b If querying and relationships are needed, other NoSQL models (e.g., document stores or column-family stores) are a better fit. -\u200b Example: Redis Cache - Retrieving a cached webpage or API response from Redis can be 10\u2013100x faster than querying a relational database -\u200b Ideal Use Cases: Real-time applications (e.g., leaderboards, message queues), Caching API responses (e.g., storing computed results from expensive operations), Rate limiting (e.g., tracking API usage quotas) 3.\u200b Scalability -\u200b Designed for Horizontal Scaling: -\u200b Unlike relational databases, which scale vertically (adding more CPU/RAM to a single server), key-value stores scale horizontally (by distributing data across multiple nodes). -\u200b This makes them ideal for handling large-scale workloads with millions of concurrent users. -\u200b Eventual Consistency: -\u200b In a distributed key-value store, replicas of data exist across multiple servers. -\u200b The system guarantees eventual consistency, meaning that all nodes will converge to the same value over time, but not necessarily"
    },
    "609": {
        "file": "Extended Notes - NoSQL Intro + KV DBs.pdf",
        "page": 8,
        "chunk": "This makes them ideal for handling large-scale workloads with millions of concurrent users. -\u200b Eventual Consistency: -\u200b In a distributed key-value store, replicas of data exist across multiple servers. -\u200b The system guarantees eventual consistency, meaning that all nodes will converge to the same value over time, but not necessarily instantly. -\u200b Some systems allow users to trade off consistency for higher availability (per the CAP theorem) -\u200b Partitioning (Sharding) -\u200b Data can be easily partitioned across multiple servers by hashing the key (e.g., using consistent hashing)."
    },
    "610": {
        "file": "Extended Notes - NoSQL Intro + KV DBs.pdf",
        "page": 9,
        "chunk": "-\u200b This ensures that reads and writes remain fast and balanced across nodes. -\u200b Example: Amazon DynamoDB - Uses partitioning and replication to distribute billions of key-value pairs across a global infrastructure -\u200b Ideal Use Cases: Distributed caching layers, Global-scale applications, Internet of Things (IoT) data storage KV DS Use Cases - Data Science 1.\u200b EDA & Experimentation Results Store -\u200b Store intermediate results from data preprocessing, exploratory data analysis (EDA), or feature engineering. -\u200b Allows for quick lookups of previous experiment results without needing to recompute expensive transformations. -\u200b Useful for A/B testing: store experiment metadata and results without polluting the production database. -\u200b Benefits: Reduces computation time by caching results, Provides an easy way to resume interrupted experiments, Eliminates unnecessary writes to a structured RDBMS 2.\u200b Feature Store: -\u200b Store frequently accessed ML features for low-latency retrieval during model training and inference. -\u200b Supports real-time feature lookups, which is crucial for applications like fraud detection and recommendation systems. -\u200b Benefits: Faster access to features without recomputation, Reduces latency in model training & serving, Simplifies feature versioning 3.\u200b Model Monitoring & Performance Tracking -\u200b Store key performance metrics for deployed models. -\u200b Track real-time accuracy, precision, recall, drift detection, etc. -\u200b Enables real-time alerting if model performance degrades. -\u200b Enables real-time monitoring and alerting, Fast retrieval of historical model performance, Supports model version tracking KV DS Use Cases - Software Engineering 1.\u200b Storing Session Information -\u200b Everything about a user's current session (e.g., authentication, preferences, browsing history) can be stored in a single PUT operation and retrieved instantly. -\u200b Ideal for stateless web applications, where user sessions must persist across multiple requests. -\u200b Benefits: Fast, single-call retrieval, Reduces reliance on traditional session storage, Works across distributed systems 2.\u200b User Profiles & Preferences -\u200b Personalized user experiences can be enabled"
    },
    "611": {
        "file": "Extended Notes - NoSQL Intro + KV DBs.pdf",
        "page": 9,
        "chunk": "stored in a single PUT operation and retrieved instantly. -\u200b Ideal for stateless web applications, where user sessions must persist across multiple requests. -\u200b Benefits: Fast, single-call retrieval, Reduces reliance on traditional session storage, Works across distributed systems 2.\u200b User Profiles & Preferences -\u200b Personalized user experiences can be enabled with a single GET operation. -\u200b Stores UI preferences, language settings, themes, and notification settings. -\u200b Eliminates the need for complex joins in relational databases."
    },
    "612": {
        "file": "Extended Notes - NoSQL Intro + KV DBs.pdf",
        "page": 10,
        "chunk": "-\u200b Benefits: Fast lookup for user-specific settings, Simplifies UI personalization, Enables instant application of preferences across sessions 3.\u200b Shopping Cart Data -\u200b A shopping cart is tied to a user and must be accessible across browsers, devices, and sessions. -\u200b Key-value stores provide fast retrieval and eliminate session timeouts issues. -\u200b Benefits: Ensures shopping carts persist across devices, Improves checkout experience with faster load times, Eliminates need for complex SQL queries 4.\u200b Caching Layer (Speeding Up Database Queries) -\u200b Key-value stores like Redis and Memcached act as a caching layer in front of a disk-based database. -\u200b Frequently accessed queries can be stored for quick retrieval, reducing database load. -\u200b Benefits: Reduces load on primary database, Improves application response times, Supports scalability Redis DB (Remote Directory Server) -\u200b An open-source, in-memory database known for high speed and low latency. -\u200b Primarily a Key-Value (KV) Store, but supports multiple data models beyond key-value pairs. -\u200b Frequently used for caching, real-time analytics, and session management. Model Description Use Cases Key-Value Simple string-based KV store Caching, session storage Lists Ordered collections of strings Message queues, logs Sets Unordered collections with unique elements Tagging, leaderboards Sorted Sets Sets with a score for sorting Ranking systems, recommendation engines Hashes Key-value pairs inside a single key Storing user profiles, objects Bitmaps Efficiently store bits User activity tracking HyperLogLogs Approximate cardinality estimation Unique visitor counting Streams Time-ordered logs of events Event sourcing, real-time analytics Geospatial Store and query location-based data Location services, geofencing JSON Native JSON document store NoSQL-like document storage Vectors Store embeddings for similarity search AI, recommendation engines"
    },
    "613": {
        "file": "Extended Notes - NoSQL Intro + KV DBs.pdf",
        "page": 10,
        "chunk": "store NoSQL-like document storage Vectors Store embeddings for similarity search AI, recommendation engines"
    },
    "614": {
        "file": "Extended Notes - NoSQL Intro + KV DBs.pdf",
        "page": 11,
        "chunk": "Redis Performance & Ranking Redis consistently ranks among the top key-value stores due to: -\u200b Ultra-low latency (sub-millisecond response times), High throughput (millions of operations per second), Support for horizontal scaling and clustering, Atomic operations for data consistency, Replication and persistence for reliability, Ranking of KV Stores (DB-Engines.com, March 2025) -\u200b Redis. Amazon DynamoDB, etcd, RocksDB, Memcached Redis -\u200b In-Memory Database \u2013 Primarily operates in RAM for ultra-fast data access. -\u200b Durability Options: 1.\u200b Snapshotting (RDB) \u2013 Saves a snapshot of the dataset to disk at specified intervals. 2.\u200b Append-Only File (AOF) \u2013 Logs every write operation to disk, allowing roll-forward recovery in case of failure. -\u200b Developed in 2009 \u2013 Written in C++, designed for speed and efficiency. -\u200b High Performance \u2013 Can handle 100,000+ SET operations per second. -\u200b Rich Command Set \u2013 Supports various operations beyond simple key-value storage. -\u200b Limitations: -\u200b No Complex Queries \u2013 Does not support SQL-like querying. -\u200b No Secondary Indexes \u2013 Data can only be accessed by its primary key. Redis Data Types Keys: -\u200b Typically strings, but can be any binary sequence (e.g., numbers, encoded objects, or raw bytes). -\u200b Keys should be short and meaningful to optimize memory usage and retrieval speed. -\u200b Naming convention best practices: -\u200b Use colons (:) as namespace separators (e.g., user:1001:profile). -\u200b Keep key lengths small to improve lookup performance. Values: Data Type Description Use Cases Strings Basic key-value storage (binary-safe, up to 512MB) Caching, counters, session storage Lists Linked lists with fast push/pop operations Message queues, task scheduling Sets Unordered collection of unique elements Tagging, leaderboards, social networks"
    },
    "615": {
        "file": "Extended Notes - NoSQL Intro + KV DBs.pdf",
        "page": 11,
        "chunk": "Message queues, task scheduling Sets Unordered collection of unique elements Tagging, leaderboards, social networks"
    },
    "616": {
        "file": "Extended Notes - NoSQL Intro + KV DBs.pdf",
        "page": 12,
        "chunk": "Sorted Sets Similar to sets, but each element has a score for sorting Ranking systems, recommendation engines Hashes Key-value pairs inside a single key User profiles, object storage Geospatial Data Stores latitude/longitude coordinates with radius queries Location-based services, geofencing Redis Data Type Breakdown 1.\u200b Strings -\u200b The most basic data type in Redis. -\u200b Supports string manipulation, bitwise operations, and numeric operations. 2.\u200b Lists (Linked Lists) -\u200b Ordered collection of string elements, allowing fast insertions/removals from both ends. -\u200b Used for queues, logs, and messaging systems. 3.\u200b Sets -\u200b Unordered collection of unique elements (no duplicates). -\u200b Useful for tagging, leaderboards, and social media followers/following. 4.\u200b Sorted Sets (ZSets) -\u200b Similar to Sets, but each element has a numeric score that determines sorting order. -\u200b Used for ranking systems, priority queues, and real-time leaderboards. 5.\u200b Hashes -\u200b Key-value pairs stored within a single key, reducing memory overhead. -\u200b Great for storing user profiles, objects, and configurations. 6.\u200b Geospatial Data -\u200b Stores latitude/longitude coordinates, enabling location-based queries. -\u200b Used in geofencing, ride-sharing apps, and proximity searches. Why Use Redis Data Types? -\u200b Optimized for speed \u2013 Most operations are O(1) or O(log N). -\u200b Memory-efficient \u2013 Compact storage for large-scale applications. -\u200b Versatile \u2013 Supports multiple models beyond just key-value storage."
    },
    "617": {
        "file": "Extended Notes - Introduction to the  Graph Data Model.pdf",
        "page": 0,
        "chunk": "What is a Graph Database? A Graph Database is a type of NoSQL database designed to store and navigate relationships between data in the form of a graph. The core structure of a graph database is based on the graph theory and consists of nodes, edges, and properties. Key Components of a Graph Database 1.\u200b Nodes -\u200b Nodes represent entities or objects in the database, such as people, places, or things. -\u200b Each node is uniquely identified by an ID and can contain various properties (key-value pairs). For example: -\u200b Person node with properties like name, age, occupation. -\u200b City node with properties like name, population, area 2.\u200b Edges -\u200b Edges represent relationships between nodes. -\u200b Just like nodes, edges are also uniquely identified and can contain properties. -\u200b Directed edges indicate the direction of the relationship (e.g., \"is friends with\" or \"works at\"). -\u200b Undirected edges can represent bi-directional relationships (e.g., \"related to\" or \"connected to\"). 3.\u200b Properties -\u200b Both nodes and edges can have properties that provide additional information about the entity or relationship. -\u200b For instance, a \"friendship\" edge between two people might have a property like \"since\", indicating when the friendship began. Graph Databases & Their Querying Model Graph databases support queries that operate directly on the relationships between nodes rather than relying on joins, which are common in relational databases. The graph structure enables operations like: 1.\u200b Traversals -\u200b Traversal is the process of visiting nodes and edges in the graph, starting from a particular node and exploring its neighbors. -\u200b Traversals allow you to find connections between entities by following edges. -\u200b Example: Find all friends of a person (Alice) by following the \"is friends with\" edges from Alice\u2019s node. 2.\u200b Shortest Path -\u200b Shortest path queries identify the smallest number of edges between"
    },
    "618": {
        "file": "Extended Notes - Introduction to the  Graph Data Model.pdf",
        "page": 0,
        "chunk": "node and exploring its neighbors. -\u200b Traversals allow you to find connections between entities by following edges. -\u200b Example: Find all friends of a person (Alice) by following the \"is friends with\" edges from Alice\u2019s node. 2.\u200b Shortest Path -\u200b Shortest path queries identify the smallest number of edges between two nodes. -\u200b This is useful for things like finding the shortest route in a map or recommending connections in a social network. -\u200b Example: Find the shortest path from Alice to Bob through a mutual friend. 3.\u200b Pattern Matching"
    },
    "619": {
        "file": "Extended Notes - Introduction to the  Graph Data Model.pdf",
        "page": 1,
        "chunk": "-\u200b Query language in graph databases (e.g., Cypher in Neo4j) supports pattern matching, where you can define a pattern of nodes and relationships to search for. -\u200b Example: Find all people who are connected to a particular person through a series of relationships. 4.\u200b Graph Algorithms -\u200b Many graph databases come with built-in graph algorithms that help analyze relationships between nodes, such as: -\u200b Centrality: Which nodes are most important or central in the graph. -\u200b PageRank: Similar to Google's algorithm for ranking web pages based on link structure. -\u200b Community detection: Identifying groups of tightly connected nodes. Advantages of Graph Databases -\u200b Efficient Relationship Handling: Graph databases excel at handling complex relationships between entities, which can be cumbersome for relational databases. -\u200b Flexible Data Model: The graph model is highly flexible, allowing you to easily evolve the structure without needing major schema changes. -\u200b Performance: Graph databases are optimized for complex relationship queries and can outperform relational databases when querying highly connected data. -\u200b Intuitive Representation: The graph structure provides a more natural representation of relationships, making it easier to model real-world problems like social networks, recommendation systems, fraud detection, etc. Use Cases for Graph Databases 1.\u200b Social Networks -\u200b Represent relationships between users, such as friends, followers, or connections -\u200b Query for things like mutual friends, friend recommendations, or the shortest path between two users. 2.\u200b Recommendation Engines -\u200b In e-commerce, graph databases can model customer-product interactions, and make personalized recommendations based on user behavior or preferences. 3.\u200b Fraud Detection -\u200b Detect fraud by analyzing suspicious patterns in transactions and relationships between different entities (e.g., customers, accounts, merchants). 4.\u200b Network Topology -\u200b Model and analyze networks such as computer networks, supply chains, or transportation systems, where connections and routes are vital. Graph Databases vs. Relational Databases -\u200b Relational"
    },
    "620": {
        "file": "Extended Notes - Introduction to the  Graph Data Model.pdf",
        "page": 1,
        "chunk": "Fraud Detection -\u200b Detect fraud by analyzing suspicious patterns in transactions and relationships between different entities (e.g., customers, accounts, merchants). 4.\u200b Network Topology -\u200b Model and analyze networks such as computer networks, supply chains, or transportation systems, where connections and routes are vital. Graph Databases vs. Relational Databases -\u200b Relational Databases (RDBMS): Data is stored in tables with rows and columns. Operations like joins are used to connect related data, but as the complexity of relationships increases, these queries can become slow and difficult to manage."
    },
    "621": {
        "file": "Extended Notes - Introduction to the  Graph Data Model.pdf",
        "page": 2,
        "chunk": "-\u200b Graph Databases: Instead of using joins, graph databases store relationships as first-class citizens, making relationship-based queries much more efficient. Complex relationships and traversals become far simpler. Popular Graph Databases 1.\u200b Neo4j \u2013 One of the most popular graph databases. Uses the Cypher query language and is widely used in social networks, recommendation engines, and fraud detection. 2.\u200b Amazon Neptune \u2013 A fully managed graph database by AWS, supports both Property Graph and RDF graph models. 3.\u200b ArangoDB \u2013 A multi-model database that supports document, key-value, and graph data models. 4.\u200b OrientDB \u2013 A multi-model graph database with support for both graph and document models. Where do Graphs Show up? Graphs are a versatile data structure that naturally models relationships and connections in many real-world systems. They appear in a wide variety of domains where relationships or interactions between entities need to be represented. Below are some key areas where graphs are frequently used: 1.\u200b Social Networks -\u200b Social Networks like Facebook, Instagram, LinkedIn, and Twitter are classic examples of graph databases in action. -\u200b Nodes: Represent individuals or accounts. -\u200b Edges: Represent relationships or interactions such as friendships, follows, likes, or comments. -\u200b Graphs help answer questions like: -\u200b Who are my mutual friends with? -\u200b What is the shortest path between two users? -\u200b How can we recommend new connections? -\u200b Beyond just online social networks, social interactions are also analyzed in fields like psychology and sociology. In these contexts, graphs are used to model: -\u200b Group dynamics: How individuals interact within a group. -\u200b Influence propagation: How information or behaviors spread within communities. -\u200b Social structure analysis: Identifying key individuals (central nodes) in a social system. 2.\u200b The Web -\u200b The World Wide Web is essentially a massive graph, with web pages as nodes and hyperlinks as"
    },
    "622": {
        "file": "Extended Notes - Introduction to the  Graph Data Model.pdf",
        "page": 2,
        "chunk": "How individuals interact within a group. -\u200b Influence propagation: How information or behaviors spread within communities. -\u200b Social structure analysis: Identifying key individuals (central nodes) in a social system. 2.\u200b The Web -\u200b The World Wide Web is essentially a massive graph, with web pages as nodes and hyperlinks as edges. -\u200b Nodes: Web pages, blogs, documents, or other content. -\u200b Edges: Hyperlinks connecting one page to another, creating a network of linked information."
    },
    "623": {
        "file": "Extended Notes - Introduction to the  Graph Data Model.pdf",
        "page": 3,
        "chunk": "-\u200b Applications: -\u200b PageRank: Google's algorithm uses graph theory to rank web pages based on the links between them, determining how important or relevant a page is in relation to a search query. -\u200b Link Analysis: Graphs can help discover relationships between web pages and identify patterns in how they are linked. 3.\u200b Chemical and Biological Data -\u200b Graphs are also crucial in fields like chemistry, biochemistry, systems biology, and genetics, where the interactions between various entities can be represented in graph form: 1.\u200b Chemical Interactions -\u200b Molecules can be represented as nodes, and the chemical bonds between atoms (or between molecules) can be represented as edges. -\u200b Applications: Molecular structure analysis: Identifying key structural features, analyzing reaction pathways, and drug design. -\u200b Chemical reaction networks: Understanding how different chemicals interact and react with one another in a reaction pathway. 2.\u200b Biological Networks -\u200b Genes, proteins, and metabolites can be represented as nodes, with edges representing interactions like protein-protein interactions, gene regulatory networks, or metabolic pathways. -\u200b Applications: Gene networks: Mapping gene relationships and pathways to understand gene expression regulation. -\u200b Protein interaction networks: Studying how proteins work together to perform cellular functions. -\u200b Disease modeling: Identifying critical proteins or genes that may be related to diseases, or predicting disease outcomes. -\u200b Genomics: Graphs are used to represent and analyze relationships in genomic data, such as the interactions between different genes or gene networks that influence biological processes. Graph databases are often used to model phylogenetic trees and the evolutionary relationships between species. 4.\u200b Logistics and Supply Chains -\u200b Graphs are ideal for modeling logistics, transportation, and supply chains: -\u200b Nodes: Represent locations, warehouses, or transport hubs. -\u200b Edges: Represent routes, shipments, or paths between locations. -\u200b Applications: Optimizing routes: Finding the shortest or fastest paths for shipments or deliveries."
    },
    "624": {
        "file": "Extended Notes - Introduction to the  Graph Data Model.pdf",
        "page": 3,
        "chunk": "relationships between species. 4.\u200b Logistics and Supply Chains -\u200b Graphs are ideal for modeling logistics, transportation, and supply chains: -\u200b Nodes: Represent locations, warehouses, or transport hubs. -\u200b Edges: Represent routes, shipments, or paths between locations. -\u200b Applications: Optimizing routes: Finding the shortest or fastest paths for shipments or deliveries. -\u200b Supply chain analysis: Identifying key suppliers, customers, or bottlenecks in the supply chain."
    },
    "625": {
        "file": "Extended Notes - Introduction to the  Graph Data Model.pdf",
        "page": 4,
        "chunk": "5.\u200b Fraud Detection -\u200b In financial services or e-commerce, fraud detection systems often rely on graphs to analyze relationships between accounts, transactions, or devices: -\u200b Nodes: Represent accounts, customers, or devices. -\u200b Edges: Represent transactions, connections, or interactions. -\u200b Graphs are useful for: Identifying fraudulent connections or suspicious activity patterns. -\u200b Anomaly detection: Spotting unusual behaviors that could indicate fraud, such as a user creating multiple accounts or making fraudulent transactions. 6.\u200b Telecommunications -\u200b Telecommunications networks can be represented as graphs, with: -\u200b Nodes: Representing phones, routers, or base stations. -\u200b Edges: Representing calls, data connections, or routes in a network. -\u200b Applications: -\u200b Optimizing the layout of communication infrastructure. -\u200b Detecting faults or inefficiencies in the network. -\u200b Predicting how communication flows and interactions occur. 7.\u200b Knowledge Graphs -\u200b Knowledge Graphs use graphs to represent the relationships between entities in a domain (such as people, places, events, and concepts) and are commonly used in search engines, recommendation systems, and AI. -\u200b Nodes: Represent entities (e.g., books, movies, authors, topics). -\u200b Edges: Represent relationships (e.g., \"written by\", \"related to\", \"directed by\"). -\u200b Applications: -\u200b Search Engines: Enhancing search results by showing related entities and their relationships. -\u200b Recommendation Systems: Recommending items based on connections between entities (e.g., books related to a specific author or genre). 8.\u200b Network Security -\u200b Network security uses graphs to model relationships between users, devices, and systems within a network. -\u200b Nodes: Represent devices, users, or systems. -\u200b Edges: Represent the flow of data or access permissions. -\u200b Applications: Identifying vulnerabilities or security breaches in the network. -\u200b Analyzing how attackers might move across a network (e.g., lateral movement in a compromised system)."
    },
    "626": {
        "file": "Extended Notes - Introduction to the  Graph Data Model.pdf",
        "page": 4,
        "chunk": "permissions. -\u200b Applications: Identifying vulnerabilities or security breaches in the network. -\u200b Analyzing how attackers might move across a network (e.g., lateral movement in a compromised system)."
    },
    "627": {
        "file": "Extended Notes - Introduction to the  Graph Data Model.pdf",
        "page": 5,
        "chunk": "What is a graph? A graph is a data structure that represents relationships between entities. It is composed of nodes (also called vertices) and edges (also called relationships). These elements form the core structure of a graph, allowing for the representation of connected data and enabling the modeling of complex relationships between entities. Labeled Property Graph - Breakdown of the components: A Labeled Property Graph is a more specific and advanced type of graph structure, where nodes and edges can have both labels and properties. It is one of the most commonly used graph representations, especially in graph databases like Neo4j. 1.\u200b Nodes (Vertices) -\u200b Definition: A node represents an entity or object in the graph. Each node can represent anything, such as a person, place, product, event, etc. -\u200b Labels: A label is a way to categorize or group nodes. Labels allow nodes to be categorized by types or categories, such as Person, Product, or City. -\u200b Properties: Nodes can have attributes that describe them. These attributes are stored as key-value pairs (think of them like columns in a database). For example, a Person node might have properties like name, age, and city. -\u200b Example: Node: A Person node with properties like {name: \"Alice\", age: 30, city: \"New York\"}; Label: Person; Properties: {name: \"Alice\", age: 30, city: \"New York\"} 2.\u200b Edges (Relationships) -\u200b Definition: An edge represents a relationship between two nodes. It connects two nodes, establishing a link or connection between them. -\u200b Labels: Relationships can also have labels that describe the type of relationship between the nodes, such as \"FRIEND_OF\", \"WORKS_AT\", or \"LIKES\". -\u200b Properties: Just like nodes, edges can also have properties. These properties describe the relationship, and they are stored as key-value pairs. For example, a relationship between two people may have a property like"
    },
    "628": {
        "file": "Extended Notes - Introduction to the  Graph Data Model.pdf",
        "page": 5,
        "chunk": "labels that describe the type of relationship between the nodes, such as \"FRIEND_OF\", \"WORKS_AT\", or \"LIKES\". -\u200b Properties: Just like nodes, edges can also have properties. These properties describe the relationship, and they are stored as key-value pairs. For example, a relationship between two people may have a property like since to represent when the relationship started. -\u200b Example: Edge: A \"FRIEND_OF\" relationship between Alice and Bob, with properties like {since: 2015}. Label: FRIEND_OF Properties: {since: 2015} 3.\u200b Labels -\u200b Labels are used to group nodes into categories or types. They allow us to organize and categorize nodes in the graph. -\u200b Nodes: A node can have one or more labels (e.g., Person, City, Product). -\u200b Example: A person node might have the label Person, and a product node might have the label Product. -\u200b Note: Labels are not exclusive to a node, meaning a node could be labeled as Person and Employee simultaneously if it fits multiple categories. 4.\u200b Properties -\u200b Definition: Properties are key-value pairs that can be attached to both nodes and edges. They are used to store additional information about the node or the relationship."
    },
    "629": {
        "file": "Extended Notes - Introduction to the  Graph Data Model.pdf",
        "page": 6,
        "chunk": "-\u200b Node Properties: These define characteristics of nodes (e.g., age, city, name for a Person node). -\u200b Edge Properties: These describe characteristics of the relationship (e.g., since, strength for a friendship edge). -\u200b Example: Node: {name: \"Alice\", age: 30, city: \"New York\"} (Person node). Relationship: [:FRIEND_OF {since: 2015}] (Friendship edge). 5.\u200b Key Properties -\u200b Nodes without relationships: It is perfectly valid for a node to not have any relationships. This can happen in scenarios where a node exists independently, such as a person who has no current friendships or a product that hasn't been sold yet. -\u200b Edges not connected to nodes: This is not allowed. Edges must always connect to nodes, as they are used to represent relationships. Having edges that don\u2019t connect to nodes doesn\u2019t make sense in a graph structure and would violate the graph model. Key Characteristics of a Labeled Property Graph 1.\u200b Flexible Schema: Unlike relational databases, labeled property graphs allow for flexible schemas where each node and relationship can have different properties. This allows the graph to evolve over time without rigid schema constraints. 2.\u200b Intuitive Representation: Labeled property graphs are great for modeling complex and interconnected data, like social networks, recommendation systems, and fraud detection, where entities and their relationships are key to understanding the data. 3.\u200b Efficient Traversal: With the use of nodes and edges, graph databases are optimized for traversing relationships, making operations like finding the shortest path, recommendations, and influence propagation very efficient. Paths A path in a graph is a sequence of nodes connected by edges, where the nodes and edges in the path are visited only once. Paths are a fundamental concept in graph theory, and they are used to represent a variety of relationships and traversals within the graph. Key Characteristics of a Path 1.\u200b Ordered Sequence:"
    },
    "630": {
        "file": "Extended Notes - Introduction to the  Graph Data Model.pdf",
        "page": 6,
        "chunk": "a sequence of nodes connected by edges, where the nodes and edges in the path are visited only once. Paths are a fundamental concept in graph theory, and they are used to represent a variety of relationships and traversals within the graph. Key Characteristics of a Path 1.\u200b Ordered Sequence: The sequence of nodes in a path is ordered, meaning the order in which nodes appear is important. A path represents a specific traversal from one node to another. 2.\u200b Nodes and Edges: A path is formed by connecting nodes with edges. The edges are the links between the nodes, and they dictate the traversal of the graph. 3.\u200b No Repeated Nodes or Edges: A simple path ensures that no node or edge is revisited, which means each node and edge can only appear once in the path. This helps avoid loops or cycles in the traversal. -\u200b Simple Path: A path that doesn\u2019t revisit any node or edge. -\u200b Non-simple Path: If nodes or edges are repeated, it\u2019s not considered a simple path."
    },
    "631": {
        "file": "Extended Notes - Introduction to the  Graph Data Model.pdf",
        "page": 7,
        "chunk": "4.\u200b Directed vs. Undirected: In a directed graph, edges have a direction, so the path must follow the direction of the edges. In an undirected graph, the path can go in either direction along the edges. Types of Paths 1.\u200b Simple Path: As mentioned, a simple path doesn\u2019t revisit any nodes or edges. For example: Path: A \u2192 B \u2192 C \u2192 D -\u200b Here, each node is visited only once, and no edge is repeated. 2.\u200b Cycle: A cycle is a special kind of path where the start and end node are the same, and the path forms a loop. In a cycle, nodes (and sometimes edges) can be revisited. -\u200b Example: A \u2192 B \u2192 C \u2192 A (this is a cycle, since it starts and ends at A). 3.\u200b Shortest Path: The shortest path is the path that has the fewest edges (or minimum distance in weighted graphs). It\u2019s often a key concept in routing algorithms. -\u200b Example: Finding the quickest route between two cities. 4.\u200b Longest Path: Conversely, the longest path is the path that takes the greatest number of edges. This can be useful in certain algorithms, especially when measuring network latency or other factors. Applications of Paths in Graphs 1.\u200b Finding Shortest Paths: Algorithms like Dijkstra's or Bellman-Ford are used to find the shortest path between two nodes in a weighted graph. Paths can be used in network routing, GPS systems, and social network analysis. 2.\u200b Traversal: In graph traversal algorithms like Depth-First Search (DFS) or Breadth-First Search (BFS), paths are followed to explore the graph. DFS explores as far as possible along a branch, while BFS explores all neighbors at the present depth level before moving on. 3.\u200b Social Network Analysis: In social networks, paths can represent relationships between users. For example, a"
    },
    "632": {
        "file": "Extended Notes - Introduction to the  Graph Data Model.pdf",
        "page": 7,
        "chunk": "(DFS) or Breadth-First Search (BFS), paths are followed to explore the graph. DFS explores as far as possible along a branch, while BFS explores all neighbors at the present depth level before moving on. 3.\u200b Social Network Analysis: In social networks, paths can represent relationships between users. For example, a path could represent the \"connection\" between two individuals, and traversing the graph could help identify friends of friends or shortest paths between users. 4.\u200b Web Crawling: In the context of the web (as a graph of pages), paths can represent how one page links to another. A web crawler follows paths (hyperlinks) from one page to another in order to index the content of the web. 5.\u200b Recommendation Systems: In recommendation systems, paths can represent connections between users and products. For example, if user A liked product X, and user B liked product X, then the system may suggest product Y to user B if user A has also liked product Y. Path Length The length of a path is measured by the number of edges traversed. In the example above, the path from A to D has a length of 3, as it involves 3 edges."
    },
    "633": {
        "file": "Extended Notes - Introduction to the  Graph Data Model.pdf",
        "page": 8,
        "chunk": "Flavors of Graph Graphs can have various characteristics depending on how nodes and edges are defined and how they interact with each other. The following are common flavors of graphs, which differ based on key attributes like connectivity, edge weights, direction, and cycles. 1.\u200b Connected vs. Disconnected Graphs -\u200b Connected Graph: A graph is connected if there is a path between every pair of nodes. In other words, you can reach any node from any other node in the graph by traversing through edges. -\u200b Example: A social network graph where everyone is connected either directly or indirectly through friends. -\u200b Disconnected Graph: A graph is disconnected if there is at least one pair of nodes in the graph that is not connected by any path. A disconnected graph can be thought of as consisting of two or more isolated subgraphs. -\u200b Example: A graph representing different cities where some cities are completely disconnected from others. 2.\u200b Weighted vs. Unweighted Graphs -\u200b Weighted Graph: A graph is weighted if each edge has an associated weight or cost. This weight could represent various things like distance, time, cost, or any other quantity that can be measured. Weighted graphs are essential in algorithms that need to account for the cost of traversing between nodes, such as shortest path algorithms (e.g., Dijkstra's algorithm). -\u200b Example: A road network graph where each edge (road) has a weight representing the travel time or distance between two locations. -\u200b Unweighted Graph: A graph is unweighted if the edges do not carry any weight. All edges are treated equally in terms of cost or distance. Unweighted graphs are simpler to process and are often used when the concern is just whether a path exists between two nodes, not the cost of traveling that path. -\u200b Example: A"
    },
    "634": {
        "file": "Extended Notes - Introduction to the  Graph Data Model.pdf",
        "page": 8,
        "chunk": "the edges do not carry any weight. All edges are treated equally in terms of cost or distance. Unweighted graphs are simpler to process and are often used when the concern is just whether a path exists between two nodes, not the cost of traveling that path. -\u200b Example: A social network graph where connections between users (edges) are not weighted by any specific metric. 3.\u200b Directed vs. Undirected Graphs -\u200b Directed Graph (Digraph): A graph is directed if the edges have a direction, meaning each edge goes from a start node to an end node. Directed graphs are used when the relationship between nodes is asymmetric, i.e., the relationship flows in one direction. -\u200b Example: A Twitter-following graph, where a directed edge indicates that one user follows another but not necessarily the reverse. -\u200b Undirected Graph: A graph is undirected if the edges have no direction. The edges are bidirectional, meaning if there is an edge between node A and node B, you can traverse it in both directions. -\u200b Example: A Facebook friend network, where if A is friends with B, then B is also friends with A."
    },
    "635": {
        "file": "Extended Notes - Introduction to the  Graph Data Model.pdf",
        "page": 9,
        "chunk": "4.\u200b Acyclic vs. Cyclic Graphs -\u200b Acyclic Graph: A graph is acyclic if it contains no cycles. A cycle is a path in the graph where the start and end nodes are the same, and the path does not repeat any nodes or edges. Acyclic graphs are crucial for representing hierarchical or tree-like structures where there is no looping or circular reference. -\u200b Example: A Tree is an acyclic graph because it has no cycles. Another example is a Task Dependency Graph, where tasks must be completed in a specific order and cannot loop back on themselves. -\u200b Cyclic Graph: A graph is cyclic if there is at least one cycle in the graph. Cyclic graphs can model situations where entities are interconnected in a way that creates loops. Cyclic graphs are useful for representing systems where feedback loops or circular dependencies exist. -\u200b Example: A web page link structure, where one page links to another, which links back to the first page, forming a cycle. Real-World Applications of Graph Flavors 1.\u200b Connected Graphs: Used in social networking applications, where every user should be able to eventually connect to every other user (directly or indirectly). 2.\u200b Weighted Graphs: Essential for applications like navigation systems or telecommunication networks, where paths have different costs, and finding the most efficient path is important. 3.\u200b Directed Graphs: Useful in systems like recommendation engines, email systems, or web crawling, where relationships have clear directional flow (e.g., one entity influences another). 4.\u200b Acyclic Graphs: Found in project planning (where tasks must be completed in a specific order), and file system structures (directories and subdirectories). 5.\u200b Cyclic Graphs: Present in systems involving feedback loops, resource allocation, and recurrent processes like neural networks or certain types of dynamic systems. Types of Graph Algorithms - Pathfinding Pathfinding algorithms are"
    },
    "636": {
        "file": "Extended Notes - Introduction to the  Graph Data Model.pdf",
        "page": 9,
        "chunk": "project planning (where tasks must be completed in a specific order), and file system structures (directories and subdirectories). 5.\u200b Cyclic Graphs: Present in systems involving feedback loops, resource allocation, and recurrent processes like neural networks or certain types of dynamic systems. Types of Graph Algorithms - Pathfinding Pathfinding algorithms are fundamental to working with graphs, especially when you need to find the shortest path between nodes, detect cycles, or optimize flow through networks. Let's break down the key concepts of pathfinding and explore the various types of pathfinding algorithms: 1.\u200b Shortest Path Algorithms -\u200b The shortest path problem involves finding the path between two nodes that minimizes a specific criterion (typically the number of edges or the total weight of edges). This is one of the most common graph operations used in various applications such as navigation, routing, and network analysis. -\u200b Shortest Path: This can refer to either: -\u200b Fewest edges: The path with the minimum number of hops (unweighted graph)."
    },
    "637": {
        "file": "Extended Notes - Introduction to the  Graph Data Model.pdf",
        "page": 10,
        "chunk": "-\u200b Lowest weight: The path where the sum of the edge weights is minimized (weighted graph). -\u200b Common Shortest Path Algorithms: -\u200b Dijkstra\u2019s Algorithm: Dijkstra\u2019s algorithm finds the shortest path from a source node to all other nodes in a weighted graph. It works by maintaining a set of unvisited nodes and progressively expanding the shortest paths from the source. -\u200b Use case: Efficient for finding the shortest path in a weighted, directed graph where edges have non-negative weights (e.g., GPS navigation systems). -\u200b Bellman-Ford Algorithm: This algorithm is similar to Dijkstra\u2019s but can handle graphs with negative edge weights and can also detect negative weight cycles. -\u200b Use case: Used in applications like currency exchange or financial systems, where negative weights might be involved. -\u200b A (A-star) Algorithm*: A* is an informed search algorithm that combines aspects of Dijkstra\u2019s algorithm with heuristics to find the shortest path more efficiently. It uses an estimate of the remaining cost (heuristic) to prioritize which node to explore next. -\u200b Use case: Commonly used in gaming, AI for pathfinding, and GPS navigation systems. -\u200b Floyd-Warshall Algorithm: This is an all-pairs shortest path algorithm, meaning it finds the shortest paths between all pairs of nodes in a graph. It\u2019s particularly useful when you need to know the shortest path between any two nodes in a graph. -\u200b Use case: Used in network optimization problems, where you need to compute paths between all nodes. 2.\u200b Average Shortest Path (Efficiency and Resiliency) -\u200b The Average Shortest Path is a metric used to monitor the efficiency and resiliency of networks. It is the average of the shortest paths between all pairs of nodes in a graph. -\u200b Resiliency: A network with a lower average shortest path suggests that there is better connectivity and fewer isolated nodes or"
    },
    "638": {
        "file": "Extended Notes - Introduction to the  Graph Data Model.pdf",
        "page": 10,
        "chunk": "Path is a metric used to monitor the efficiency and resiliency of networks. It is the average of the shortest paths between all pairs of nodes in a graph. -\u200b Resiliency: A network with a lower average shortest path suggests that there is better connectivity and fewer isolated nodes or subgraphs, making it more resilient to failures. -\u200b Efficiency: A low average shortest path indicates that it\u2019s easy to get from one point to another in the network, implying efficient communication or transportation. -\u200b Use case: This metric is helpful in network optimization, where improving the average shortest path can lead to better performance in systems like communication networks, transportation systems, or social networks."
    },
    "639": {
        "file": "Extended Notes - Introduction to the  Graph Data Model.pdf",
        "page": 11,
        "chunk": "3.\u200b Minimum Spanning Tree (MST) -\u200b A Minimum Spanning Tree (MST) is a spanning tree of a weighted graph where the total edge weight is minimized. It connects all nodes in the graph with the minimum total weight and without any cycles. MST is useful for designing cost-efficient networks. -\u200b Common MST Algorithms: -\u200b Kruskal\u2019s Algorithm: Kruskal\u2019s algorithm works by sorting all edges in the graph by weight and adding edges to the tree in increasing order of weight, ensuring no cycles are formed. -\u200b Use case: Building minimum cost networks like telephone lines, electrical grids, or road networks. -\u200b Prim\u2019s Algorithm: Prim\u2019s algorithm starts with a single node and grows the MST by adding the least expensive edge connecting a node in the tree to a node outside it, until all nodes are included. -\u200b Use case: Used in network design, where it\u2019s important to connect all nodes at the minimum cost. 4.\u200b Cycle Detection -\u200b Cycle Detection is an important pathfinding task to identify whether a graph contains any cycles. Cycles can be problematic, especially in systems like task scheduling, where cycles represent circular dependencies that can prevent completion. -\u200b Cycle Detection Algorithms: -\u200b Depth-First Search (DFS): DFS can be used to detect cycles by keeping track of the nodes that are currently in the recursion stack. If you encounter a node that\u2019s already in the recursion stack, a cycle has been detected. -\u200b Use case: Important in dependency resolution (e.g., package managers, task schedulers), or network protocols to avoid infinite loops. -\u200b Union-Find (Disjoint Set Union): This algorithm can detect cycles in an undirected graph. It works by keeping track of the connected components and checking if adding an edge creates a cycle by connecting two previously disconnected components. -\u200b Use case: Efficient for detecting cycles in"
    },
    "640": {
        "file": "Extended Notes - Introduction to the  Graph Data Model.pdf",
        "page": 11,
        "chunk": "to avoid infinite loops. -\u200b Union-Find (Disjoint Set Union): This algorithm can detect cycles in an undirected graph. It works by keeping track of the connected components and checking if adding an edge creates a cycle by connecting two previously disconnected components. -\u200b Use case: Efficient for detecting cycles in network connectivity problems. 5.\u200b Maximum/Minimum Flow Algorithms -\u200b Flow algorithms are used to determine the maximum flow of material, data, or other resources through a network. These algorithms are commonly used in network design and transportation to model how resources can be efficiently routed through a system. -\u200b Common Flow Algorithms:"
    },
    "641": {
        "file": "Extended Notes - Introduction to the  Graph Data Model.pdf",
        "page": 12,
        "chunk": "-\u200b Ford-Fulkerson Algorithm: This algorithm finds the maximum flow in a flow network by repeatedly augmenting the flow along paths from the source to the sink until no more augmenting paths can be found. -\u200b Use case: Used in traffic flow optimization, network throughput maximization, and supply chain management. -\u200b Edmonds-Karp Algorithm: A specific implementation of Ford-Fulkerson that uses Breadth-First Search (BFS) to find augmenting paths. It guarantees polynomial time complexity and is used in scenarios that require maximum flow calculations. -\u200b Use case: Common in data packet routing and resource allocation. Other Pathfinding Variations -\u200b Maximal Paths: Algorithms that identify the longest paths or paths with the largest capacity, often used in optimization problems. -\u200b All-Pairs Shortest Path: Algorithms like Floyd-Warshall that calculate the shortest path between every pair of nodes in a graph. Types of Graph Algorithms - Centrality & Community Detection Graph algorithms related to centrality and community detection focus on understanding the structure and behavior of nodes within a graph. These algorithms help in identifying important nodes (centrality) and detecting groups of nodes that are strongly connected (community detection). These concepts are essential for applications like social networks, biological networks, transportation systems, and more. 1.\u200b Centrality Algorithms: Centrality measures how important or influential a node is within a graph. Nodes with higher centrality have more influence, connections, or \"importance\" relative to other nodes. Centrality measures are widely used in social networks, recommendation systems, and even in understanding traffic flow or information propagation. -\u200b Degree Centrality: The degree centrality of a node is simply the number of edges connected to it. In undirected graphs, it is the number of neighbors (connections), while in directed graphs, it can be divided into in-degree (number of incoming edges) and out-degree (number of outgoing edges). -\u200b Example: In a social network,"
    },
    "642": {
        "file": "Extended Notes - Introduction to the  Graph Data Model.pdf",
        "page": 12,
        "chunk": "degree centrality of a node is simply the number of edges connected to it. In undirected graphs, it is the number of neighbors (connections), while in directed graphs, it can be divided into in-degree (number of incoming edges) and out-degree (number of outgoing edges). -\u200b Example: In a social network, the person with the highest degree centrality might be the most connected (i.e., having the most friends or followers). -\u200b Closeness Centrality: Closeness centrality measures how close a node is to all other nodes in the graph. It is the reciprocal of the average shortest path distance from a node to all other nodes. Nodes with high closeness centrality are typically able to spread information quickly throughout the network. -\u200b Example: In a communication network, the node with the highest closeness centrality would be able to relay information to all other nodes faster."
    },
    "643": {
        "file": "Extended Notes - Introduction to the  Graph Data Model.pdf",
        "page": 13,
        "chunk": "-\u200b Betweenness Centrality: Betweenness centrality measures the extent to which a node lies on the shortest path between other nodes. Nodes with high betweenness centrality are considered brokers or gatekeepers in the network because they control information flow between other nodes. -\u200b Example: In a social network, a user with high betweenness centrality might play the role of a connector between different groups of people. -\u200b Eigenvector Centrality: Eigenvector centrality considers not just the number of connections a node has (like degree centrality) but also the quality of those connections. A node is considered important if it is connected to other important nodes. This measure is recursive and can be calculated using the power iteration method. -\u200b Example: In a recommendation system, a highly influential user might have connections to other influential users, which makes them more central in the network. -\u200b Use Cases for Centrality Algorithms: -\u200b Influencers in Social Networks: Identifying influential people (e.g., celebrities or thought leaders) who can affect trends and decisions. -\u200b Critical Infrastructure: Finding critical nodes in transportation, electrical grids, or communication networks, where failure could have widespread impacts. -\u200b Marketing and Ads: Determining which users to target in advertising campaigns for maximum reach and engagement. 2.\u200b Community Detection Algorithms: Community detection refers to the process of grouping or partitioning a graph's nodes into clusters (or communities), where nodes within the same cluster are more densely connected to each other than to nodes outside the cluster. The goal is to identify meaningful subgroups or communities within a graph. These communities can represent real-world groupings, like social groups, biological pathways, or clusters of related entities. -\u200b Modularity Maximization (Louvain Algorithm): Modularity is a measure that quantifies the strength of division of a network into communities. The Louvain method is an efficient algorithm that maximizes modularity"
    },
    "644": {
        "file": "Extended Notes - Introduction to the  Graph Data Model.pdf",
        "page": 13,
        "chunk": "communities within a graph. These communities can represent real-world groupings, like social groups, biological pathways, or clusters of related entities. -\u200b Modularity Maximization (Louvain Algorithm): Modularity is a measure that quantifies the strength of division of a network into communities. The Louvain method is an efficient algorithm that maximizes modularity by iteratively optimizing community assignments. It starts by treating each node as its own community and then merges communities that lead to a higher modularity score. -\u200b Example: Detecting communities in a social network (e.g., different interest groups or subcultures). -\u200b Girvan-Newman Algorithm: The Girvan-Newman algorithm detects communities by iteratively removing edges with the highest betweenness centrality. It works by progressively removing the most \"central\" edges in the graph, causing the graph to split into smaller connected components. This approach is computationally expensive but can work well for small to medium-sized graphs. -\u200b Example: Identifying groups within organizational structures or collaborative networks."
    },
    "645": {
        "file": "Extended Notes - Introduction to the  Graph Data Model.pdf",
        "page": 14,
        "chunk": "-\u200b Spectral Clustering: Spectral clustering uses the eigenvalues of the graph's Laplacian matrix to identify the graph's community structure. By performing eigenvalue decomposition, this algorithm finds groups of nodes that have similar connectivity patterns. Spectral clustering is particularly useful for graphs that are difficult to divide using traditional partitioning methods. -\u200b Example: Detecting communities in molecular networks or groupings in recommendation systems. -\u200b Label Propagation Algorithm: The Label Propagation Algorithm (LPA) is a simple, fast, and efficient algorithm that assigns a unique label to each node and then propagates the labels based on neighbors' labels until convergence. This algorithm works well for large networks due to its efficiency. -\u200b Example: Identifying communities in large-scale social media networks or knowledge graphs. -\u200b Infomap Algorithm: The Infomap algorithm models the graph as a network of flow and uses information theory to find communities by optimizing the compression of a random walk on the graph. It has been shown to perform well on large, real-world networks. -\u200b Example: Detecting hierarchical communities in large networks like the web or scientific citation graphs. -\u200b Use Cases for Community Detection Algorithms: -\u200b Social Network Analysis: Finding groups of users who share similar interests or behaviors (e.g., \"fan clubs\" on a social media platform). -\u200b Biological Networks: Identifying functional modules or gene clusters in molecular networks or protein interaction networks. -\u200b Recommendation Systems: Grouping users or items into similar clusters to make better recommendations. -\u200b Fraud Detection: Detecting hidden groups of fraudulent activities in financial transactions or social interactions. Neo4j Neo4j is a powerful, highly popular graph database that is designed specifically to store and query graph-based data efficiently. It supports both transactional and analytical processing of graph data, making it a versatile choice for a wide range of use cases, including real-time applications and complex analytics."
    },
    "646": {
        "file": "Extended Notes - Introduction to the  Graph Data Model.pdf",
        "page": 14,
        "chunk": "social interactions. Neo4j Neo4j is a powerful, highly popular graph database that is designed specifically to store and query graph-based data efficiently. It supports both transactional and analytical processing of graph data, making it a versatile choice for a wide range of use cases, including real-time applications and complex analytics. Key Features of Neo4j 1.\u200b Graph-Based Data Model: -\u200b Nodes and Relationships: Neo4j stores data as nodes (entities), edges (relationships), and properties (attributes). This structure is highly intuitive for graph data, allowing for efficient traversal and querying. -\u200b Schema Optional: While Neo4j can work with a flexible, schema-less design, it also allows users to define a schema if desired, making it adaptable to different use cases."
    },
    "647": {
        "file": "Extended Notes - Introduction to the  Graph Data Model.pdf",
        "page": 15,
        "chunk": "2.\u200b Transactional and Analytical Processing: -\u200b Neo4j is optimized for both transactional operations (like adding, deleting, and updating nodes and relationships) and analytical operations (such as complex graph algorithms like shortest path, centrality, and community detection). -\u200b This dual focus enables it to handle use cases that require both real-time transaction processing and large-scale graph analytics. 3.\u200b ACID Compliant: -\u200b Neo4j guarantees ACID (Atomicity, Consistency, Isolation, Durability) properties for database transactions, which ensures reliability and data integrity. This is crucial for applications where data accuracy is critical, such as financial systems or social networks. 4.\u200b Indexing: -\u200b Neo4j supports various indexing mechanisms to efficiently retrieve nodes and relationships. For example, B-tree and Lucene indexes can be used for faster lookups based on node properties. -\u200b Indexes are vital for performance, especially when working with large datasets. 5.\u200b Distributed Computing: -\u200b Neo4j supports distributed computing, allowing it to scale across multiple nodes and handle large graph datasets in a distributed fashion. This enables horizontal scalability and ensures the database can handle high workloads in a fault-tolerant manner. 6.\u200b Flexible Query Language (Cypher): -\u200b Neo4j uses Cypher, a graph-specific query language, which is intuitive and designed for easy interaction with graph structures. It enables users to express complex graph queries in a human-readable form. -\u200b Cypher allows users to perform graph traversals, pattern matching, and relationship analysis effectively. Comparison to Similar Graph Databases 1.\u200b Microsoft CosmosDB: -\u200b CosmosDB is a multi-model database that supports various types of data, including graph data, using Gremlin (another graph query language). -\u200b While it offers graph capabilities, CosmosDB is more of a general-purpose database, and Neo4j focuses specifically on graph-based workloads with more advanced graph-specific features like complex algorithms and native graph processing. 2.\u200b Amazon Neptune: -\u200b Neptune is a managed graph database service provided by"
    },
    "648": {
        "file": "Extended Notes - Introduction to the  Graph Data Model.pdf",
        "page": 15,
        "chunk": "Gremlin (another graph query language). -\u200b While it offers graph capabilities, CosmosDB is more of a general-purpose database, and Neo4j focuses specifically on graph-based workloads with more advanced graph-specific features like complex algorithms and native graph processing. 2.\u200b Amazon Neptune: -\u200b Neptune is a managed graph database service provided by AWS that supports both Gremlin (for property graphs) and SPARQL (for RDF graphs). -\u200b It is similar to Neo4j in that it handles graph-based data, but Neo4j has a larger focus on native graph analytics and a dedicated query language (Cypher) designed specifically for graph data, which can give it an edge in some use cases."
    },
    "649": {
        "file": "Extended Notes - Introduction to the  Graph Data Model.pdf",
        "page": 16,
        "chunk": "Common Use Cases for Neo4j -\u200b Social Networks: Mapping and analyzing connections between users, friends, and interests. Neo4j\u2019s graph-based structure allows for fast and efficient traversal and analysis of social interactions. -\u200b Recommendation Systems: Based on user preferences, behavior, and relationships, Neo4j can recommend products, content, or services by analyzing user connections and behavior patterns. -\u200b Fraud Detection: In financial systems, Neo4j can analyze transaction data for patterns and connections that may indicate fraudulent activity. The graph model makes it easy to track relationships across multiple nodes (e.g., users, transactions). -\u200b Network and IT Operations: Neo4j is used to model complex networks (like telecom or IT networks) and to optimize network operations, monitor infrastructure, or detect anomalies in the graph of network connections. -\u200b Knowledge Graphs: Neo4j is popular in building knowledge graphs that represent relationships between concepts, entities, and facts, providing a foundation for intelligent applications like search engines and virtual assistants."
    },
    "650": {
        "file": "05 - NoSQL Intro + KV DBs.pdf",
        "page": 0,
        "chunk": "DS 4300 NoSQL & KV DBs Mark Fontenot, PhD Northeastern University Some material used with permission from Dr. Rachlin, with thanks!"
    },
    "651": {
        "file": "05 - NoSQL Intro + KV DBs.pdf",
        "page": 1,
        "chunk": "Distributed DBs and ACID - Pessimistic Concurrency \u25cfACID transactions \u25cb Focuses on \u201cdata safety\u201d \u25cb considered a pessimistic concurrency model because it assumes one transaction has to protect itself from other transactions \u25a0 IOW, it assumes that if something can go wrong, it will. \u25cb Con\ufb02icts are prevented by locking resources until a transaction is complete (there are both read and write locks) \u25cb Write Lock Analogy \u2192 borrowing a book from a library\u2026 If you have it, no one else can. 2 See https://www.freecodecamp.org/news/how-databases-guarantee-isolation for more for a deeper dive."
    },
    "652": {
        "file": "05 - NoSQL Intro + KV DBs.pdf",
        "page": 2,
        "chunk": "Optimistic Concurrency \u25cfTransactions do not obtain locks on data when they read or write \u25cfOptimistic because it assumes con\ufb02icts are unlikely to occur \u25cb Even if there is a con\ufb02ict, everything will still be OK. \u25cfBut how? \u25cb Add last update timestamp and version number columns to every table\u2026 read them when changing. THEN, check at the end of transaction to see if any other transaction has caused them to be modi\ufb01ed. 3"
    },
    "653": {
        "file": "05 - NoSQL Intro + KV DBs.pdf",
        "page": 3,
        "chunk": "Optimistic Concurrency \u25cfLow Con\ufb02ict Systems (backups, analytical dbs, etc.) \u25cb Read heavy systems \u25cb the con\ufb02icts that arise can be handled by rolling back and re-running a transaction that notices a con\ufb02ict. \u25cb So, optimistic concurrency works well - allows for higher concurrency \u25cfHigh Con\ufb02ict Systems \u25cb rolling back and rerunning transactions that encounter a con\ufb02ict \u2192 less ef\ufb01cient \u25cb So, a locking scheme (pessimistic model) might be preferable 4"
    },
    "654": {
        "file": "05 - NoSQL Intro + KV DBs.pdf",
        "page": 4,
        "chunk": "NoSQL - \u201cNoSQL\u201d \ufb01rst used in 1998 by Carlo Strozzi to describe his relational database system that did not use SQL. - More common, modern meaning is \u201cNot Only SQL\u201d - But, sometimes thought of as non-relational DBs - Idea originally developed, in part, as a response to processing unstructured web-based data. 5 https://www.dataversity.net/a-brief-history-of-non-relational-databases/"
    },
    "655": {
        "file": "05 - NoSQL Intro + KV DBs.pdf",
        "page": 5,
        "chunk": "CAP Theorem Review 6 Reference: https://alperenbayramoglu.com/posts/understanding-cap-theorem/ You can have 2, but not 3, of the following: - Consistency*: Every user of the DB has an identical view of the data at any given instant - Availability: In the event of a failure, the database system remains operational - Partition Tolerance: The database can maintain operations in the event of the network\u2019s failing between two segments of the distributed system * Note, the definition of Consistency in CAP is different from that of ACID."
    },
    "656": {
        "file": "05 - NoSQL Intro + KV DBs.pdf",
        "page": 6,
        "chunk": "CAP Theorem Review 7 Reference: https://alperenbayramoglu.com/posts/understanding-cap-theorem/ - Consistency + Availability: System always responds with the latest data and every request gets a response, but may not be able to deal with network partitions - Consistency + Partition Tolerance: If system responds with data from the distrib. system, it is always the latest, else data request is dropped. - Availability + Partition Tolerance: System always sends are responds based on distributed store, but may not be the absolute latest data."
    },
    "657": {
        "file": "05 - NoSQL Intro + KV DBs.pdf",
        "page": 7,
        "chunk": "ACID Alternative for Distrib Systems - BASE \u25cfBasically Available \u25cbGuarantees the availability of the data (per CAP), but response can be \u201cfailure\u201d/\u201cunreliable\u201d because the data is in an inconsistent or changing state \u25cbSystem appears to work most of the time 8"
    },
    "658": {
        "file": "05 - NoSQL Intro + KV DBs.pdf",
        "page": 8,
        "chunk": "ACID Alternative for Distrib Systems - BASE \u25cfSoft State - The state of the system could change over time, even w/o input. Changes could be result of eventual consistency. \u25cbData stores don\u2019t have to be write-consistent \u25cbReplicas don\u2019t have to be mutually consistent 9"
    },
    "659": {
        "file": "05 - NoSQL Intro + KV DBs.pdf",
        "page": 9,
        "chunk": "ACID Alternative for Distrib Systems - BASE \u25cfEventual Consistency - The system will eventually become consistent \u25cbAll writes will eventually stop so all nodes/replicas can be updated 10"
    },
    "660": {
        "file": "05 - NoSQL Intro + KV DBs.pdf",
        "page": 10,
        "chunk": "Categories of NoSQL DBs - Review 11"
    },
    "661": {
        "file": "05 - NoSQL Intro + KV DBs.pdf",
        "page": 11,
        "chunk": "First Up \u2192 Key-Value Databases 12"
    },
    "662": {
        "file": "05 - NoSQL Intro + KV DBs.pdf",
        "page": 12,
        "chunk": "Key Value Stores key = value - Key-value stores are designed around: - simplicity - the data model is extremely simple - comparatively, tables in a RDBMS are very complex. - lends itself to simple CRUD ops and API creation 13"
    },
    "663": {
        "file": "05 - NoSQL Intro + KV DBs.pdf",
        "page": 13,
        "chunk": "Key Value Stores key = value - Key-value stores are designed around: - speed - usually deployed as in-memory DB - retrieving a value given its key is typically a O(1) op b/c hash tables or similar data structs used under the hood - no concept of complex queries or joins\u2026 they slow things down 14"
    },
    "664": {
        "file": "05 - NoSQL Intro + KV DBs.pdf",
        "page": 14,
        "chunk": "Key Value Stores key = value - Key-value stores are designed around: - scalability - Horizontal Scaling is simple - add more nodes - Typically concerned with eventual consistency, meaning in a distributed environment, the only guarantee is that all nodes will eventually converge on the same value. 15"
    },
    "665": {
        "file": "05 - NoSQL Intro + KV DBs.pdf",
        "page": 15,
        "chunk": "KV DS Use Cases - EDA/Experimentation Results Store - store intermediate results from data preprocessing and EDA - store experiment or testing (A/B) results w/o prod db - Feature Store - store frequently accessed feature \u2192 low-latency retrieval for model training and prediction - Model Monitoring - store key metrics about performance of model, for example, in real-time inferencing. 16"
    },
    "666": {
        "file": "05 - NoSQL Intro + KV DBs.pdf",
        "page": 16,
        "chunk": "KV SWE Use Cases - Storing Session Information - everything about the current session can be stored via a single PUT or POST and retrieved with a single GET \u2026. VERY Fast - User Pro\ufb01les & Preferences - User info could be obtained with a single GET operation\u2026 language, TZ, product or UI preferences - Shopping Cart Data - Cart data is tied to the user - needs to be available across browsers, machines, sessions - Caching Layer: - In front of a disk-based database 17"
    },
    "667": {
        "file": "05 - NoSQL Intro + KV DBs.pdf",
        "page": 17,
        "chunk": "Redis DB - Redis (Remote Directory Server) - Open source, in-memory database - Sometimes called a data structure store - Primarily a KV store, but can be used with other models: Graph, Spatial, Full Text Search, Vector, Time Series - From db-engines.com Ranking of KV Stores: 18"
    },
    "668": {
        "file": "05 - NoSQL Intro + KV DBs.pdf",
        "page": 18,
        "chunk": "Redis - It is considered an in-memory database system, but\u2026 - Supports durability of data by: a) essentially saving snapshots to disk at speci\ufb01c intervals or b) append-only \ufb01le which is a journal of changes that can be used for roll-forward if there is a failure - Originally developed in 2009 in C++ - Can be very fast \u2026 > 100,000 SET ops / second - Rich collection of commands - Does NOT handle complex data. No secondary indexes. Only supports lookup by Key. 19"
    },
    "669": {
        "file": "05 - NoSQL Intro + KV DBs.pdf",
        "page": 19,
        "chunk": "Redis Data Types Keys: - usually strings but can be any binary sequence Values: - Strings - Lists (linked lists) - Sets (unique unsorted string elements) - Sorted Sets - Hashes (string \u2192 string) - Geospatial data 20"
    },
    "670": {
        "file": "05 - NoSQL Intro + KV DBs.pdf",
        "page": 20,
        "chunk": "Setting Up Redis in Docker - In Docker Desktop, search for Redis. - Pull/Run the latest image (see above) - Optional Settings: add 6379 to Ports to expose that port so we can connect to it. - Normally, you would not expose the Redis port for security reasons - If you did this in a prod environment, major security hole. - Notice, we didn\u2019t set a password\u2026 21"
    },
    "671": {
        "file": "05 - NoSQL Intro + KV DBs.pdf",
        "page": 21,
        "chunk": "Connecting from DataGrip - File > New > Data Source > Redis - Give the Data Source a Name - Make sure the port is 6379 - Test the connection \u2705 22"
    },
    "672": {
        "file": "05 - NoSQL Intro + KV DBs.pdf",
        "page": 22,
        "chunk": "Redis Database and Interaction - Redis provides 16 databases by default - They are numbered 0 to 15 - There is no other name associated - Direct interaction with Redis is through a set of commands related to setting and getting k/v pairs (and variations) - Many language libraries available as well. 23"
    },
    "673": {
        "file": "05 - NoSQL Intro + KV DBs.pdf",
        "page": 23,
        "chunk": "Foundation Data Type - String - Sequence of bytes - text, serialized objects, bin arrays - Simplest data type - Maps a string to another string - Use Cases: - caching frequently accessed HTML/CSS/JS fragments - con\ufb01g settings, user settings info, token management - counting web page/app screen views OR rate limiting 24"
    },
    "674": {
        "file": "05 - NoSQL Intro + KV DBs.pdf",
        "page": 24,
        "chunk": "Some Initial Basic Commands - SET /path/to/resource 0 SET user:1 \u201cJohn Doe\u201d GET /path/to/resource EXISTS user:1 DEL user:1 KEYS user* - SELECT 5 - select a different database 25"
    },
    "675": {
        "file": "05 - NoSQL Intro + KV DBs.pdf",
        "page": 25,
        "chunk": "Some Basic Commands - SET someValue 0 INCR someValue #increment by 1 INCRBY someValue 10 #increment by 10 DECR someValue #decrement by 1 DECRBY someValue 5 #decrement by 5 - INCR parses the value as int and increments (or adds to value) - SETNX key value - only sets value to key if key does not already exist 26"
    },
    "676": {
        "file": "05 - NoSQL Intro + KV DBs.pdf",
        "page": 26,
        "chunk": "Hash Type 27 - Value of KV entry is a collection of \ufb01eld-value pairs - Use Cases: - Can be used to represent basic objects/structures - number of \ufb01eld/value pairs per hash is 2^32-1 - practical limit: available system resources (e.g. memory) - Session information management - User/Event tracking (could include TTL) - Active Session Tracking (all sessions under one hash key)"
    },
    "677": {
        "file": "05 - NoSQL Intro + KV DBs.pdf",
        "page": 27,
        "chunk": "Hash Commands 28 HSET bike:1 model Demios brand Ergonom price 1971 HGET bike:1 model HGET bike:1 price HGETALL bike:1 HMGET bike:1 model price weight HINCRBY bike:1 price 100 What is returned?"
    },
    "678": {
        "file": "05 - NoSQL Intro + KV DBs.pdf",
        "page": 28,
        "chunk": "List Type - Value of KV Pair is linked lists of string values - Use Cases: - implementation of stacks and queues - queue management & message passing queues (producer/consumer model) - logging systems (easy to keep in chronological order) - build social media streams/feeds - message history in a chat application - batch processing by queueing up a set of tasks to be executed sequentially at a later time 29"
    },
    "679": {
        "file": "05 - NoSQL Intro + KV DBs.pdf",
        "page": 29,
        "chunk": "Linked Lists Crash Course - Sequential data structure of linked nodes (instead of contiguously allocated memory) - Each node points to the next element of the list (except the last one - points to nil/null) - O(1) to insert new value at front or insert new value at end 30 10 front back nil"
    },
    "680": {
        "file": "05 - NoSQL Intro + KV DBs.pdf",
        "page": 30,
        "chunk": "List Commands - Queue Queue-like Ops LPUSH bikes:repairs bike:1 LPUSH bikes:repairs bike:2 RPOP bikes:repairs RPOP biles:repairs 31"
    },
    "681": {
        "file": "05 - NoSQL Intro + KV DBs.pdf",
        "page": 31,
        "chunk": "List Commands - Stack Stack-like Ops LPUSH bikes:repairs bike:1 LPUSH bikes:repairs bike:2 LPOP bikes:repairs LPOP biles:repairs 32"
    },
    "682": {
        "file": "05 - NoSQL Intro + KV DBs.pdf",
        "page": 32,
        "chunk": "List Commands - Others Other List Ops LLEN mylist LRANGE <key> <start> <stop> LRANGE mylist 0 3 LRANGE mylist 0 0 LRANGE mylist -2 -1 33 LPUSH mylist \u201cone\u201d LPUSH mylist \u201ctwo\u201d LPUSH mylist \u201cthree\u201d"
    },
    "683": {
        "file": "05 - NoSQL Intro + KV DBs.pdf",
        "page": 33,
        "chunk": "JSON Type - Full support of the JSON standard - Uses JSONPath syntax for parsing/navigating a JSON document - Internally, stored in binary in a tree-structure \u2192 fast access to sub elements 34"
    },
    "684": {
        "file": "05 - NoSQL Intro + KV DBs.pdf",
        "page": 34,
        "chunk": "Set Type - Unordered collection of unique strings (members) - Use Cases: - track unique items (IP addresses visiting a site, page, screen) - primitive relation (set of all students in DS4300) - access control lists for users and permission structures - social network friends lists and/or group membership - Supports set operations!! 35"
    },
    "685": {
        "file": "05 - NoSQL Intro + KV DBs.pdf",
        "page": 35,
        "chunk": "Set Commands SADD ds4300 \u201cMark\u201d SADD ds4300 \u201cSam\u201d SADD cs3200 \u201cNick\u201d SADD cs3200 \u201cSam\u201d SISMEMBER ds4300 \u201cMark\u201d SISMEMBER ds4300 \u201cNick\u201d SCARD ds4300 36"
    },
    "686": {
        "file": "05 - NoSQL Intro + KV DBs.pdf",
        "page": 36,
        "chunk": "Set Commands SADD ds4300 \u201cMark\u201d SADD ds4300 \u201cSam\u201d SADD cs3200 \u201cNick\u201d SADD cs3200 \u201cSam\u201d SCARD ds4300 SINTER ds4300 cs3200 SDIFF ds4300 cs3200 SREM ds4300 \u201cMark\u201d SRANDMEMBER ds4300 37"
    },
    "687": {
        "file": "05 - NoSQL Intro + KV DBs.pdf",
        "page": 37,
        "chunk": "?? 38"
    },
    "688": {
        "file": "Misc Professor Questions.pdf",
        "page": 0,
        "chunk": "1.\u200b Redis in Machine Learning: a.\u200b Pipeline: Data Source (AWS) -> Transformations (Apache Spark) -> Inference Store (Stored on Redis) + Training Store (Stored on AWS) i.\u200b Question: Why would you add complexity by adding the inference store on Redis? ii.\u200b Answer: For latency issues: accessing a key from Redis is much faster than running a SELECT statement. Redis is stored on disk which is faster than accessing from RAM. 2.\u200b Port mapping a:b a.\u200b A = host port on the physical machine b.\u200b B = port inside the container c.\u200b Maximum port number = 65535 d.\u200b At the operating system level, http maps to port 80, https maps to port 443, ssh maps to port 22, ftp maps to 21 e.\u200b Range of ports reserved for system services: 0 - 102"
    },
    "689": {
        "file": "07 - Document DBs and Mongo.pdf",
        "page": 0,
        "chunk": "DS 4300 Document Databases & MongoDB Mark Fontenot, PhD Northeastern University Some material used with permission from Dr. Rachlin, with thanks!"
    },
    "690": {
        "file": "07 - Document DBs and Mongo.pdf",
        "page": 1,
        "chunk": "Document Database A Document Database is a non-relational database that stores data as structured documents, usually in JSON. They are designed to be simple, \ufb02exible, and scalable. 2"
    },
    "691": {
        "file": "07 - Document DBs and Mongo.pdf",
        "page": 2,
        "chunk": "What is JSON? \u25cf JSON (JavaScript Object Notation) \u25cb a lightweight data-interchange format \u25cb It is easy for humans to read and write. \u25cb It is easy for machines to parse and generate. \u25cf JSON is built on two structures: \u25cb A collection of name/value pairs. In various languages, this is operationalized as an object, record, struct, dictionary, hash table, keyed list, or associative array. \u25cb An ordered list of values. In most languages, this is operationalized as an array, vector, list, or sequence. \u25cf These are two universal data structures supported by virtually all modern programming languages \u25cb Thus, JSON makes a great data interchange format. 3"
    },
    "692": {
        "file": "07 - Document DBs and Mongo.pdf",
        "page": 3,
        "chunk": "JSON Syntax 4 https://www.json.org/json-en.html"
    },
    "693": {
        "file": "07 - Document DBs and Mongo.pdf",
        "page": 4,
        "chunk": "Binary JSON? BSON - BSON \u2192 Binary JSON - binary-encoded serialization of a JSON-like document structure - supports extended types not part of basic JSON (e.g. Date, BinaryData, etc) - Lightweight - keep space overhead to a minimum - Traversable - designed to be easily traversed, which is vitally important to a document DB - Ef\ufb01cient - encoding and decoding must be ef\ufb01cient - Supported by many modern programming languages 5"
    },
    "694": {
        "file": "07 - Document DBs and Mongo.pdf",
        "page": 5,
        "chunk": "XML (eXtensible Markup Language) \u25cfPrecursor to JSON as data exchange format \u25cfXML + CSS \u2192 web pages that separated content and formatting \u25cfStructurally similar to HTML, but tag set is extensible 6"
    },
    "695": {
        "file": "07 - Document DBs and Mongo.pdf",
        "page": 6,
        "chunk": "XML-Related Tools/Technologies - Xpath - a syntax for retrieving speci\ufb01c elements from an XML doc - Xquery - a query language for interrogating XML documents; the SQL of XML - DTD - Document Type De\ufb01nition - a language for describing the allowed structure of an XML document - XSLT - eXtensible Stylesheet Language Transformation - tool to transform XML into other formats, including non-XML formats such as HTML. 7"
    },
    "696": {
        "file": "07 - Document DBs and Mongo.pdf",
        "page": 7,
        "chunk": "Why Document Databases? - Document databases address the impedance mismatch problem between object persistence in OO systems and how relational DBs structure data. - OO Programming \u2192 Inheritance and Composition of types. - How do we save a complex object to a relational database? We basically have to deconstruct it. - The structure of a document is self-describing. - They are well-aligned with apps that use JSON/XML as a transport layer 8"
    },
    "697": {
        "file": "07 - Document DBs and Mongo.pdf",
        "page": 8,
        "chunk": "MongoDB 9"
    },
    "698": {
        "file": "07 - Document DBs and Mongo.pdf",
        "page": 9,
        "chunk": "MongoDB - Started in 2007 after Doubleclick was acquired by Google, and 3 of its veterans realized the limitations of relational databases for serving > 400,000 ads per second - MongoDB was short for Humongous Database - MongoDB Atlas released in 2016 \u2192 documentdb as a service 10 https://www.mongodb.com/company/our-story"
    },
    "699": {
        "file": "07 - Document DBs and Mongo.pdf",
        "page": 10,
        "chunk": "MongoDB Structure 11 Database Collection A Collection B Collection C Document 1 Document 2 Document 3 Document 1 Document 2 Document 3 Document 1 Document 2 Document 3"
    },
    "700": {
        "file": "07 - Document DBs and Mongo.pdf",
        "page": 11,
        "chunk": "MongoDB Documents - No prede\ufb01ned schema for documents is needed - Every document in a collection could have different data/schema 12"
    },
    "701": {
        "file": "07 - Document DBs and Mongo.pdf",
        "page": 12,
        "chunk": "Relational vs Mongo/Document DB 13 RDBMS MongoDB Database Database Table/View Collection Row Document Column Field Index Index Join Embedded Document Foreign Key Reference"
    },
    "702": {
        "file": "07 - Document DBs and Mongo.pdf",
        "page": 13,
        "chunk": "MongoDB Features - Rich Query Support - robust support for all CRUD ops - Indexing - supports primary and secondary indices on document \ufb01elds - Replication - supports replica sets with automatic failover - Load balancing built in 14"
    },
    "703": {
        "file": "07 - Document DBs and Mongo.pdf",
        "page": 14,
        "chunk": "MongoDB Versions \u25cfMongoDB Atlas \u25cb Fully managed MongoDB service in the cloud (DBaaS) \u25cfMongoDB Enterprise \u25cb Subscription-based, self-managed version of MongoDB \u25cfMongoDB Community \u25cb source-available, free-to-use, self-managed 15"
    },
    "704": {
        "file": "07 - Document DBs and Mongo.pdf",
        "page": 15,
        "chunk": "Interacting with MongoDB \u25cfmongosh \u2192 MongoDB Shell \u25cb CLI tool for interacting with a MongoDB instance \u25cfMongoDB Compass \u25cb free, open-source GUI to work with a MongoDB database \u25cfDataGrip and other 3rd Party Tools \u25cfEvery major language has a library to interface with MongoDB \u25cb PyMongo (Python), Mongoose (JavaScript/node), \u2026 16"
    },
    "705": {
        "file": "07 - Document DBs and Mongo.pdf",
        "page": 16,
        "chunk": "Mongodb Community Edition in Docker - Create a container - Map host:container port 27017 - Give initial username and password for superuser 17 E D"
    },
    "706": {
        "file": "07 - Document DBs and Mongo.pdf",
        "page": 17,
        "chunk": "MongoDB Compass - GUI Tool for interacting with MongoDB instance - Download and install from > here <. 18"
    },
    "707": {
        "file": "07 - Document DBs and Mongo.pdf",
        "page": 18,
        "chunk": "Load MFlix Sample Data Set - In Compass, create a new Database named m\ufb02ix - Download m\ufb02ix sample dataset and unzip it - Import JSON \ufb01les for users, theaters, movies, and comments into new collections in the m\ufb02ix database 19"
    },
    "708": {
        "file": "07 - Document DBs and Mongo.pdf",
        "page": 19,
        "chunk": "Creating a Database and Collection 20 m\ufb02ix users To Create a new DB: To Create a new Collection:"
    },
    "709": {
        "file": "07 - Document DBs and Mongo.pdf",
        "page": 20,
        "chunk": "mongosh - Mongo Shell - \ufb01nd(...) is like SELECT 21 collection.find({ ____ }, { ____ }) \ufb01lters projections"
    },
    "710": {
        "file": "07 - Document DBs and Mongo.pdf",
        "page": 21,
        "chunk": "mongosh - \ufb01nd() - SELECT * FROM users; 22 use mflix db.users.find()"
    },
    "711": {
        "file": "07 - Document DBs and Mongo.pdf",
        "page": 22,
        "chunk": "mongosh - \ufb01nd() - SELECT * FROM users WHERE name = \u201cDavos Seaworth\u201d; 23 db.users.find({\"name\": \"Davos Seaworth\"}) \ufb01lter"
    },
    "712": {
        "file": "07 - Document DBs and Mongo.pdf",
        "page": 23,
        "chunk": "mongosh - \ufb01nd() - SELECT * FROM movies WHERE rated in (\"PG\", \"PG-13\") 24 db.movies.find({rated: {$in:[ \"PG\", \"PG-13\" ]}})"
    },
    "713": {
        "file": "07 - Document DBs and Mongo.pdf",
        "page": 24,
        "chunk": "mongosh - \ufb01nd() - Return movies which were released in Mexico and have an IMDB rating of at least 7 25 db.movies.find( { \"countries\": \"Mexico\", \"imdb.rating\": { $gte: 7 } } )"
    },
    "714": {
        "file": "07 - Document DBs and Mongo.pdf",
        "page": 25,
        "chunk": "mongosh - \ufb01nd() - Return movies from the movies collection which were released in 2010 and either won at least 5 awards or have a genre of Drama 26 db.movies.find( { \u201cyear\u201d: 2010, $or: [ { \"awards.wins\": { $gte: 5 } }, { \u201cgenres\u201d: \"Drama\" } ] })"
    },
    "715": {
        "file": "07 - Document DBs and Mongo.pdf",
        "page": 26,
        "chunk": "Comparison Operators 27"
    },
    "716": {
        "file": "07 - Document DBs and Mongo.pdf",
        "page": 27,
        "chunk": "mongosh - countDocuments() - How many movies from the movies collection were released in 2010 and either won at least 5 awards or have a genre of Drama 28 db.movies.countDocuments( { \u201cyear\u201d: 2010, $or: [ { \"awards.wins\": { $gte: 5 } }, { \u201cgenres\u201d: \"Drama\" } ] })"
    },
    "717": {
        "file": "07 - Document DBs and Mongo.pdf",
        "page": 28,
        "chunk": "mongosh - project - Return the names of all movies from the movies collection that were released in 2010 and either won at least 5 awards or have a genre of Drama 29 db.movies.countDocuments( { \u201cyear\u201d: 2010, $or: [ { \"awards.wins\": { $gte: 5 } }, { \u201cgenres\u201d: \"Drama\" } ] }, {\u201cname\u201d: 1, \u201c_id\u201d: 0} ) 1 = return; 0 = don\u2019t return"
    },
    "718": {
        "file": "07 - Document DBs and Mongo.pdf",
        "page": 29,
        "chunk": "PyMongo 30"
    },
    "719": {
        "file": "07 - Document DBs and Mongo.pdf",
        "page": 30,
        "chunk": "PyMongo \u25cfPyMongo is a Python library for interfacing with MongoDB instances 31 from pymongo import MongoClient client = MongoClient( \u2018mongodb://user_name:pw@localhost:27017\u2019 )"
    },
    "720": {
        "file": "07 - Document DBs and Mongo.pdf",
        "page": 31,
        "chunk": "Getting a Database and Collection 32 from pymongo import MongoClient client = MongoClient( \u2018mongodb://user_name:pw@localhost:27017\u2019 ) db = client[\u2018ds4300\u2019] collection = db[\u2018myCollection\u2019]"
    },
    "721": {
        "file": "07 - Document DBs and Mongo.pdf",
        "page": 32,
        "chunk": "Inserting a Single Document 33 db = client[\u2018ds4300\u2019] collection = db[\u2018myCollection\u2019] post = { \u201cauthor\u201d: \u201cMark\u201d, \u201ctext\u201d: \u201cMongoDB is Cool!\u201d, \u201ctags\u201d: [\u201cmongodb\u201d, \u201cpython\u201d] } post_id = collection.insert_one(post).inserted_id print(post_id)"
    },
    "722": {
        "file": "07 - Document DBs and Mongo.pdf",
        "page": 33,
        "chunk": "Count Documents in Collection - SELECT count(*) FROM collection 34 demodb.collection.count_documents({})"
    },
    "723": {
        "file": "07 - Document DBs and Mongo.pdf",
        "page": 34,
        "chunk": "?? 35"
    },
    "724": {
        "file": "Extended Notes - Open DSA B+ Trees.pdf",
        "page": 0,
        "chunk": "12.6. B-Trees 12.6.1. B-Trees \u00b6 This module presents the B-tree. B-trees are usually attributed to R. Bayer and E. McCreight who described the B-tree in a 1972 paper. By 1979, B-trees had replaced virtually all large-file access methods other than hashing. B-trees, or some variant of B-trees, are the standard file organization for applications requiring insertion, deletion, and key range searches. They are used to implement most modern file systems. B-trees address effectively all of the major problems encountered when implementing disk-based search trees: 1. The B-tree is shallow, in part because the tree is always height balanced (all leaf nodes are at the same level), and in part because the branching factor is quite high. So only a small number of disk blocks are accessed to reach a given record. 2. Update and search operations affect only those disk blocks on the path from the root to the leaf node containing the query record. The fewer the number of disk blocks affected during an operation, the less disk I/O is required. 3. B-trees keep related records (that is, records with similar key values) on the same disk block, which helps to minimize disk I/O on range searches. 4. B-trees guarantee that every node in the tree will be full at least to a certain minimum percentage. This improves space efficiency while reducing the typical number of disk fetches necessary during a search or update operation. A B-tree of order is defined to have the following shape properties: The root is either a leaf or has at least two children. Each internal node, except for the root, has between and children. All leaves are at the same level in the tree, so the tree is always height balanced. The B-tree is a generalization of the 2-3 tree. Put another way,"
    },
    "725": {
        "file": "Extended Notes - Open DSA B+ Trees.pdf",
        "page": 0,
        "chunk": "is either a leaf or has at least two children. Each internal node, except for the root, has between and children. All leaves are at the same level in the tree, so the tree is always height balanced. The B-tree is a generalization of the 2-3 tree. Put another way, a 2-3 tree is a B-tree of order three. Normally, the size of a node in the B-tree is chosen to fill a disk block. A B-tree node implementation typically allows 100 or more children. Thus, a B-tree node is equivalent to a disk block, and a \u201cpointer\u201d value stored in the tree is actually the number of the block containing the child node (usually interpreted as an offset from the beginning of the corresponding disk file). In a typical application, the B-tree\u2019s access to the disk file will be managed using a buffer pool and a block-replacement scheme such as LRU. Figure 12.6.1 shows a B-tree of order four. Each node contains up to three keys, and internal nodes have up to four children. Figure 12.6.1: A B-tree of order four. Search in a B-tree is a generalization of search in a 2-3 tree. It is an alternating two-step process, beginning with the root node of the B-tree. 1. Perform a binary search on the records in the current node. If a record with the search key is found, then return that record. If the current node is a leaf node and the key is not found, then report an unsuccessful search. 2. Otherwise, follow the proper branch and repeat the process. For example, consider a search for the record with key value 47 in the tree of Figure 12.6.1. The root node is examined and the second (right) branch taken. After examining the node at level 1, the"
    },
    "726": {
        "file": "Extended Notes - Open DSA B+ Trees.pdf",
        "page": 0,
        "chunk": "an unsuccessful search. 2. Otherwise, follow the proper branch and repeat the process. For example, consider a search for the record with key value 47 in the tree of Figure 12.6.1. The root node is examined and the second (right) branch taken. After examining the node at level 1, the third branch is taken to the next level to arrive at the leaf node containing a record with key value 47. B-tree insertion is a generalization of 2-3 tree insertion. The first step is to find the leaf node that should contain the key to be inserted, space permitting. If there is room in this node, then insert the key. If there is not, then split the node into two and promote the middle key to the parent. If the parent becomes full, then it is split in turn, and its middle key promoted. Note that this insertion process is guaranteed to keep all nodes at least half full. For example, when we attempt to insert into a full internal node of a B-tree of order four, there will now be five children that must be dealt with. The node is split into two nodes containing two keys each, thus retaining the B-tree property. The middle of the five children is promoted to its parent. 12.6.1.1. B+ Trees The previous section mentioned that B-trees are universally used to implement large-scale disk-based systems. Actually, the B-tree as described in the previous section is almost never implemented. What is most commonly implemented is a variant of the B-tree, called the tree. When greater efficiency is required, a more complicated variant known as the tree is used. Consider again the linear index. When the collection of records will not change, a linear index provides an extremely efficient way to search. The problem is"
    },
    "727": {
        "file": "Extended Notes - Open DSA B+ Trees.pdf",
        "page": 0,
        "chunk": "is a variant of the B-tree, called the tree. When greater efficiency is required, a more complicated variant known as the tree is used. Consider again the linear index. When the collection of records will not change, a linear index provides an extremely efficient way to search. The problem is how to handle those pesky inserts and deletes. We could try to keep the core idea of storing a sorted array-based list, but make it more flexible by breaking the list into manageable chunks that are more easily updated. How might we do that? First, we need to decide how big the chunks should be. Since the data are on disk, it seems reasonable to store a chunk that is the size of a disk block, or a small multiple of the disk block size. If the next record to be inserted belongs to a chunk that hasn\u2019t filled its block then we can just insert it there. The fact that this might cause other records in that chunk to move a little bit in the array is not important, since this does not cause any extra disk accesses so long as we move data within that chunk. But what if the chunk fills up the entire block that contains it? We could just split it in half. What if we want to delete a record? We could just take the deleted record out of the chunk, but we might not want a lot of near-empty chunks. So we could put adjacent chunks together if they have only a small amount of data between them. Or we could shuffle data between adjacent chunks that together contain more data. The big problem would be how to find the desired chunk when processing a record with a given key. Perhaps some sort"
    },
    "728": {
        "file": "Extended Notes - Open DSA B+ Trees.pdf",
        "page": 0,
        "chunk": "put adjacent chunks together if they have only a small amount of data between them. Or we could shuffle data between adjacent chunks that together contain more data. The big problem would be how to find the desired chunk when processing a record with a given key. Perhaps some sort of tree-like structure could be used to locate the appropriate chunk. These ideas are exactly what motivate the tree. The tree is essentially a mechanism for managing a sorted array-based list, where the list is broken into chunks. The most significant difference between the tree and the BST or the standard B-tree is that the tree stores records only at the leaf nodes. Internal nodes store key values, but these are used solely as placeholders to guide the search. This means that internal nodes are significantly different in structure from leaf nodes. Internal nodes store keys to guide the search, associating m \u2308m/2\u2309 m 24 15 20 33 45 48 10 12 18 21 23 30 30 38 47 50 52 60 B+ B\u2217 B+ B+ B+ B+"
    },
    "729": {
        "file": "Extended Notes - Open DSA B+ Trees.pdf",
        "page": 1,
        "chunk": "each key with a pointer to a child tree node. Leaf nodes store actual records, or else keys and pointers to actual records in a separate disk file if the tree is being used purely as an index. Depending on the size of a record as compared to the size of a key, a leaf node in a tree of order might have enough room to store more or less than records. The requirement is simply that the leaf nodes store enough records to remain at least half full. The leaf nodes of a tree are normally linked together to form a doubly linked list. Thus, the entire collection of records can be traversed in sorted order by visiting all the leaf nodes on the linked list. Here is a Java-like pseudocode representation for the tree node interface. Leaf node and internal node subclasses would implement this interface. /** Interface for B+ Tree nodes */ public interface BPNode<Key,E> { public boolean isLeaf(); public int numrecs(); public Key[] keys(); } An important implementation detail to note is that while Figure 12.6.1 shows internal nodes containing three keys and four pointers, class BPNode is slightly different in that it stores key/pointer pairs. Figure 12.6.1 shows the tree as it is traditionally drawn. To simplify implementation in practice, nodes really do associate a key with each pointer. Each internal node should be assumed to hold in the leftmost position an additional key that is less than or equal to any possible key value in the node\u2019s leftmost subtree. tree implementations typically store an additional dummy record in the leftmost leaf node whose key value is less than any legal key value. Let\u2019s see in some detail how the simplest tree works. This would be the \u201c tree\u201d, or a tree of order 3."
    },
    "730": {
        "file": "Extended Notes - Open DSA B+ Trees.pdf",
        "page": 1,
        "chunk": "in the node\u2019s leftmost subtree. tree implementations typically store an additional dummy record in the leftmost leaf node whose key value is less than any legal key value. Let\u2019s see in some detail how the simplest tree works. This would be the \u201c tree\u201d, or a tree of order 3. Figure 12.6.2: An example of building a tree Next, let\u2019s see how to search. Figure 12.6.3: An example of searching a tree Finally, let\u2019s see an example of deleting from the tree B+ B+ B+ m m B+ B+ B+ B+ B+ 2 \u22123+ B+ 1 / 28 << < > >> Example 2-3+ Tree Visualization: Insert 2 \u22123+ 1 / 10 << < > >> Example 2-3+ Tree Visualization: Search 15 J 22 X 52 B 33 65 S 71 W 89 M 71 46 65 33 O 46 H 47 L 52 2 \u22123+ 2 \u22123+ 1 / 33 << < > >> Example 2-3+ Tree Visualization: Delete 46 65"
    },
    "731": {
        "file": "Extended Notes - Open DSA B+ Trees.pdf",
        "page": 2,
        "chunk": "Figure 12.6.4: An example of deleting from a tree Now, let\u2019s extend these ideas to a tree of higher order. trees are exceptionally good for range queries. Once the first record in the range has been found, the rest of the records with keys in the range can be accessed by sequential processing of the remaining records in the first node, and then continuing down the linked list of leaf nodes as far as necessary. Figure illustrates the tree. Figure 12.6.5: An example of search in a B+ tree of order four. Internal nodes must store between two and four children. Search in a tree is nearly identical to search in a regular B-tree, except that the search must always continue to the proper leaf node. Even if the search-key value is found in an internal node, this is only a placeholder and does not provide access to the actual record. Here is a pseudocode sketch of the tree search algorithm. private E findhelp(BPNode<Key,E> rt, Key k) { int currec = binaryle(rt.keys(), rt.numrecs(), k); if (rt.isLeaf()) { if ((((BPLeaf<Key,E>)rt).keys())[currec] == k) { return ((BPLeaf<Key,E>)rt).recs(currec); } else { return null; } } else{ return findhelp(((BPInternal<Key,E>)rt).pointers(currec), k); } } tree insertion is similar to B-tree insertion. First, the leaf that should contain the record is found. If is not full, then the new record is added, and no other tree nodes are affected. If is already full, split it in two (dividing the records evenly among the two nodes) and promote a copy of the least-valued key in the newly formed right node. As with the 2-3 tree, promotion might cause the parent to split in turn, perhaps eventually leading to splitting the root and causing the tree to gain a new level. tree insertion keeps all leaf nodes at equal depth."
    },
    "732": {
        "file": "Extended Notes - Open DSA B+ Trees.pdf",
        "page": 2,
        "chunk": "copy of the least-valued key in the newly formed right node. As with the 2-3 tree, promotion might cause the parent to split in turn, perhaps eventually leading to splitting the root and causing the tree to gain a new level. tree insertion keeps all leaf nodes at equal depth. Figure illustrates the insertion process through several examples. 15 J 71 W 89 M 22 65 S 70 F 51 B 52 T 71 46 H 47 L 22 X 33 O 51 2 \u22123+ B+ B+ B+ 1 / 10 << < > >> Example B+ Tree Visualization: Search in a tree of degree 4 10 S 18 E 40 Q 55 F 25 40 77 A 89 B 98 A 127 V 25 T 39 F 98 77 B+ B+ B+ L L B+ L B+ B+ 1 / 42 << < > >> Example B+ Tree Visualization: Insert into a tree of degree 4"
    },
    "733": {
        "file": "Extended Notes - Open DSA B+ Trees.pdf",
        "page": 3,
        "chunk": "Figure 12.6.6: An example of building a B+ tree of order four. Here is a a Java-like pseudocode sketch of the tree insert algorithm. private BPNode<Key,E> inserthelp(BPNode<Key,E> rt, Key k, E e) { BPNode<Key,E> retval; if (rt.isLeaf()) { // At leaf node: insert here return ((BPLeaf<Key,E>)rt).add(k, e); } // Add to internal node int currec = binaryle(rt.keys(), rt.numrecs(), k); BPNode<Key,E> temp = inserthelp( ((BPInternal<Key,E>)root).pointers(currec), k, e); if (temp != ((BPInternal<Key,E>)rt).pointers(currec)) { return ((BPInternal<Key,E>)rt). add((BPInternal<Key,E>)temp); } else{ return rt; } } Here is an exercise to see if you get the basic idea of tree insertion. To delete record from the tree, first locate the leaf that contains . If is more than half full, then we need only remove , leaving still at least half full. This is demonstrated by Figure . Figure 12.6.7: An example of deletion in a B+ tree of order four. B+ B+ B+ Tree Insertion Instructions: In this exercise your job is to insert the values from the stack to the B+ tree. Search for the leaf node where the topmost value of the stack should be inserted, and click on that node. The exercise will take care of the rest. Continue this procedure until you have inserted all the values in the stack. Undo Reset Model Answer Grade 47 64 54 72 38 90 92 14 21 76 95 49 35 13 10 34 60 86 R B+ L R L R L 1 / 23 << < > >> Example B+ Tree Visualization: Delete from a tree of degree 4 5 F 10 S 44 Q 48 E 12 44 67 A 88 B 58 A 60 F 12 V 27 T 67 58"
    },
    "734": {
        "file": "Extended Notes - Open DSA B+ Trees.pdf",
        "page": 3,
        "chunk": "Delete from a tree of degree 4 5 F 10 S 44 Q 48 E 12 44 67 A 88 B 58 A 60 F 12 V 27 T 67 58"
    },
    "735": {
        "file": "Extended Notes - Open DSA B+ Trees.pdf",
        "page": 4,
        "chunk": "If deleting a record reduces the number of records in the node below the minimum threshold (called an underflow), then we must do something to keep the node sufficiently full. The first choice is to look at the node\u2019s adjacent siblings to determine if they have a spare record that can be used to fill the gap. If so, then enough records are transferred from the sibling so that both nodes have about the same number of records. This is done so as to delay as long as possible the next time when a delete causes this node to underflow again. This process might require that the parent node has its placeholder key value revised to reflect the true first key value in each node. If neither sibling can lend a record to the under-full node (call it ), then must give its records to a sibling and be removed from the tree. There is certainly room to do this, because the sibling is at most half full (remember that it had no records to contribute to the current node), and has become less than half full because it is under-flowing. This merge process combines two subtrees of the parent, which might cause it to underflow in turn. If the last two children of the root merge together, then the tree loses a level. Here is a Java-like pseudocode for the tree delete algorithm. /** Delete a record with the given key value, and return true if the root underflows */ private boolean removehelp(BPNode<Key,E> rt, Key k) { int currec = binaryle(rt.keys(), rt.numrecs(), k); if (rt.isLeaf()) { if (((BPLeaf<Key,E>)rt).keys()[currec] == k) { return ((BPLeaf<Key,E>)rt).delete(currec); } else { return false; } } else{ // Process internal node if (removehelp(((BPInternal<Key,E>)rt).pointers(currec), k)) { // Child will merge if necessary return ((BPInternal<Key,E>)rt).underflow(currec); } else"
    },
    "736": {
        "file": "Extended Notes - Open DSA B+ Trees.pdf",
        "page": 4,
        "chunk": "*/ private boolean removehelp(BPNode<Key,E> rt, Key k) { int currec = binaryle(rt.keys(), rt.numrecs(), k); if (rt.isLeaf()) { if (((BPLeaf<Key,E>)rt).keys()[currec] == k) { return ((BPLeaf<Key,E>)rt).delete(currec); } else { return false; } } else{ // Process internal node if (removehelp(((BPInternal<Key,E>)rt).pointers(currec), k)) { // Child will merge if necessary return ((BPInternal<Key,E>)rt).underflow(currec); } else { return false; } } } The tree requires that all nodes be at least half full (except for the root). Thus, the storage utilization must be at least 50%. This is satisfactory for many implementations, but note that keeping nodes fuller will result both in less space required (because there is less empty space in the disk file) and in more efficient processing (fewer blocks on average will be read into memory because the amount of information in each block is greater). Because B-trees have become so popular, many algorithm designers have tried to improve B-tree performance. One method for doing so is to use the tree variant known as the tree. The tree is identical to the tree, except for the rules used to split and merge nodes. Instead of splitting a node in half when it overflows, the tree gives some records to its neighboring sibling, if possible. If the sibling is also full, then these two nodes split into three. Similarly, when a node underflows, it is combined with its two siblings, and the total reduced to two nodes. Thus, the nodes are always at least two thirds full. [1] Finally, here is an example of building a B+ Tree of order five. You can compare this to the example above of building a tree of order four with the same records. Figure 12.6.8: An example of building a B+ tree of degree 5 Click here for a visualization that will let you construct and interact"
    },
    "737": {
        "file": "Extended Notes - Open DSA B+ Trees.pdf",
        "page": 4,
        "chunk": "building a B+ Tree of order five. You can compare this to the example above of building a tree of order four with the same records. Figure 12.6.8: An example of building a B+ tree of degree 5 Click here for a visualization that will let you construct and interact with a tree. This visualization was written by David Galles of the University of San Francisco as part of his Data Structure Visualizations package. [1] This concept can be extended further if higher space utilization is required. However, the update routines become much more complicated. I once worked on a project where we implemented 3- for-4 node split and merge routines. This gave better performance than the 2-for-3 node split and merge routines of the tree. However, the spitting and merging routines were so complicated that even their author could no longer understand them once they were completed! 12.6.1.2. B-Tree Analysis The asymptotic cost of search, insertion, and deletion of records from B-trees, trees, and trees is where is the total number of records in the tree. However, the base of the log is the (average) branching factor of the tree. Typical database applications use extremely high branching factors, perhaps 100 or more. Thus, in practice the B-tree and its variants are extremely shallow. As an illustration, consider a tree of order 100 and leaf nodes that contain up to 100 records. A B- tree with height one (that is, just a single leaf node) can have at most 100 records. A tree with height two (a root internal node whose children are leaves) must have at least 100 records (2 leaves with 50 records each). It has at most 10,000 records (100 leaves with 100 records each). A tree with height three must have at least 5000 records (two"
    },
    "738": {
        "file": "Extended Notes - Open DSA B+ Trees.pdf",
        "page": 4,
        "chunk": "records. A tree with height two (a root internal node whose children are leaves) must have at least 100 records (2 leaves with 50 records each). It has at most 10,000 records (100 leaves with 100 records each). A tree with height three must have at least 5000 records (two second-level nodes with 50 children containing 50 records each) and at most one million records (100 second-level nodes N N N B+ B+ B+ B\u2217 B\u2217 B+ B\u2217 1 / 33 << < > >> Example B+ Tree Visualization: Insert into a tree of degree 5 B+ B\u2217 B+ B\u2217 \u0398(log n) n B+ B+ B+ B+"
    },
    "739": {
        "file": "Extended Notes - Open DSA B+ Trees.pdf",
        "page": 5,
        "chunk": "with 100 full children each). A tree with height four must have at least 250,000 records and at most 100 million records. Thus, it would require an extremely large database to generate a tree of more than height four. The tree split and insert rules guarantee that every node (except perhaps the root) is at least half full. So they are on average about 3/4 full. But the internal nodes are purely overhead, since the keys stored there are used only by the tree to direct search, rather than store actual data. Does this overhead amount to a significant use of space? No, because once again the high fan-out rate of the tree structure means that the vast majority of nodes are leaf nodes. A K-ary tree has approximately of its nodes as internal nodes. This means that while half of a full binary tree\u2019s nodes are internal nodes, in a tree of order 100 probably only about of its nodes are internal nodes. This means that the overhead associated with internal nodes is very low. We can reduce the number of disk fetches required for the B-tree even more by using the following methods. First, the upper levels of the tree can be stored in main memory at all times. Because the tree branches so quickly, the top two levels (levels 0 and 1) require relatively little space. If the B-tree is only height four, then at most two disk fetches (internal nodes at level two and leaves at level three) are required to reach the pointer to any given record. A buffer pool could be used to manage nodes of the B-tree. Several nodes of the tree would typically be in main memory at one time. The most straightforward approach is to use a standard method such as"
    },
    "740": {
        "file": "Extended Notes - Open DSA B+ Trees.pdf",
        "page": 5,
        "chunk": "level three) are required to reach the pointer to any given record. A buffer pool could be used to manage nodes of the B-tree. Several nodes of the tree would typically be in main memory at one time. The most straightforward approach is to use a standard method such as LRU to do node replacement. However, sometimes it might be desirable to \u201clock\u201d certain nodes such as the root into the buffer pool. In general, if the buffer pool is even of modest size (say at least twice the depth of the tree), no special techniques for node replacement will be required because the upper-level nodes will naturally be accessed frequently. B+ B+ B+ 1/K B+ 1/75"
    },
    "741": {
        "file": "Hashmaps and B+ Trees.pdf",
        "page": 0,
        "chunk": "Hashmaps \u25cf\u200b Like a python dictionary \u25cf\u200b Collection of each slots; each slot has an \u201caddress\u201d \u25cf\u200b Given a key, apply some hashing function to it that maps it to an integer from 0 to n - 1, where n is the size of the table \u25cf\u200b To ensure that the hash function maps to an integer between 0 and n - 1, you can mod it by the table size \u25cf\u200b Many values may map to the same slot, therefore we have to store the key and the value to get the information we\u2019re looking for \u25cf\u200b The load factor \u03bb can be computed by dividing the the number of inserted values (n) by the table size (m) \u25cb\u200b \u03bb = n / m \u25cb\u200b We usually like to keep the load factor under a particular threshold \u25cf\u200b Why would we use hash tables over AVL trees? \u25cb\u200b Hash functions take O(1) time and passing the same input results in the same output each time, making it consistent across insertion and searching \u25cb\u200b Insertion can be done in O(1) time, making it faster than AVL insertion \u25cf\u200b Collisions - occurs when two keys are inserted into the same slot \u25cb\u200b Resolutions \u25a0\u200b Not insert \u25a0\u200b Look for next open space (open addressing) \u25a0\u200b Make the value a list of values (separate chaining) \u25cf\u200b How to choose the best table size? \u25cb\u200b Start with some set size \u25cb\u200b With each insertion, check the load factor, and resize if needed \u25cb\u200b When you do need to resize, multiply the original table size by some factor k \u25cb\u200b You have to re-hash every time you resize the table \u25cf\u200b Hash Table efficiency \u25cb\u200b Searching in a large table with short chains is practically constant time \u25a0\u200b Finding one slot out of 100,000"
    },
    "742": {
        "file": "Hashmaps and B+ Trees.pdf",
        "page": 0,
        "chunk": "When you do need to resize, multiply the original table size by some factor k \u25cb\u200b You have to re-hash every time you resize the table \u25cf\u200b Hash Table efficiency \u25cb\u200b Searching in a large table with short chains is practically constant time \u25a0\u200b Finding one slot out of 100,000 = O(1) \u25a0\u200b Once you find a slot, finding one key out of maximum five key-value pairs is also constant time, similar to searching in an AVL tree with height log(100,000) \u25cb\u200b We want hash tables with good dispersion (broad distribution) B+ Trees \u25cf\u200b B+ Trees are the most common indexing structure used in RBDs"
    },
    "743": {
        "file": "Hashmaps and B+ Trees.pdf",
        "page": 1,
        "chunk": "\u25cf\u200b Optimized for disk-based accessing \u25cf\u200b A B+ tree is an m-way tree with order M \u25cb\u200b M is the maximum number of keys in each node \u25cb\u200b M+1 is the maximum number of children that each node can have \u25cb\u200b Node structure for M = 3 \u25a0\u200b A, B, C are keys, where A < B < C \u25a0\u200b The left and right children of the keys are pointers. \u25cf\u200b The left pointer of A contains keys that are < A. The right pointer of A contains values that are \u2265 A, but < B. The right pointer of B contains values that are \u2265B but < C. The right pointer of C contains values that are \u2265 C. \u25cf\u200b Properties of a B+ Tree \u25cb\u200b All nodes (except the root) must be at least half full (of children) \u25cb\u200b Root node does not have to be half full \u25cb\u200b Insertions are always done at the leaf level \u25cb\u200b Leaves are stored as a doubly-linked list \u25cb\u200b For the same set of values, a B+ tree will be shallowers, but wider, than a BST \u25cb\u200b Within a node, keys ar kept sortd \u25cf\u200b Insertion in a B+ Tree \u25cb\u200b The insertion process involves adding new keys to the appropriate leaf nodes while maintaining the tree's balanced properties. \u25cb\u200b Steps for Insertion in B+ Tree: \u25a0\u200b Locate the Appropriate Leaf Node: \u25cf\u200b Begin by traversing the tree from the root to find the correct leaf node where the new key should be inserted. \u25a0\u200b Insert the Key: \u25cf\u200b If the leaf node has fewer than the maximum allowed keys, insert the new key in the correct position to maintain sorted order. \u25cf\u200b If the leaf node is full, split it into two nodes: \u25cf\u200b Distribute the existing keys and the"
    },
    "744": {
        "file": "Hashmaps and B+ Trees.pdf",
        "page": 1,
        "chunk": "should be inserted. \u25a0\u200b Insert the Key: \u25cf\u200b If the leaf node has fewer than the maximum allowed keys, insert the new key in the correct position to maintain sorted order. \u25cf\u200b If the leaf node is full, split it into two nodes: \u25cf\u200b Distribute the existing keys and the new key between the two nodes evenly. \u25cf\u200b Copy the middle key (median) to the parent node to act as a separator between the two new nodes. \u25a0\u200b Handle Parent Node: \u25cf\u200b If the parent node also becomes full due to the insertion of the median key, repeat the splitting process recursively up the tree."
    },
    "745": {
        "file": "Hashmaps and B+ Trees.pdf",
        "page": 2,
        "chunk": "\u25cf\u200b If the splitting reaches the root and it becomes full, create a new root node, increasing the height of the tree."
    },
    "746": {
        "file": "Extended Notes - Document Databases & MongoDB.pdf",
        "page": 0,
        "chunk": "Document Database: A Flexible NoSQL Solution -\u200b A document database is a type of NoSQL database that stores, retrieves, and manages data in the form of structured documents, typically JSON, BSON, or XML. Unlike relational databases that store data in rows and columns, document databases store data as self-contained, hierarchical documents that can be easily modified and scaled. Key Characteristics of Document Databases 1.\u200b Schema Flexibility -\u200b Unlike relational databases, document databases do not require a predefined schema. -\u200b Each document can have different fields and structures, making it easy to store complex and nested data without requiring multiple tables. -\u200b Schema evolution is seamless, allowing for rapid changes without migration issues. -\u200b Example: In a relational database, this would require multiple tables (users, preferences), but in a document database, it is stored as a single document. 2.\u200b JSON-Based Storage Model -\u200b Data is typically stored in JSON-like formats (e.g., BSON in MongoDB). -\u200b JSON is human-readable, hierarchical, and self-contained, making it a natural choice for modern applications. -\u200b Documents can store arrays and nested structures directly. -\u200b Example: Storing product details in an e-commerce platform 3.\u200b High Scalability -\u200b Document databases are designed for horizontal scaling using sharding and replication. -\u200b Distributed across multiple nodes, making them ideal for large-scale applications. -\u200b Can handle massive workloads and high read/write throughput. 4.\u200b Optimized for Read and Write Performance -\u200b Direct lookup by key enables fast reads. -\u200b Indexes on document fields can optimize queries for performance. -\u200b Unlike relational databases, which require complex joins, document databases store related data in a single document, reducing query complexity. Advantages: -\u200b Flexible schema \u2013 Easily adapt to changing data structures. -\u200b Simplifies development \u2013 JSON format aligns with modern web & mobile applications. -\u200b Fast read/write performance \u2013 No complex joins, data stored"
    },
    "747": {
        "file": "Extended Notes - Document Databases & MongoDB.pdf",
        "page": 0,
        "chunk": "which require complex joins, document databases store related data in a single document, reducing query complexity. Advantages: -\u200b Flexible schema \u2013 Easily adapt to changing data structures. -\u200b Simplifies development \u2013 JSON format aligns with modern web & mobile applications. -\u200b Fast read/write performance \u2013 No complex joins, data stored in a hierarchical structure. -\u200b Scalability \u2013 Can scale out horizontally across multiple servers."
    },
    "748": {
        "file": "Extended Notes - Document Databases & MongoDB.pdf",
        "page": 1,
        "chunk": "Popular Document Databases -\u200b MongoDB \u2013 Most widely used document database, supports indexing, aggregation, and distributed scaling. -\u200b CouchDB \u2013 Focuses on offline sync and distributed data storage. -\u200b Firebase Firestore \u2013 Serverless, real-time NoSQL document store. -\u200b Amazon DocumentDB \u2013 Managed document database service by AWS, compatible with MongoDB. Use Cases of Document Databases 1.\u200b Content Management Systems (CMS) -\u200b Store articles, blog posts, and media content in JSON format. -\u200b Supports nested data, making it easy to retrieve full documents efficiently. 2.\u200b E-Commerce Platforms -\u200b Store product catalogs, orders, and customer profiles in a flexible schema. -\u200b Each product or order can have dynamic fields, avoiding rigid table constraints. 3.\u200b Real-Time Analytics -\u200b Store logs and events in a structured format. -\u200b Quickly analyze large volumes of semi-structured data. What is JSON? -\u200b JSON (JavaScript Object Notation) is a lightweight data-interchange format that is widely used for storing and exchanging data. It is a text-based, human-readable format that is easy for both humans to read and write and machines to parse and generate. Why JSON? -\u200b Simple and Readable \u2013 Uses a clear and structured syntax. -\u200b Language-Independent \u2013 Supported by almost all modern programming languages. -\u200b Efficient for Data Exchange \u2013 Commonly used in web APIs, databases, and configuration files. -\u200b Lightweight \u2013 Minimal syntax overhead compared to XML. JSON is Built on Two Core Structures 1.\u200b Name/Value Pairs (Key-Value Pairs / Objects) -\u200b Represented as a collection of key-value pairs, similar to dictionaries (Python), hashes (Ruby), objects (JavaScript), or associative arrays (PHP, Java). -\u200b Each key is a string, and each value can be a string, number, boolean, array, object, or null. 2.\u200b Ordered Lists of Values (Arrays) -\u200b A list of values, where each value can be of any JSON-supported type. -\u200b Similar to arrays"
    },
    "749": {
        "file": "Extended Notes - Document Databases & MongoDB.pdf",
        "page": 1,
        "chunk": "objects (JavaScript), or associative arrays (PHP, Java). -\u200b Each key is a string, and each value can be a string, number, boolean, array, object, or null. 2.\u200b Ordered Lists of Values (Arrays) -\u200b A list of values, where each value can be of any JSON-supported type. -\u200b Similar to arrays (JavaScript, Java, C), lists (Python), or vectors (C++). Why JSON is a Great Data Interchange Format -\u200b Universally Supported \u2013 Works with nearly all programming languages."
    },
    "750": {
        "file": "Extended Notes - Document Databases & MongoDB.pdf",
        "page": 2,
        "chunk": "-\u200b Easily Convertible \u2013 Can be transformed into objects in languages like JavaScript, Python, Java, and C#. -\u200b Widely Used in Web and APIs \u2013 JSON is the standard format for RESTful APIs. -\u200b Compact and Efficient \u2013 Less verbose than XML, making it faster to parse. Common Uses of JSON -\u200b Configuration files (config.json in applications). -\u200b Web APIs (REST, GraphQL, Firebase Firestore). -\u200b Database Storage (MongoDB, CouchDB). -\u200b Data exchange between web clients and servers (JavaScript fetch requests). Binary JSON? BSON 1.\u200b Binary-encoded serialization of a JSON-like document structure: -\u200b BSON is essentially a binary representation of JSON (JavaScript Object Notation). While JSON is a human-readable text format used to represent data, BSON is a more compact binary format that is optimized for storage and processing efficiency. -\u200b BSON retains the hierarchical structure of JSON documents (objects, arrays, etc.), but encodes them in a binary format for more efficient parsing, storage, and transmission. This makes it particularly useful in systems where performance and memory efficiency are critical, such as in databases. 2.\u200b Supports extended types not part of basic JSON (e.g., Date, BinaryData, etc.): -\u200b BSON is not limited to the basic JSON data types (strings, numbers, booleans, arrays, and objects). It extends JSON to support additional types that are often required in real-world applications. -\u200b Date: BSON includes a native Date type for efficiently representing timestamps, which is particularly useful for databases that need to store and query time-related data. -\u200b BinaryData: BSON also supports a binary data type, which is important for handling raw binary data, such as images or files. -\u200b Other types like ObjectId, Regular Expression, Decimal128, and more can be supported by BSON, making it versatile in handling different data formats. 3.\u200b Lightweight - keeps space overhead to a minimum: -\u200b One of"
    },
    "751": {
        "file": "Extended Notes - Document Databases & MongoDB.pdf",
        "page": 2,
        "chunk": "data type, which is important for handling raw binary data, such as images or files. -\u200b Other types like ObjectId, Regular Expression, Decimal128, and more can be supported by BSON, making it versatile in handling different data formats. 3.\u200b Lightweight - keeps space overhead to a minimum: -\u200b One of the key advantages of BSON over JSON is that it is more compact. JSON stores data as human-readable text, which can lead to a lot of redundant or unnecessary space usage (e.g., extra characters like quotes and braces). BSON, on the other hand, uses a binary format that eliminates this overhead. -\u200b BSON is designed to minimize space while maintaining sufficient metadata to describe the data structure, ensuring efficient storage and transmission. 4.\u200b Traversable - designed to be easily traversed, which is vitally important to a document DB:"
    },
    "752": {
        "file": "Extended Notes - Document Databases & MongoDB.pdf",
        "page": 3,
        "chunk": "-\u200b BSON is optimized for fast traversal and easy access to nested data structures. This is especially important for document-oriented databases (like MongoDB) where documents can have complex, deeply nested structures. -\u200b The format supports indexing of documents, allowing for rapid retrieval of information, and is designed to be traversed efficiently by the database engine. 5.\u200b Efficient - encoding and decoding must be efficient: -\u200b BSON is designed to allow for quick encoding (serialization) and decoding (deserialization) of data. This is essential for high-performance applications, especially when dealing with large volumes of data or when working with real-time systems. -\u200b The binary format allows for faster processing compared to text-based formats like JSON, which requires parsing text into data structures before it can be used. With BSON, the data is already in a binary format, so it can be accessed more quickly. 6.\u200b Supported by many modern programming languages: -\u200b BSON is widely supported by many programming languages, making it a versatile choice for developers working in different environments. Languages such as JavaScript, Python, Java, C++, and others have libraries or native support for working with BSON, particularly in the context of databases like MongoDB. -\u200b The wide support for BSON means that developers can easily integrate BSON into their applications without having to worry about compatibility or performance issues across platforms. XML (eXtensible Markup Language) 1.\u200b Precursor to JSON as a Data Exchange Format -\u200b Before JSON became the dominant format for data exchange, XML was widely used for storing and transmitting structured data between systems. -\u200b XML was particularly common in web services (SOAP-based APIs) and enterprise applications due to its structured, hierarchical nature. -\u200b Unlike JSON, which is lightweight and easy to parse, XML is more verbose, requiring additional tags for defining data structure, which can increase"
    },
    "753": {
        "file": "Extended Notes - Document Databases & MongoDB.pdf",
        "page": 3,
        "chunk": "storing and transmitting structured data between systems. -\u200b XML was particularly common in web services (SOAP-based APIs) and enterprise applications due to its structured, hierarchical nature. -\u200b Unlike JSON, which is lightweight and easy to parse, XML is more verbose, requiring additional tags for defining data structure, which can increase file size and parsing complexity. 2.\u200b XML + CSS \u2192 Web Pages That Separated Content and Formatting -\u200b XML was used in combination with CSS (Cascading Style Sheets) to create structured web pages where content and formatting were separated. -\u200b The idea was similar to the separation of HTML (for structure) and CSS (for styling) in modern web development. -\u200b By applying CSS styles to XML documents, users could control the appearance of data without altering the content itself. 3.\u200b Structurally Similar to HTML, but Tag Set is Extensible -\u200b XML shares a similar hierarchical structure with HTML, using opening (<tag>) and closing (</tag>) tags."
    },
    "754": {
        "file": "Extended Notes - Document Databases & MongoDB.pdf",
        "page": 4,
        "chunk": "-\u200b However, unlike HTML, XML does not have predefined tags. Instead, users can define their own tags, making it highly flexible for different applications. -\u200b This extensibility allows XML to be used for a wide range of applications, including document storage, configuration files, and data representation in various domains (e.g., RSS feeds, SVG graphics, and Microsoft Office file formats). XML-Related Tools/Technologies 1.\u200b XPath \u2013 A Syntax for Retrieving Specific Elements from an XML Document -\u200b XPath (XML Path Language) is used to navigate and select specific parts of an XML document. -\u200b It provides a way to query XML elements, attributes, and text nodes based on their hierarchical structure. -\u200b XPath expressions are commonly used in XSLT transformations and XQuery to filter or manipulate XML data. 2.\u200b XQuery \u2013 The SQL of XML -\u200b XQuery (XML Query Language) is a powerful language for querying and manipulating XML data, similar to how SQL is used for relational databases. -\u200b It allows searching, filtering, and transforming XML documents, making it ideal for XML-based databases and structured data retrieval. 3.\u200b DTD \u2013 Document Type Definition -\u200b DTD (Document Type Definition) defines the structure, elements, and attributes that are allowed in an XML document. -\u200b It ensures data consistency by validating whether an XML document conforms to a predefined structure. 4.\u200b XSLT \u2013 eXtensible Stylesheet Language Transformation -\u200b XSLT (Extensible Stylesheet Language Transformations) is used to convert XML documents into different formats, such as HTML, plain text, or another XML format. -\u200b It enables dynamic content transformation, making XML more useful for web and data processing applications. Why Document Databases? 1.\u200b Addressing the Impedance Mismatch Problem -\u200b The impedance mismatch problem arises when there is a disconnect between how data is structured in object-oriented (OO) programming languages and how it must be stored in"
    },
    "755": {
        "file": "Extended Notes - Document Databases & MongoDB.pdf",
        "page": 4,
        "chunk": "dynamic content transformation, making XML more useful for web and data processing applications. Why Document Databases? 1.\u200b Addressing the Impedance Mismatch Problem -\u200b The impedance mismatch problem arises when there is a disconnect between how data is structured in object-oriented (OO) programming languages and how it must be stored in relational databases. -\u200b Object-Oriented Programming (OOP): Objects in languages like Python, Java, or C# are hierarchical and support complex structures, including inheritance, composition, and nested types. -\u200b Relational Databases (RDBs): RDBs require data to be stored in flat, structured tables with rows and columns. Relationships between objects must be manually established through foreign keys and joins."
    },
    "756": {
        "file": "Extended Notes - Document Databases & MongoDB.pdf",
        "page": 5,
        "chunk": "-\u200b This means that saving an object in an RDB requires breaking it down into multiple tables and reassembling it when retrieving data. -\u200b In a relational database, we must store this data as multiple tables: -\u200b BlogPost Table (ID, Title, Author), Tags Table (BlogPostID, TagName), Comments Table (BlogPostID, User, Text) -\u200b To retrieve the full blog post, we would need multiple JOIN queries across these tables, which can be inefficient and complex to manage. 2.\u200b The Structure of a Document is Self-Describing -\u200b In a relational database, we need a predefined schema that specifies column names, data types, and relationships before inserting any data. -\u200b Document databases are schema-less, meaning that each document can have its own structure without requiring predefined schemas. -\u200b This flexibility is useful when dealing with evolving data models (e.g., adding new fields without altering the database schema). 3.\u200b Well-Aligned with JSON/XML Transport Layers -\u200b Many modern applications, particularly web APIs, use JSON or XML to send and receive data. -\u200b Since document databases store data in JSON-like formats (JSON, BSON, or XML), they integrate seamlessly with APIs and web services. -\u200b No need for conversion between objects, JSON, and database storage, unlike relational databases that require object-relational mapping (ORM) tools. MongoDB MongoDB is a widely used NoSQL document database designed for scalability, flexibility, and performance. It emerged as a response to the limitations of relational databases when handling large-scale, high-throughput applications. 1.\u200b Origins: Why MongoDB Was Created -\u200b MongoDB was founded in 2007 by Dwight Merriman, Eliot Horowitz, and Kevin Ryan, who were part of DoubleClick, a digital advertising company later acquired by Google. -\u200b DoubleClick needed to serve over 400,000 ads per second, but traditional relational databases struggled with the high-speed, high-volume data needs. -\u200b Challenges with relational databases: -\u200b Scalability: RDBMS struggled"
    },
    "757": {
        "file": "Extended Notes - Document Databases & MongoDB.pdf",
        "page": 5,
        "chunk": "Dwight Merriman, Eliot Horowitz, and Kevin Ryan, who were part of DoubleClick, a digital advertising company later acquired by Google. -\u200b DoubleClick needed to serve over 400,000 ads per second, but traditional relational databases struggled with the high-speed, high-volume data needs. -\u200b Challenges with relational databases: -\u200b Scalability: RDBMS struggled with horizontal scaling for high traffic loads. -\u200b Complex queries: SQL-based joins became a bottleneck at massive scales. -\u200b Schema rigidity: Frequent schema changes caused downtime and required migrations. -\u200b Seeing these challenges, they set out to build a new kind of database designed for modern applications."
    },
    "758": {
        "file": "Extended Notes - Document Databases & MongoDB.pdf",
        "page": 6,
        "chunk": "2.\u200b Why the Name \"MongoDB\"? -\u200b \"MongoDB\" comes from \"Humongous Database\", emphasizing its ability to handle large-scale data efficiently. -\u200b Unlike traditional databases, MongoDB uses a flexible document model instead of rigid tables and schemas. 3.\u200b Key Innovations of MongoDB (MongoDB introduced several features that made it a preferred choice for modern applications): -\u200b Document-Oriented Storage \u2013 Stores data in JSON-like BSON (Binary JSON) format, making it more natural for object-oriented programming. -\u200b Schema Flexibility \u2013 No predefined schema is required; each document can have different fields. -\u200b Horizontal Scalability \u2013 Uses sharding to distribute data across multiple servers, handling high traffic and large datasets. -\u200b High Performance \u2013 Avoids complex joins and supports indexing for fast lookups. -\u200b Replication & High Availability \u2013 Uses replica sets to ensure fault tolerance and automatic failover. 4.\u200b MongoDB Atlas: DocumentDB as a Service (2016) -\u200b In 2016, MongoDB released MongoDB Atlas, a fully managed cloud database service that made it easier to deploy and scale MongoDB without managing infrastructure. -\u200b Benefits of MongoDB Atlas: -\u200b Automated scaling and performance tuning. -\u200b Multi-cloud support (AWS, Azure, GCP). -\u200b Built-in security and backup features. -\u200b Integration with modern data tools and analytics platforms. 5.\u200b MongoDB's Role in Modern Applications (MongoDB has become a dominant database for): -\u200b Big Data & Analytics \u2013 Handles massive data volumes efficiently. -\u200b Real-Time Applications \u2013 Used in ad tech, IoT, and financial services. -\u200b Microservices & Cloud-Based Systems \u2013 Provides a flexible and scalable data backend. -\u200b Content Management & E-Commerce \u2013 Supports complex, dynamic data structures like catalogs and user-generated content. MongoDB Documents - Schema Flexibility & Dynamic Structure One of MongoDB\u2019s biggest advantages is its schema-less nature, meaning that documents in a collection do not require a predefined schema. This allows for high flexibility and makes"
    },
    "759": {
        "file": "Extended Notes - Document Databases & MongoDB.pdf",
        "page": 6,
        "chunk": "Management & E-Commerce \u2013 Supports complex, dynamic data structures like catalogs and user-generated content. MongoDB Documents - Schema Flexibility & Dynamic Structure One of MongoDB\u2019s biggest advantages is its schema-less nature, meaning that documents in a collection do not require a predefined schema. This allows for high flexibility and makes MongoDB well-suited for applications with evolving data structures. 1.\u200b What is a MongoDB Document? -\u200b In MongoDB, data is stored as documents in a JSON-like BSON (Binary JSON) format."
    },
    "760": {
        "file": "Extended Notes - Document Databases & MongoDB.pdf",
        "page": 7,
        "chunk": "-\u200b Each document is a self-contained unit of data, similar to an object in object-oriented programming. -\u200b Documents are grouped into collections, which are analogous to tables in relational databases. -\u200b Unlike relational databases, MongoDB does not enforce a fixed schema for documents. 2.\u200b No Predefined Schema: Why is this Useful? -\u200b Flexibility \u2013 Developers can add, remove, or modify fields in documents without altering a predefined schema. -\u200b Faster Iteration \u2013 Applications can evolve without requiring database migrations. -\u200b Different Data Structures \u2013 Documents in the same collection can have different fields and types. 3.\u200b When is Schema Flexibility Beneficial? -\u200b MongoDB\u2019s schema-less approach is ideal for: 1.\u200b Applications with frequently changing data models (e.g., startups, evolving projects). 2.\u200b Content management systems (CMS) where different types of content require different fields. 3.\u200b IoT and event logging where incoming data structures may vary. 4.\u200b User profiles & e-commerce, where different users or products may have different attributes. RDBMS MongoDB Database Database Table/View Collection Row Document Column Field Index Index Join Embedded Document Foreign Key Reference MongoDB Features: Why It\u2019s a Powerful NoSQL Database MongoDB offers a range of features that make it a high-performance, scalable, and flexible document database. Let\u2019s break down some of its key capabilities: 1.\u200b Rich Query Support -\u200b MongoDB provides powerful and flexible querying capabilities beyond simple key-value lookups. -\u200b It supports CRUD operations (Create, Read, Update, Delete) with complex query options, including:"
    },
    "761": {
        "file": "Extended Notes - Document Databases & MongoDB.pdf",
        "page": 8,
        "chunk": "-\u200b Filtering \u2013 Find documents using field values, ranges, and conditions. -\u200b Aggregation Framework \u2013 Perform complex data processing like grouping, filtering, and transformations (similar to SQL\u2019s GROUP BY). -\u200b Full-Text Search \u2013 Built-in support for text indexing and searching. -\u200b Geospatial Queries \u2013 Search by location, distance, or geospatial relationships. 2.\u200b Indexing -\u200b MongoDB supports primary and secondary indexes to improve query performance. -\u200b Without indexes, MongoDB would have to scan every document in a collection to find a match. -\u200b Types of Indexes: -\u200b Single Field Index \u2013 Speeds up lookups on a specific field. -\u200b Compound Index \u2013 Optimized searches across multiple fields. -\u200b Text Index \u2013 Enables efficient full-text search. -\u200b Geospatial Index \u2013 Supports location-based queries. 3.\u200b Replication (High Availability & Fault Tolerance) -\u200b MongoDB ensures high availability by supporting replica sets, which automatically maintain multiple copies of data across different servers. -\u200b How it works: -\u200b One primary node handles all writes. -\u200b Multiple secondary nodes replicate the data in real-time. -\u200b If the primary fails, MongoDB automatically elects a new primary (automatic failover). -\u200b This ensures zero downtime in case of server failures. 4.\u200b Load Balancing (Sharding for Horizontal Scaling) -\u200b MongoDB has built-in load balancing through sharding, which allows databases to scale horizontally. -\u200b Why use sharding? 1.\u200b Handles large datasets by distributing data across multiple servers. 2.\u200b Supports high-throughput applications with heavy read/write operations. 3.\u200b Prevents bottlenecks by dividing query loads among different servers. -\u200b How Sharding Works: 1.\u200b Data is split into chunks based on a shard key (e.g., userID). 2.\u200b Chunks are distributed across multiple servers (shards). 3.\u200b A Mongos query router directs queries to the correct shard. 4.\u200b As data grows, new shards can be added dynamically. MongoDB Versions: Understanding the Different Offerings MongoDB offers several versions of"
    },
    "762": {
        "file": "Extended Notes - Document Databases & MongoDB.pdf",
        "page": 8,
        "chunk": "split into chunks based on a shard key (e.g., userID). 2.\u200b Chunks are distributed across multiple servers (shards). 3.\u200b A Mongos query router directs queries to the correct shard. 4.\u200b As data grows, new shards can be added dynamically. MongoDB Versions: Understanding the Different Offerings MongoDB offers several versions of its database, each catering to different needs and use cases. Here\u2019s a breakdown of the three primary versions: MongoDB Atlas, MongoDB Enterprise, and MongoDB Community:"
    },
    "763": {
        "file": "Extended Notes - Document Databases & MongoDB.pdf",
        "page": 9,
        "chunk": "1.\u200b MongoDB Atlas -\u200b Fully Managed MongoDB Service: MongoDB Atlas is a Database as a Service (DBaaS), providing a fully managed cloud-based MongoDB experience. -\u200b What it offers: -\u200b Fully managed \u2013 MongoDB Atlas handles all database management tasks such as provisioning, monitoring, scaling, and backups. -\u200b Multi-Cloud \u2013 Available across all major cloud providers (AWS, Azure, Google Cloud), allowing for flexible deployment options. -\u200b Automated Scaling \u2013 Atlas can automatically scale resources as demand increases. -\u200b Built-in Security \u2013 Offers data encryption at rest, VPC peering, advanced access controls, and more. -\u200b Performance Optimization \u2013 Includes real-time performance monitoring and alerts to help optimize database performance. -\u200b Backup and Disaster Recovery \u2013 Automated backups with easy restoration options. -\u200b Ideal Use Cases: -\u200b MongoDB Atlas is perfect for organizations that want to focus on development and scaling without worrying about managing infrastructure. It is commonly used by startups, enterprises, and developers who need cloud-native database solutions. -\u200b Key Features of MongoDB Atlas: -\u200b Global distribution \u2013 Deploy MongoDB clusters in multiple regions for reduced latency and better redundancy. -\u200b Data Insights \u2013 Offers built-in analytics tools like MongoDB Charts to visualize your data directly. -\u200b Serverless Options \u2013 MongoDB Atlas also offers serverless databases, where you only pay for the compute and storage you use. 2.\u200b MongoDB Enterprise -\u200b Subscription-Based, Self-Managed Version: MongoDB Enterprise is the enterprise-grade, subscription-based version of MongoDB, designed for large organizations with self-managed infrastructure needs. -\u200b What it offers: -\u200b Advanced Security \u2013 Includes LDAP integration, Kerberos authentication, and auditing capabilities. -\u200b Commercial Support \u2013 Includes 24/7 enterprise-level support and professional services. -\u200b Increased Scalability \u2013 MongoDB Enterprise is optimized for larger, more complex deployments with advanced features such as sharding and replication. -\u200b Additional Tools \u2013 Comes with additional features such as MongoDB Ops"
    },
    "764": {
        "file": "Extended Notes - Document Databases & MongoDB.pdf",
        "page": 9,
        "chunk": "integration, Kerberos authentication, and auditing capabilities. -\u200b Commercial Support \u2013 Includes 24/7 enterprise-level support and professional services. -\u200b Increased Scalability \u2013 MongoDB Enterprise is optimized for larger, more complex deployments with advanced features such as sharding and replication. -\u200b Additional Tools \u2013 Comes with additional features such as MongoDB Ops Manager (for automation, backups, and monitoring) and MongoDB Compass (for graphical data exploration)."
    },
    "765": {
        "file": "Extended Notes - Document Databases & MongoDB.pdf",
        "page": 10,
        "chunk": "-\u200b Compliance \u2013 Features designed to meet compliance requirements like HIPAA, SOC2, and more. -\u200b Ideal Use Cases: -\u200b MongoDB Enterprise is typically used by large organizations that require extra security, compliance, and support for mission-critical applications. It is suitable for companies that prefer to manage their own infrastructure while benefiting from additional tools and services. -\u200b Key Features of MongoDB Enterprise: -\u200b MongoDB Ops Manager \u2013 Automates operational tasks such as provisioning, backups, and performance tuning. -\u200b Commercial Licensing \u2013 Offers full access to MongoDB\u2019s proprietary features and the ability to deploy on-premises or in the cloud. 3.\u200b MongoDB Community -\u200b Free, Source-Available Version: MongoDB Community is the open-source version of MongoDB, which is completely free to use and self-managed. -\u200b What it offers: -\u200b Core MongoDB Features \u2013 The Community edition provides all of MongoDB\u2019s core features, such as collections, documents, indexing, and queries. -\u200b Self-Managed \u2013 MongoDB Community requires users to manage their own infrastructure, including installation, scaling, backups, and monitoring. -\u200b Source-Available \u2013 The source code for MongoDB Community is publicly available, allowing users to inspect and modify it if needed. -\u200b Community Support \u2013 MongoDB Community users can rely on online documentation and community forums for support. -\u200b Ideal Use Cases: -\u200b MongoDB Community is suitable for developers, small teams, or anyone working on personal projects or proof-of-concept applications. It\u2019s a great choice for startups, educational purposes, and low-cost environments where enterprise-grade support and features are not required. -\u200b Key Features of MongoDB Community: -\u200b Open Source \u2013 Fully free and open-source. -\u200b No Commercial Features \u2013 Does not include advanced features such as LDAP integration, advanced monitoring, or commercial support."
    },
    "766": {
        "file": "Extended Notes - Document Databases & MongoDB.pdf",
        "page": 10,
        "chunk": "Open Source \u2013 Fully free and open-source. -\u200b No Commercial Features \u2013 Does not include advanced features such as LDAP integration, advanced monitoring, or commercial support."
    },
    "767": {
        "file": "Extended Notes - UCI AVL Trees.pdf",
        "page": 0,
        "chunk": "ICS 46 Spring 2022 | News | Course Reference | Schedule | Project Guide | Notes and Examples | Reinforcement Exercises | Grade Calculator | About Alex ICS 46 Spring 2022 Notes and Examples: AVL Trees Why we must care about binary search tree balancing We've seen previously that the performance characteristics of binary search trees can vary rather wildly, and that they're mainly dependent on the shape of the tree, with the height of the tree being the key determining factor. By definition, binary search trees restrict what keys are allowed to present in which nodes \u2014 smaller keys have to be in left subtrees and larger keys in right subtrees \u2014 but they specify no restriction on the tree's shape, meaning that both of these are perfectly legal binary search trees containing the keys 1, 2, 3, 4, 5, 6, and 7. Yet, while both of these are legal, one is better than the other, because the height of the first tree (called a perfect binary tree) is smaller than the height of the second (called a degenerate tree). These two shapes represent the two extremes \u2014 the best and worst possible shapes for a binary search tree containing seven keys. Of course, when all you have is a very small number of keys like this, any shape will do. But as the number of keys grows, the distinction between these two tree shapes becomes increasingly vital. What's more, the degenerate shape isn't even necessarily a rare edge case: It's what you get when you start with an empty tree and add keys that are already in order, which is a surprisingly common scenario in real-world programs. For example, one very obvious algorithm for generating unique integer keys \u2014 when all you care about is that they're unique"
    },
    "768": {
        "file": "Extended Notes - UCI AVL Trees.pdf",
        "page": 0,
        "chunk": "case: It's what you get when you start with an empty tree and add keys that are already in order, which is a surprisingly common scenario in real-world programs. For example, one very obvious algorithm for generating unique integer keys \u2014 when all you care about is that they're unique \u2014 is to generate them sequentially. What's so bad about a degenerate tree, anyway? Just looking at a picture of a degenerate tree, your intuition should already be telling you that something is amiss. In particular, if you tilt your head 45 degrees to the right, they look just like linked lists; that perception is no accident, as they behave like them, too (except that they're more complicated, to boot!). From a more analytical perspective, there are three results that should give us pause: Every time you perform a lookup in a degenerate binary search tree, it will take O(n) time, because it's possible that you'll have to reach every node in the tree before you're done. As n grows, this is a heavy burden to bear. If you implement your lookup recursively, you might also be using O(n) memory, too, as you might end up with as many as n frames on your run-time stack \u2014 one for every recursive call. There are ways to mitigate this \u2014 for example, some kinds of carefully-written recursion (in some programming languages, including C++) can avoid run-time stack growth as you recurse \u2014 but it's still a sign of potential trouble. The time it will take you to build the degenerate tree will also be prohibitive. If you start with an empty binary search tree and add keys to it in order, how long does it take to do it? The first key you add will go directly to the root. You"
    },
    "769": {
        "file": "Extended Notes - UCI AVL Trees.pdf",
        "page": 0,
        "chunk": "time it will take you to build the degenerate tree will also be prohibitive. If you start with an empty binary search tree and add keys to it in order, how long does it take to do it? The first key you add will go directly to the root. You could think of this as taking a single step: creating the node. The second key you add will require you to look at the root node, then take one step to the right. You could think of this as taking two steps. Each subsequent key you add will require one more step than the one before it. The total number of steps it would take to add n keys would be determined by the sum 1 + 2 + 3 + ... + n. This sum, which we'll see several times throughout this course, is equal to n(n + 1) / 2. So, the total number of steps to build the entire tree would be \u0398(n2). Overall, when n gets large, the tree would be hideously expensive to build, and then every subsequent search would be painful, as well. So this, in general, is a situation we need to be sure to avoid, or else we should probably consider a data structure other than a binary search tree; the worst case is simply too much of a burden to bear if n might get large. But if we can find a way to control the tree's shape more carefully, to force it to remain more balanced, we'll be fine. The question, of course, is how to do it, and, as importantly, whether we can do it while keeping the cost low enough that it doesn't outweigh the benefit. Aiming for perfection The best goal for us to shoot for would"
    },
    "770": {
        "file": "Extended Notes - UCI AVL Trees.pdf",
        "page": 0,
        "chunk": "force it to remain more balanced, we'll be fine. The question, of course, is how to do it, and, as importantly, whether we can do it while keeping the cost low enough that it doesn't outweigh the benefit. Aiming for perfection The best goal for us to shoot for would be to maintain perfection. In other words, every time we insert a key into our binary search tree, it would ideally still be a perfect binary tree, in which case we'd know that the height of the tree would always be \u0398(log n), with a commensurate effect on performance. However, when we consider this goal, a problem emerges almost immediately. The following are all perfect binary trees, by definition: The perfect binary trees pictured above have 1, 3, 7, and 15 nodes respectively, and are the only possible perfect shapes for binary trees with that number of nodes. The problem, though, lies in the fact that there is no valid perfect binary tree with 2 nodes, or with 4, 5, 6, 8, 9, 10, 11, 12, 13, or 14 nodes. So, generally, it's impossible for us to guarantee that a binary search tree will always be \"perfect,\" by our definition, because there's simply no way to represent most numbers of keys. So, first thing's first: We'll need to relax our definition of \"perfection\" to accommodate every possible number of keys we might want to store."
    },
    "771": {
        "file": "Extended Notes - UCI AVL Trees.pdf",
        "page": 1,
        "chunk": "Complete binary trees A somewhat more relaxed notion of \"perfection\" is something called a complete binary tree, which is defined as follows. A complete binary tree of height h is a binary tree where: If h = 0, its left and right subtrees are empty. If h > 0, one of two things is true: The left subtree is a perfect binary tree of height h \u2212 1 and the right subtree is a complete binary tree of height h \u2212 1 The left subtree is a complete binary tree of height h \u2212 1 and the right subtree is a perfect binary tree of height h \u2212 2 That can be a bit of a mind-bending definition, but it actually leads to a conceptually simple result: On every level of a complete binary tree, every node that could possibly be present will be, except the last level might be missing nodes, but if it is missing nodes, the nodes that are there will be as far to the left as possible. The following are all complete binary trees: Furthermore, these are the only possible complete binary trees with these numbers of nodes in them; any other arrangement of, say, 6 keys besides the one shown above would violate the definition. We've seen that the height of a perfect binary tree is \u0398(log n). It's not a stretch to see that the height a complete binary tree will be \u0398(log n), as well, and we'll accept that via our intuition for now and proceed. All in all, a complete binary tree would be a great goal for us to attain: If we could keep the shape of our binary search trees complete, we would always have binary search trees with height \u0398(log n). The cost of maintaining completeness The trouble,"
    },
    "772": {
        "file": "Extended Notes - UCI AVL Trees.pdf",
        "page": 1,
        "chunk": "for now and proceed. All in all, a complete binary tree would be a great goal for us to attain: If we could keep the shape of our binary search trees complete, we would always have binary search trees with height \u0398(log n). The cost of maintaining completeness The trouble, of course, is that we need an algorithm for maintaining completeness. And before we go to the trouble of trying to figure one out, we should consider whether it's even worth our time. What can we deduce about the cost of maintaining completeness, even if we haven't figured out an algorithm yet? One example demonstrates a very big problem. Suppose we had the binary search tree on the left \u2014 which is complete, by our definition \u2014 and we wanted to insert the key 1 into it. If so, we would need an algorithm that would transform the tree on the left into the tree on the right. The tree on the right is certainly complete, so this would be the outcome we'd want. But consider what it would take to do it. Every key in the tree had to move! So, no matter what algorithm we used, we would still have to move every key. If there are n keys in the tree, that would take \u03a9(n) time \u2014 moving n keys takes at least linear time, even if you have the best possible algorithm for moving them; the work still has to get done. So, in the worst case, maintaining completeness after a single insertion requires \u03a9(n) time. Unfortunately, this is more time than we ought to be spending on maintaining balance. This means we'll need to come up with a compromise; as is often the case when we learn or design algorithms, our willingness to tolerate an"
    },
    "773": {
        "file": "Extended Notes - UCI AVL Trees.pdf",
        "page": 1,
        "chunk": "case, maintaining completeness after a single insertion requires \u03a9(n) time. Unfortunately, this is more time than we ought to be spending on maintaining balance. This means we'll need to come up with a compromise; as is often the case when we learn or design algorithms, our willingness to tolerate an imperfect result that's still \"good enough\" for our uses will often lead to an algorithm that is much faster than one that achieves a perfect result. So what would a \"good enough\" result be? What is a \"good\" balance condition Our overall goal is for lookups, insertions, and removals from a binary search tree to require O(log n) time in every case, rather than letting them degrade to a worst-case behavior of O(n). To do that, we need to decide on a balance condition, which is to say that we need to understand what shape is considered well-enough balanced for our purposes, even if not perfect. A \"good\" balance condition has two properties: The height of a binary search tree meeting the condition is \u0398(log n). It takes O(log n) time to re-balance the tree on insertions and removals. In other words, it guarantees that the height of the tree is still logarithmic, which will give us logarithmic-time lookups, and the time spent re-balancing won't exceed the logarithmic time we would otherwise spend on an insertion or removal when the tree has logarithmic height. The cost won't outweigh the benefit. Coming up with a balance condition like this on our own is a tall task, but we can stand on the shoulders of the giants who came before us, with the definition above helping to guide us toward an understanding of whether we've found what we're looking for. A compromise: AVL trees There are a few well-known approaches for maintaining"
    },
    "774": {
        "file": "Extended Notes - UCI AVL Trees.pdf",
        "page": 1,
        "chunk": "own is a tall task, but we can stand on the shoulders of the giants who came before us, with the definition above helping to guide us toward an understanding of whether we've found what we're looking for. A compromise: AVL trees There are a few well-known approaches for maintaining binary search trees in a state of near-balance that meets our notion of a \"good\" balance condition. One of them is called an AVL tree, which we'll explore here. Others, which are outside the scope of this course, include red-black trees (which meet our definition of \"good\") and splay trees (which don't always meet our definition of \"good\", but do meet it on an amortized basis), but we'll stick with the one solution to the problem for now. AVL trees AVL trees are what you might called \"nearly balanced\" binary search trees. While they certainly aren't as perfectly-balanced as possible, they nonetheless achieve the goals we've decided on: maintaining logarithmic height at no more than logarithmic cost. So, what makes a binary search tree \"nearly balanced\" enough to be considered an AVL tree? The core concept is embodied by something called the AVL property. We say that a node in a binary search tree has the AVL property if the heights of its left and right subtrees differ by no more than 1. In other words, we tolerate a certain amount of imbalance \u2014 heights of subtrees can be slightly different, but no more than that \u2014 in hopes that we can more efficiently maintain it. Since we're going to be comparing heights of subtrees, there's one piece of background we need to consider. Recall that the height of a tree is the length of its longest path. By definition, the height of a tree with just a root node"
    },
    "775": {
        "file": "Extended Notes - UCI AVL Trees.pdf",
        "page": 1,
        "chunk": "we can more efficiently maintain it. Since we're going to be comparing heights of subtrees, there's one piece of background we need to consider. Recall that the height of a tree is the length of its longest path. By definition, the height of a tree with just a root node (and empty subtrees) would then be zero. But what about a tree that's totally empty? To maintain a clear pattern, relative to other tree heights, we'll say that the height of an empty tree is -1. This means that a node with, say, a childless left child and no right child would still be considered balanced. This leads us, finally, to the definition of an AVL tree:"
    },
    "776": {
        "file": "Extended Notes - UCI AVL Trees.pdf",
        "page": 2,
        "chunk": "An AVL tree is a binary search tree in which all nodes have the AVL property. Below are a few binary trees, two of which are AVL and two of which are not. The thing to keep in mind about AVL is that it's not a matter of squinting at a tree and deciding whether it \"looks\" balanced. There's a precise definition, and the two trees above that don't meet that definition fail to meet it because they each have at least one node (marked in the diagrams by a dashed square) that doesn't have the AVL property. AVL trees, by definition, are required to meet the balance condition after every operation; every time you insert or remove a key, every node in the tree should have the AVL property. To meet that requirement, we need to restructure the tree periodically, essentially detecting and correcting imbalance whenever and wherever it happens. To do that, we need to rearrange the tree in ways that improve its shape without losing the essential ordering property of a binary search tree: smaller keys toward the left, larger ones toward the right. Rotations Re-balancing of AVL trees is achieved using what are called rotations, which, when used at the proper times, efficiently improve the shape of the tree by altering a handful of pointers. There are a few kinds of rotations; we should first understand how they work, then focus our attention on when to use them. The first kind of rotation is called an LL rotation, which takes the tree on the left and turns it into the tree on the right. The circle with A and B written in them are each a single node containing a single key; the triangles with T1, T2, and T3 written in them are arbitrary subtrees, which"
    },
    "777": {
        "file": "Extended Notes - UCI AVL Trees.pdf",
        "page": 2,
        "chunk": "LL rotation, which takes the tree on the left and turns it into the tree on the right. The circle with A and B written in them are each a single node containing a single key; the triangles with T1, T2, and T3 written in them are arbitrary subtrees, which may be empty or may contain any number of nodes (but which are, themselves, binary search trees). It's important to remember that both of these trees \u2014 before and after \u2014 are binary search trees; the rotation doesn't harm the ordering of the keys in nodes, because the subtrees T1, T2, and T3 maintain the appropriate positions relative to the keys A and B: All keys in T1 are smaller than A. All keys in T2 are larger than A and smaller than B. All keys in T3 are larger than B. Performing this rotation would be a simple matter of adjusting a few pointers \u2014 notably, a constant number of pointers, no matter how many nodes are in the tree, which means that this rotation would run in \u0398(1) time: B's parent would now point to A where it used to point to B A's right child would now be B instead of the root of T2 B's left child would now be the root of T2 instead of A A second kind of rotation is an RR rotation, which makes a similar adjustment. Note that an RR rotation is the mirror image of an LL rotation. A third kind of rotation is an LR rotation, which makes an adjustment that's slightly more complicated. An LR rotation requires five pointer updates instead of three, but this is still a constant number of changes and runs in \u0398(1) time. Finally, there is an RL rotation, which is the mirror image"
    },
    "778": {
        "file": "Extended Notes - UCI AVL Trees.pdf",
        "page": 2,
        "chunk": "kind of rotation is an LR rotation, which makes an adjustment that's slightly more complicated. An LR rotation requires five pointer updates instead of three, but this is still a constant number of changes and runs in \u0398(1) time. Finally, there is an RL rotation, which is the mirror image of an LR rotation."
    },
    "779": {
        "file": "Extended Notes - UCI AVL Trees.pdf",
        "page": 3,
        "chunk": "Once we understand the mechanics of how rotations work, we're one step closer to understanding AVL trees. But these rotations aren't arbitrary; they're used specifically to correct imbalances that are detected after insertions or removals. An insertion algorithm Inserting a key into an AVL tree starts out the same way as insertion into a binary search tree: Perform a lookup. If you find the key already in the tree, you're done, because keys in a binary search tree must be unique. When the lookup terminates without the key being found, add a new node in the appropriate leaf position where the lookup ended. The problem is that adding the new node introduced the possibility of an imbalance. For example, suppose we started with this AVL tree: and then we inserted the key 35 into it. A binary search tree insertion would give us this as a result: But this resulting tree is not an AVL tree, because the node containing the key 40 does not have the AVL property, because the difference in the heights of its subtrees is 2. (Its left subtree has height 1, its right subtree \u2014 which is empty \u2014 has height -1.) What can we do about it? The answer lies in the following algorithm, which we perform after the normal insertion process: Work your way back up the tree from the position where you just added a node. (This could be quite simple if the insertion was done recursively.) Compare the heights of the left and right subtrees of each node. When they differ by more than 1, choose a rotation that will fix the imbalance. Note that comparing the heights of the left and right subtrees would be quite expensive if you didn't already know what they were. The solution to this problem"
    },
    "780": {
        "file": "Extended Notes - UCI AVL Trees.pdf",
        "page": 3,
        "chunk": "left and right subtrees of each node. When they differ by more than 1, choose a rotation that will fix the imbalance. Note that comparing the heights of the left and right subtrees would be quite expensive if you didn't already know what they were. The solution to this problem is for each node to store its height (i.e., the height of the subtree rooted there). This can be cheaply updated after every insertion or removal as you unwind the recursion. The rotation is chosen considering the two links along the path below the node where the imbalance is, heading back down toward where you inserted a node. (If you were wondering where the names LL, RR, LR, and RL come from, this is the answer to that mystery.) If the two links are both to the left, perform an LL rotation rooted where the imbalance is. If the two links are both to the right, perform an RR rotation rooted where the imbalance is. If the first link is to the left and the second is to the right, perform an LR rotation rooted where the imbalance is. If the first link is to the right and the second is to the left, perform an RL rotation rooted where the imbalance is. It can be shown that any one of these rotations \u2014 LL, RR, LR, or RL \u2014 will correct any imbalance brought on by inserting a key. In this case, we'd perform an LR rotation \u2014 the first two links leading from 40 down toward 35 are a Left and a Right \u2014 rooted at 40, which would correct the imbalance, and the tree would be rearranged to look like this: Compare this to the diagram describing an LR rotation: The node containing 40 is C The"
    },
    "781": {
        "file": "Extended Notes - UCI AVL Trees.pdf",
        "page": 3,
        "chunk": "first two links leading from 40 down toward 35 are a Left and a Right \u2014 rooted at 40, which would correct the imbalance, and the tree would be rearranged to look like this: Compare this to the diagram describing an LR rotation: The node containing 40 is C The node containing 30 is A The node containing 35 is B The (empty) left subtree of the node containing 30 is T1"
    },
    "782": {
        "file": "Extended Notes - UCI AVL Trees.pdf",
        "page": 4,
        "chunk": "The (empty) left subtree of the node containing 35 is T2 The (empty) right subtree of the node containing 35 is T3 The (empty) right subtree of the node containing 40 is T4 After the rotation, we see what we'd expect: The node B, which in our example contained 35, is now the root of the newly-rotated subtree The node A, which in our example contained 30, is now the left child of the root of the newly-rotated subtree The node C, which in our example contained 40, is now the right child of the root of the newly-rotated subtree The four subtrees T1, T2, T3, and T4 were all empty, so they are still empty. Note, too, that the tree is more balanced after the rotation than it was before. This is no accident; a single rotation (LL, RR, LR, or RL) is all that's necessary to correct an imbalance introduced by the insertion algorithm. A removal algorithm Removals are somewhat similar to insertions, in the sense that you would start with the usual binary search tree removal algorithm, then find and correct imbalances while the recursion unwinds. The key difference is that removals can require more than one rotation to correct imbalances, but will still only require rotations on the path back up to the root from where the removal occurred \u2014 so, generally, O(log n) rotations. Asymptotic analysis The key question here is What is the height of an AVL tree with n nodes? If the answer is \u0398(log n), then we can be certain that lookups, insertions, and removals will take O(log n) time. How can we be so sure? Lookups would be O(log n) because they're the same as they are in a binary search tree that doesn't have the AVL property. If the height"
    },
    "783": {
        "file": "Extended Notes - UCI AVL Trees.pdf",
        "page": 4,
        "chunk": "is \u0398(log n), then we can be certain that lookups, insertions, and removals will take O(log n) time. How can we be so sure? Lookups would be O(log n) because they're the same as they are in a binary search tree that doesn't have the AVL property. If the height of the tree is \u0398(log n), lookups will run in O(log n) time. Insertions and removals, despite being slightly more complicated in an AVL tree, do their work by traversing a single path in the tree \u2014 potentially all the way down to a leaf position, then all the way back up. If the length of the longest path \u2014 that's what the height of a tree is! \u2014 is \u0398(log n), then we know that none of these paths is longer than that, so insertions and removals will take O(log n) time. So we're left with that key question. What is the height of an AVL tree with n nodes? (If you're not curious, you can feel free to just assume this; if you want to know more, keep reading.) What is the height of an AVL tree with n nodes? (Optional) The answer revolves around noting how many nodes, at minimum, could be in a binary search tree of height n and still have it be an AVL tree. It turns out AVL trees of height n \u2265 2 that have the minimum number of nodes in them all share a similar property: The AVL tree with height h \u2265 2 with the minimum number of nodes consists of a root node with two subtrees, one of which is an AVL tree with height h \u2212 1 with the minimum number of nodes, the other of which is an AVL tree with height h \u2212 2 with the"
    },
    "784": {
        "file": "Extended Notes - UCI AVL Trees.pdf",
        "page": 4,
        "chunk": "h \u2265 2 with the minimum number of nodes consists of a root node with two subtrees, one of which is an AVL tree with height h \u2212 1 with the minimum number of nodes, the other of which is an AVL tree with height h \u2212 2 with the minimum number of nodes. Given that observation, we can write a recurrence that describes the number of nodes, at minimum, in an AVL tree of height h. M(0) = 1 When height is 0, minimum number of nodes is 1 (a root node with no children) M(1) = 2 When height is 1, minimum number of nodes is 2 (a root node with one child and not the other) M(h) = 1 + M(h - 1) + M(h - 2) While the repeated substitution technique we learned previously isn't a good way to try to solve this particular recurrence, we can prove something interesting quite easily. We know for sure that AVL trees with larger heights have a bigger minimum number of nodes than AVL trees with smaller heights \u2014 that's fairly self-explanatory \u2014 which means that we can be sure that 1 + M(h \u2212 1) \u2265 M(h \u2212 2). Given that, we can conclude the following: M(h) \u2265 2M(h - 2) We can then use the repeated substitution technique to determine a lower bound for this recurrence: M(h) \u2265 2M(h - 2) \u2265 2(2M(h - 4)) \u2265 4M(h - 4) \u2265 4(2M(h - 6)) \u2265 8M(h - 6) ... \u2265 2jM(h - 2j) We could prove this by induction on j, but we'll accept it on faith let j = h/2 \u2265 2h/2M(h - h) \u2265 2h/2M(0) M(h) \u2265 2h/2 So, we've shown that the minimum number of nodes that can be present in an AVL tree of"
    },
    "785": {
        "file": "Extended Notes - UCI AVL Trees.pdf",
        "page": 4,
        "chunk": "... \u2265 2jM(h - 2j) We could prove this by induction on j, but we'll accept it on faith let j = h/2 \u2265 2h/2M(h - h) \u2265 2h/2M(0) M(h) \u2265 2h/2 So, we've shown that the minimum number of nodes that can be present in an AVL tree of height h is at least 2h/2. In reality, it's actually more than that, but this gives us something useful to work with; we can use this result to figure out what we're really interested in, which is the opposite: what is the height of an AVL tree with n nodes? M(h) \u2265 2h/2 log2M(h) \u2265 h/2 2 log2M(h) \u2265 h Finally, we see that, for AVL trees of height h with the minimum number of nodes, the height is no more than 2 log2n, where n is the number of nodes in the tree. For AVL trees with more than the minimum number of nodes, the relationship between the number of nodes and the height is even better, though, for reasons we've seen previously, we know that the relationship between the number of nodes and the height of a binary tree can never be better than logarithmic. So, ultimately, we see that the height of an AVL tree with n nodes is \u0398(log n). (In reality, it turns out that the bound is lower than 2 log2n; it's something more akin to about 1.44 log2n, even for AVL trees with the minimum number of nodes, though the proof of that is more involved and doesn't change the asymptotic result.)"
    },
    "786": {
        "file": "Extended Notes - UCI AVL Trees.pdf",
        "page": 4,
        "chunk": "is more involved and doesn't change the asymptotic result.)"
    },
    "787": {
        "file": "03 - Moving Beyond the Relational Model.pdf",
        "page": 0,
        "chunk": "DS 4300 Moving Beyond the Relational Model Mark Fontenot, PhD Northeastern University"
    },
    "788": {
        "file": "03 - Moving Beyond the Relational Model.pdf",
        "page": 1,
        "chunk": "Bene\ufb01ts of the Relational Model - (Mostly) Standard Data Model and Query Language - ACID Compliance (more on this in a second) - Atomicity, Consistency, Isolation, Durability - Works well will highly structured data - Can handle large amounts of data - Well understood, lots of tooling, lots of experience 2"
    },
    "789": {
        "file": "03 - Moving Beyond the Relational Model.pdf",
        "page": 2,
        "chunk": "Relational Database Performance Many ways that a RDBMS increases ef\ufb01ciency: - indexing (the topic we focused on) - directly controlling storage - column oriented storage vs row oriented storage - query optimization - caching/prefetching - materialized views - precompiled stored procedures - data replication and partitioning 3"
    },
    "790": {
        "file": "03 - Moving Beyond the Relational Model.pdf",
        "page": 3,
        "chunk": "Transaction Processing - Transaction - a sequence of one or more of the CRUD operations performed as a single, logical unit of work - Either the entire sequence succeeds (COMMIT) - OR the entire sequence fails (ROLLBACK or ABORT) - Help ensure - Data Integrity - Error Recovery - Concurrency Control - Reliable Data Storage - Simpli\ufb01ed Error Handling 4"
    },
    "791": {
        "file": "03 - Moving Beyond the Relational Model.pdf",
        "page": 4,
        "chunk": "ACID Properties - Atomicity - transaction is treated as an atomic unit - it is fully executed or no parts of it are executed - Consistency - a transaction takes a database from one consistent state to another consistent state - consistent state - all data meets integrity constraints 5"
    },
    "792": {
        "file": "03 - Moving Beyond the Relational Model.pdf",
        "page": 5,
        "chunk": "ACID Properties - Isolation - Two transactions T1 and T2 are being executed at the same time but cannot affect each other - If both T1 and T2 are reading the data - no problem - If T1 is reading the same data that T2 may be writing, can result in: - Dirty Read - Non-repeatable Read - Phantom Reads 6"
    },
    "793": {
        "file": "03 - Moving Beyond the Relational Model.pdf",
        "page": 6,
        "chunk": "Isolation: Dirty Read 7 Figure from: https://www.mybluelinux.com/relational-databases-explained/ Dirty Read - a transaction T1 is able to read a row that has been modi\ufb01ed by another transaction T2 that hasn\u2019t yet executed a COMMIT"
    },
    "794": {
        "file": "03 - Moving Beyond the Relational Model.pdf",
        "page": 7,
        "chunk": "Isolation: Non-Repeatable Read 8 Figure from: https://www.mybluelinux.com/relational-databases-explained/ Non-repeatable Read - two queries in a single transaction T1 execute a SELECT but get different values because another transaction T2 has changed data and COMMITTED"
    },
    "795": {
        "file": "03 - Moving Beyond the Relational Model.pdf",
        "page": 8,
        "chunk": "Isolation: Phantom Reads 9 Figure from: https://www.mybluelinux.com/relational-databases-explained/ Phantom Reads - when a transaction T1 is running and another transaction T2 adds or deletes rows from the set T1 is using"
    },
    "796": {
        "file": "03 - Moving Beyond the Relational Model.pdf",
        "page": 9,
        "chunk": "Example Transaction - Transfer $$ 10 DELIMITER // CREATE PROCEDURE transfer( IN sender_id INT, IN receiver_id INT, IN amount DECIMAL(10,2) ) BEGIN DECLARE rollback_message VARCHAR(255) DEFAULT 'Transaction rolled back: Insufficient funds'; DECLARE commit_message VARCHAR(255) DEFAULT 'Transaction committed successfully'; -- Start the transaction START TRANSACTION; -- Attempt to debit money from account 1 UPDATE accounts SET balance = balance - amount WHERE account_id = sender_id; -- Attempt to credit money to account 2 UPDATE accounts SET balance = balance + amount WHERE account_id = receiver_id; -- Continued Next Slide"
    },
    "797": {
        "file": "03 - Moving Beyond the Relational Model.pdf",
        "page": 10,
        "chunk": "Example Transaction - Transfer $$ 11 -- Continued from previous slide -- Check if there are sufficient funds in account 1 -- Simulate a condition where there are insufficient funds IF (SELECT balance FROM accounts WHERE account_id = sender_id) < 0 THEN -- Roll back the transaction if there are insufficient funds ROLLBACK; SIGNAL SQLSTATE '45000' -- 45000 is unhandled, user-defined error SET MESSAGE_TEXT = rollback_message; ELSE -- Log the transactions if there are sufficient funds INSERT INTO transactions (account_id, amount, transaction_type) VALUES (sender_id, -amount, 'WITHDRAWAL'); INSERT INTO transactions (account_id, amount, transaction_type) VALUES (receiver_id, amount, 'DEPOSIT'); -- Commit the transaction COMMIT; SELECT commit_message AS 'Result'; END IF; END // DELIMITER ;"
    },
    "798": {
        "file": "03 - Moving Beyond the Relational Model.pdf",
        "page": 11,
        "chunk": "ACID Properties - Durability - Once a transaction is completed and committed successfully, its changes are permanent. - Even in the event of a system failure, committed transactions are preserved - For more info on Transactions, see: - Kleppmann Book Chapter 7 12"
    },
    "799": {
        "file": "03 - Moving Beyond the Relational Model.pdf",
        "page": 12,
        "chunk": "But \u2026 Relational Databases may not be the solution to all problems\u2026 - sometimes, schemas evolve over time - not all apps may need the full strength of ACID compliance - joins can be expensive - a lot of data is semi-structured or unstructured (JSON, XML, etc) - Horizontal scaling presents challenges - some apps need something more performant (real time, low latency systems) 13"
    },
    "800": {
        "file": "03 - Moving Beyond the Relational Model.pdf",
        "page": 13,
        "chunk": "Scalability - Up or Out? Conventional Wisdom: Scale vertically (up, with bigger, more powerful systems) until the demands of high-availability make it necessary to scale out with some type of distributed computing model But why? Scaling up is easier - no need to really modify your architecture. But there are practical and \ufb01nancial limits However: There are modern systems that make horizontal scaling less problematic. 14"
    },
    "801": {
        "file": "03 - Moving Beyond the Relational Model.pdf",
        "page": 14,
        "chunk": "So what? Distributed Data when Scaling Out A distributed system is \u201ca collection of independent computers that appear to its users as one computer.\u201d -Andrew Tennenbaum Characteristics of Distributed Systems: - computers operate concurrently - computers fail independently - no shared global clock 15"
    },
    "802": {
        "file": "03 - Moving Beyond the Relational Model.pdf",
        "page": 15,
        "chunk": "Distributed Storage - 2 Directions 16 Single Main Node"
    },
    "803": {
        "file": "03 - Moving Beyond the Relational Model.pdf",
        "page": 16,
        "chunk": "Distributed Data Stores - Data is stored on > 1 node, typically replicated - i.e. each block of data is available on N nodes - Distributed databases can be relational or non-relational - MySQL and PostgreSQL support replication and sharding - CockroachDB - new player on the scene - Many NoSQL systems support one or both models - But remember: Network partitioning is inevitable! - network failures, system failures - Overall system needs to be Partition Tolerant - System can keep running even w/ network partition 17"
    },
    "804": {
        "file": "03 - Moving Beyond the Relational Model.pdf",
        "page": 17,
        "chunk": "The CAP Theorem 18"
    },
    "805": {
        "file": "03 - Moving Beyond the Relational Model.pdf",
        "page": 18,
        "chunk": "The CAP Theorem 19 The CAP Theorem states that it is impossible for a distributed data store to simultaneously provide more than two out of the following three guarantees: - Consistency - Every read receives the most recent write or error thrown - Availability - Every request receives a (non-error) response - but no guarantee that the response contains the most recent write - Partition Tolerance - The system can continue to operate despite arbitrary network issues."
    },
    "806": {
        "file": "03 - Moving Beyond the Relational Model.pdf",
        "page": 19,
        "chunk": "CAP Theorem - Database View 20 Reference: https://alperenbayramoglu.com/posts/understanding-cap-theorem/ - Consistency*: Every user of the DB has an identical view of the data at any given instant - Availability: In the event of a failure, the database remains operational - Partition Tolerance: The database can maintain operations in the event of the network\u2019s failing between two segments of the distributed system * Note, the definition of Consistency in CAP is different from that of ACID."
    },
    "807": {
        "file": "03 - Moving Beyond the Relational Model.pdf",
        "page": 20,
        "chunk": "CAP Theorem - Database View 21 Reference: https://alperenbayramoglu.com/posts/understanding-cap-theorem/ - Consistency + Availability: System always responds with the latest data and every request gets a response, but may not be able to deal with network issues - Consistency + Partition Tolerance: If system responds with data from a distributed store, it is always the latest, else data request is dropped. - Availability + Partition Tolerance: System always sends are responds based on distributed store, but may not be the absolute latest data."
    },
    "808": {
        "file": "03 - Moving Beyond the Relational Model.pdf",
        "page": 21,
        "chunk": "CAP in Reality What it is really saying: - If you cannot limit the number of faults, requests can be directed to any server, and you insist on serving every request, then you cannot possibly be consistent. But it is interpreted as: - You must always give up something: consistency, availability, or tolerance to failure. 22"
    },
    "809": {
        "file": "03 - Moving Beyond the Relational Model.pdf",
        "page": 22,
        "chunk": "?? 23"
    },
    "810": {
        "file": "Memory Structure and Indexing.pdf",
        "page": 0,
        "chunk": "Computer Memory Structure 1.\u200b Processor a.\u200b CPU i.\u200b Super fast ii.\u200b Super expensive iii.\u200b Tiny capacity b.\u200b CPU Cache (L1 - L3 Cache) i.\u200b Not as fast as CPU ii.\u200b Expensive iii.\u200b Small capacity 2.\u200b RAM a.\u200b Physical Memory (Random Access Memory) i.\u200b Not as fast as CPU cache ii.\u200b Priced reasonably iii.\u200b Average Capacity 3.\u200b Solid State Drives (Solid State Memory) a.\u200b Non-Volatile Flash-Based Memory i.\u200b Average Speed ii.\u200b Priced reasonable iii.\u200b Average capacity 4.\u200b Mechanical Hard Drives (Virtual Memory) a.\u200b File-Based Memory i.\u200b Slow ii.\u200b Cheap iii.\u200b Large Capacity \u25cf\u200b For fast databases, we want to minimize access to solid state drives and hard drives \u25cf\u200b A 64-bit integer takes up 8 bytes of memory \u25cf\u200b Say you have a 2048 byte block size. To get 1, 64-bit (8 byte) integer out of memory, the DB has to read 2048 bytes. \u25cf\u200b In an AVL node containing a key-value pair of two 64-bit integers, we are using 32 bytes of memory \u25cb\u200b 8 bytes for key, 8 bytes for value, 8 bytes for right child pointer, 8 bytes for left child pointer \u25cb\u200b Therefore, we are not optimizing the amount of memory we have available. We can increase the performance by decreasing the height of the tree \u25cf\u200b Consider the following case: We have a sorted array of 128 integers"
    },
    "811": {
        "file": "Memory Structure and Indexing.pdf",
        "page": 1,
        "chunk": "\u25cb\u200b 128 * 8 = 1024 bytes \u25cb\u200b In the worst case, binary search o 128 integers is still much faster than a single additional disk access \u25cb\u200b Reading only one block of memory from a hard drive is significantly faster than reading for a second time \u25cb\u200b What if we could have two keys per node? Or three or four\u2026? \u25a0\u200b As you add more keys to each node, the tree gets shallower (smaller height) and searching gets faster \u25a0\u200b The main point: in databases, we want to minimize hard drive access \u25cf\u200b The ideal structure or databases and indexing are B+ trees \u25cb\u200b Each node can have up to 128, 256, etc values (keys) for indexing. \u25cb\u200b For n-1 keys, we can have n children \u25cb\u200b Searching for a child is still much faster than another disk read \u25cb\u200b This way, we can store more values and keys in much fewer levels \u25cf\u200b The goal of indexing structures = minimize the height of the tree"
    },
    "812": {
        "file": "Extended Notes - U Rochester BST.pdf",
        "page": 0,
        "chunk": "Chapter 12: Binary Search Trees A binary search tree is a binary tree with a special property called the BST-property, which is given as follows: \u22c6 For all nodes x and y, if y belongs to the left subtree of x, then the key at y is less than the key at x, and if y belongs to the right subtree of x, then the key at y is greater than the key at x. We will assume that the keys of a BST are pairwise distinct. Each node has the following attributes: \u2022 p, left, and right, which are pointers to the parent, the left child, and the right child, respectively, and \u2022 key, which is key stored at the node. 1"
    },
    "813": {
        "file": "Extended Notes - U Rochester BST.pdf",
        "page": 1,
        "chunk": "An example 4 2 3 6 5 12 9 8 11 15 19 20 7 2"
    },
    "814": {
        "file": "Extended Notes - U Rochester BST.pdf",
        "page": 2,
        "chunk": "Traversal of the Nodes in a BST By \u201ctraversal\u201d we mean visiting all the nodes in a graph. Traversal strategies can be speci\ufb01ed by the ordering of the three objects to visit: the current node, the left subtree, and the right subtree. We assume the the left subtree always comes before the right subtree. Then there are three strategies. 1. Inorder. The ordering is: the left subtree, the current node, the right subtree. 2. Preorder. The ordering is: the current node, the left subtree, the right subtree. 3. Postorder. The ordering is: the left subtree, the right subtree, the current node. 3"
    },
    "815": {
        "file": "Extended Notes - U Rochester BST.pdf",
        "page": 3,
        "chunk": "Inorder Traversal Pseudocode This recursive algorithm takes as the input a pointer to a tree and executed inorder traversal on the tree. While doing traversal it prints out the key of each node that is visited. Inorder-Walk(x) 1: if x = nil then return 2: Inorder-Walk(left[x]) 3: Print key[x] 4: Inorder-Walk(right[x]) We can write a similar pseudocode for preorder and postorder. 4"
    },
    "816": {
        "file": "Extended Notes - U Rochester BST.pdf",
        "page": 4,
        "chunk": "preorder postorder inorder 2 1 3 1 1 2 2 3 3 4 2 3 6 5 12 9 8 11 15 19 20 7 What is the outcome of inorder traversal on this BST? How about postorder traversal and preorder traversal? 5"
    },
    "817": {
        "file": "Extended Notes - U Rochester BST.pdf",
        "page": 5,
        "chunk": "Inorder traversal gives: 2, 3, 4, 5, 6, 7, 8 , 9, 11, 12, 15, 19, 20. Preorder traversal gives: 7, 4, 2, 3, 6, 5, 12, 9, 8, 11, 19, 15, 20. Postorder traversal gives: 3, 2, 5, 6, 4, 8, 11, 9, 15, 20, 19, 12, 7. So, inorder travel on a BST \ufb01nds the keys in nondecreasing order! 6"
    },
    "818": {
        "file": "Extended Notes - U Rochester BST.pdf",
        "page": 6,
        "chunk": "Operations on BST 1. Searching for a key We assume that a key and the subtree in which the key is searched for are given as an input. We\u2019ll take the full advantage of the BST-property. Suppose we are at a node. If the node has the key that is being searched for, then the search is over. Otherwise, the key at the current node is either strictly smaller than the key that is searched for or strictly greater than the key that is searched for. If the former is the case, then by the BST property, all the keys in th left subtree are strictly less than the key that is searched for. That means that we do not need to search in the left subtree. Thus, we will examine only the right subtree. If the latter is the case, by symmetry we will examine only the right subtree. 7"
    },
    "819": {
        "file": "Extended Notes - U Rochester BST.pdf",
        "page": 7,
        "chunk": "Algorithm Here k is the key that is searched for and x is the start node. BST-Search(x, k) 1: y \u2190x 2: while y \u0338= nil do 3: if key[y] = k then return y 4: else if key[y] < k then y \u2190right[y] 5: else y \u2190left[y] 6: return (\u201cNOT FOUND\u201d) 8"
    },
    "820": {
        "file": "Extended Notes - U Rochester BST.pdf",
        "page": 8,
        "chunk": "An Example search for 8 7 4 2 6 9 13 11 NIL What is the running time of search? 9"
    },
    "821": {
        "file": "Extended Notes - U Rochester BST.pdf",
        "page": 9,
        "chunk": "2. The Maximum and the Minimum To \ufb01nd the minimum identify the leftmost node, i.e. the farthest node you can reach by following only left branches. To \ufb01nd the maximum identify the rightmost node, i.e. the farthest node you can reach by following only right branches. BST-Minimum(x) 1: if x = nil then return (\u201cEmpty Tree\u201d) 2: y \u2190x 3: while left[y] \u0338= nil do y \u2190left[y] 4: return (key[y]) BST-Maximum(x) 1: if x = nil then return (\u201cEmpty Tree\u201d) 2: y \u2190x 3: while right[y] \u0338= nil do y \u2190right[y] 4: return (key[y]) 10"
    },
    "822": {
        "file": "Extended Notes - U Rochester BST.pdf",
        "page": 10,
        "chunk": "3. Insertion Suppose that we need to insert a node z such that k = key[z]. Using binary search we \ufb01nd a nil such that replacing it by z does not break the BST-property. 11"
    },
    "823": {
        "file": "Extended Notes - U Rochester BST.pdf",
        "page": 11,
        "chunk": "BST-Insert(x, z, k) 1: if x = nil then return \u201cError\u201d 2: y \u2190x 3: while true do { 4: if key[y] < k 5: then z \u2190left[y] 6: else z \u2190right[y] 7: if z = nil break 8: } 9: if key[y] > k then left[y] \u2190z 10: else right[p[y]] \u2190z 12"
    },
    "824": {
        "file": "Extended Notes - U Rochester BST.pdf",
        "page": 12,
        "chunk": "4. The Successor and The Predecessor The successor (respectively, the predecessor) of a key k in a search tree is the smallest (respectively, the largest) key that belongs to the tree and that is strictly greater than (respectively, less than) k. The idea for \ufb01nding the successor of a given node x. \u2022 If x has the right child, then the successor is the minimum in the right subtree of x. \u2022 Otherwise, the successor is the parent of the farthest node that can be reached from x by following only right branches backward. 13"
    },
    "825": {
        "file": "Extended Notes - U Rochester BST.pdf",
        "page": 13,
        "chunk": "An Example 4 2 3 6 5 12 9 8 11 15 19 20 7 25 23 14"
    },
    "826": {
        "file": "Extended Notes - U Rochester BST.pdf",
        "page": 14,
        "chunk": "Algorithm BST-Successor(x) 1: if right[x] \u0338= nil then 2: { y \u2190right[x] 3: while left[y] \u0338= nil do y \u2190left[y] 4: return (y) } 5: else 6: { y \u2190x 7: while right[p[x]] = x do y \u2190p[x] 8: if p[x] \u0338= nil then return (p[x]) 9: else return (\u201cNO SUCCESSOR\u201d) } 15"
    },
    "827": {
        "file": "Extended Notes - U Rochester BST.pdf",
        "page": 15,
        "chunk": "The predecessor can be found similarly with the roles of left and right exchanged and with the roles of maximum and minimum exchanged. For which node is the successor unde\ufb01ned? What is the running time of the successor algorithm? 16"
    },
    "828": {
        "file": "Extended Notes - U Rochester BST.pdf",
        "page": 16,
        "chunk": "5. Deletion Suppose we want to delete a node z. 1. If z has no children, then we will just replace z by nil. 2. If z has only one child, then we will promote the unique child to z\u2019s place. 3. If z has two children, then we will identify z\u2019s successor. Call it y. The successor y either is a leaf or has only the right child. Promote y to z\u2019s place. Treat the loss of y using one of the above two solutions. 17"
    },
    "829": {
        "file": "Extended Notes - U Rochester BST.pdf",
        "page": 17,
        "chunk": "10 10 5 6 1 3 2 4 7 9 13 11 8 5 6 1 3 2 4 9 13 11 8 4 2 3 5 6 1 3 2 4 7 9 13 11 8 5 6 9 13 11 8 7 10 10 10 5 6 1 3 2 4 7 9 13 11 8 5 6 1 3 2 4 13 11 9 10 18"
    },
    "830": {
        "file": "Extended Notes - U Rochester BST.pdf",
        "page": 18,
        "chunk": "Algorithm This algorithm deletes z from BST T. BST-Delete(T, z) 1: if left[z] = nil or right[z] = nil 2: then y \u2190z 3: else y \u2190BST-Successor(z) 4: \u0003 y is the node that\u2019s actually removed. 5: \u0003 Here y does not have two children. 6: if left[y] \u0338= nil 7: then x \u2190left[y] 8: else x \u2190right[y] 9: \u0003 x is the node that\u2019s moving to y\u2019s position. 10: if x \u0338= nil then p[x] \u2190p[y] 11: \u0003 p[x] is reset If x isn\u2019t NIL. 12: \u0003 Resetting is unnecessary if x is NIL. 19"
    },
    "831": {
        "file": "Extended Notes - U Rochester BST.pdf",
        "page": 19,
        "chunk": "Algorithm (cont\u2019d) 13: if p[y] = nil then root[T] \u2190x 14: \u0003 If y is the root, then x becomes the root. 15: \u0003 Otherwise, do the following. 16: else if y = left[p[y]] 17: then left[p[y]] \u2190x 18: \u0003 If y is the left child of its parent, then 19: \u0003 Set the parent\u2019s left child to x. 20: else right[p[y]] \u2190x 21: \u0003 If y is the right child of its parent, then 22: \u0003 Set the parent\u2019s right child to x. 23: if y \u0338= z then 24: { key[z] \u2190key[y] 25: Move other data from y to z } 27: return (y) 20"
    },
    "832": {
        "file": "06 - Redis + Python.pdf",
        "page": 0,
        "chunk": "DS 4300 Redis + Python Mark Fontenot, PhD Northeastern University"
    },
    "833": {
        "file": "06 - Redis + Python.pdf",
        "page": 1,
        "chunk": "Redis-py 2 - Redis-py is the standard client for Python. - Maintained by the Redis Company itself - GitHub Repo: redis/redis-py - In your 4300 Conda Environment: pip install redis"
    },
    "834": {
        "file": "06 - Redis + Python.pdf",
        "page": 2,
        "chunk": "Connecting to the Server - For your Docker deployment, host could be localhost or 127.0.0.1 - Port is the port mapping given when you created the container (probably the default 6379) - db is the database 0-15 you want to connect to - decode_responses \u2192 data comes back from server as bytes. Setting this true converter them (decodes) to strings. 3 import redis redis_client = redis.Redis(host=\u2019localhost\u2019, port=6379, db=2, decode_responses=True)"
    },
    "835": {
        "file": "06 - Redis + Python.pdf",
        "page": 3,
        "chunk": "Redis Command List - Full List > here < - Use Filter to get to command for the particular data structure you\u2019re targeting (list, hash, set, etc.) - Redis.py Documentation > here < - The next slides are not meant to be an exhaustive list of commands, only some highlights. Check the documentation for a complete list. 4"
    },
    "836": {
        "file": "06 - Redis + Python.pdf",
        "page": 4,
        "chunk": "String Commands # r represents the Redis client object r.set(\u2018clickCount:/abc\u2019, 0) val = r.get(\u2018clickCount:/abc\u2019) r.incr(\u2018clickCount:/abc\u2019) ret_val = r.get(\u2018clickCount:/abc\u2019) print(f\u2019click count = {ret_val}\u2019) 5"
    },
    "837": {
        "file": "06 - Redis + Python.pdf",
        "page": 5,
        "chunk": "String Commands - 2 # r represents the Redis client object redis_client.mset({'key1': 'val1', 'key2': 'val2', 'key3': 'val3'}) print(redis_client.mget('key1', 'key2', 'key3')) # returns as list [\u2018val1\u2019, \u2018val2\u2019, \u2018val3\u2019] 6"
    },
    "838": {
        "file": "06 - Redis + Python.pdf",
        "page": 6,
        "chunk": "String Commands - 3 - set(), mset(), setex(), msetnx(), setnx() - get(), mget(), getex(), getdel() - incr(), decr(), incrby(), decrby() - strlen(), append() 7"
    },
    "839": {
        "file": "06 - Redis + Python.pdf",
        "page": 7,
        "chunk": "List Commands - 1 # create list: key = \u2018names\u2019 # values = [\u2018mark\u2019, \u2018sam\u2019, \u2018nick\u2019] redis_client.rpush('names', 'mark', 'sam', 'nick') # prints [\u2018mark\u2019, \u2018sam\u2019, \u2018nick\u2019] print(redis_client.lrange('names', 0, -1)) 8"
    },
    "840": {
        "file": "06 - Redis + Python.pdf",
        "page": 8,
        "chunk": "List Commands - 2 - lpush(), lpop(), lset(), lrem() - rpush(), rpop() - lrange(), llen(), lpos() - Other commands include moving elements between lists, popping from multiple lists at the same time, etc. 9"
    },
    "841": {
        "file": "06 - Redis + Python.pdf",
        "page": 9,
        "chunk": "Hash Commands - 1 redis_client.hset('user-session:123', mapping={'first': 'Sam', 'last': 'Uelle', 'company': 'Redis', 'age': 30 }) # prints: #{'name': 'Sam', 'surname': 'Uelle', 'company': 'Redis', 'age': '30'} print(redis_client.hgetall('user-session:123')) 10"
    },
    "842": {
        "file": "06 - Redis + Python.pdf",
        "page": 10,
        "chunk": "Hash Commands - 2 - hset(), hget(), hgetall() - hkeys() - hdel(), hexists(), hlen(), hstrlen() 11"
    },
    "843": {
        "file": "06 - Redis + Python.pdf",
        "page": 11,
        "chunk": "Redis Pipelines - Helps avoid multiple related calls to the server \u2192 less network overhead 12 r = redis.Redis(decode_responses=True) pipe = r.pipeline() for i in range(5): pipe.set(f\"seat:{i}\", f\"#{i}\") set_5_result = pipe.execute() print(set_5_result) # >>> [True, True, True, True, True] pipe = r.pipeline() # \"Chain\" pipeline commands together. get_3_result = pipe.get(\"seat:0\").get(\"seat:3\").get(\"seat:4\").execute() print(get_3_result) # >>> ['#0', '#3', '#4']"
    },
    "844": {
        "file": "06 - Redis + Python.pdf",
        "page": 12,
        "chunk": "Redis in Context 13"
    },
    "845": {
        "file": "06 - Redis + Python.pdf",
        "page": 13,
        "chunk": "Redis in ML - Simpli\ufb01ed Example 14 Source: https://www.featureform.com/post/feature-stores-explained-the-three-common-architectures"
    },
    "846": {
        "file": "06 - Redis + Python.pdf",
        "page": 14,
        "chunk": "Redis in DS/ML 15 Source: https://madewithml.com/courses/mlops/feature-store/"
    },
    "847": {
        "file": "Copy of Midterm Sample Questions.pdf",
        "page": 0,
        "chunk": "DS 4300 - Spring 2025 Sample Midterm Questions (& HW 04) Below are some sample exam questions with which you can test your RAG \u201cCheat Sheet\u201d. Question: What is the difference between a list where memory is contiguously allocated and a list where linked structures are used? Answer: In a list where memory is contiguously allocated, there is no need for pointers. The next element in the list simply lies in the next block of memory. Therefore, a program can easily find the next element in the list by increasing the memory address by 1 unit. In a case where linked structures are used, the memory address of the next element in the list is not necessarily the next memory address like in contiguous allocation. Therefore, in linked structures, each element must have a pointer to the next element\u2019s memory address. Question: When are linked lists faster than contiguously-allocated lists? Answer: Linked lists are faster when you want to insert an element anywhere. Question: Add 23 to the AVL Tree below. What imbalance case is created with inserting 23? \u200b \u200b 30 \u200b / \\ \u200b 25 35 \u200b / 20\u200b Answer: Since 23 is less than 30, we go to 30\u2019s left child (25). Since 25 is less than 25, we go to 25\u2019s left child (20). Since 23 is greater than 20, we place 23 as 20\u2019s right child. This presents a left-right insertion imbalance. 25 is the node of imbalance, and 23 was inserted into the right subtree of the left child of 25. Question: Why is a B+ Tree a better than an AVL tree when indexing a large dataset? Answer: B+ trees are the most optimal structure for indexing large datasets because they minimize disk access. B+ trees are \u201cwider\u201d than AVL trees, but this"
    },
    "848": {
        "file": "Copy of Midterm Sample Questions.pdf",
        "page": 0,
        "chunk": "subtree of the left child of 25. Question: Why is a B+ Tree a better than an AVL tree when indexing a large dataset? Answer: B+ trees are the most optimal structure for indexing large datasets because they minimize disk access. B+ trees are \u201cwider\u201d than AVL trees, but this allows them to be shallower. The shallower a tree is, the less disk access we need to make to retrieve an element. Since each node in a B+ tree can contain an arbitrary number of sorted keys and their values, we can maximize the storage efficiency on disk. Question: What is disk-based indexing and why is it important for database systems? Answer: Disk-based indexing is a process by which we store frequently accessed data on disk. This allows for quick retrieval which is essential for database systems. Question: In the context of a relational database system, what is a transaction? Answer: In the context of a RDBMS, a transaction refers to a sequence of operations that are performed on data that are executed as a single, indivisible unit. Question: Succinctly describe the four components of ACID compliant transactions. Answer: Atomicity - each operation is a single, indivisible unit (like an atom). Consistency - operations take the database from one valid state to another, preserving the integrity constraints. Isolations - transactions execute independently without interfering with one another. If both transactions access the same data, they cannot affect each other\u2019s results. Durability - once a transaction is committed, its changes are permanent, even in the event of a system failure. Question: Why does the CAP principle not make sense when applied to a single-node MongoDB"
    },
    "849": {
        "file": "Copy of Midterm Sample Questions.pdf",
        "page": 0,
        "chunk": "permanent, even in the event of a system failure. Question: Why does the CAP principle not make sense when applied to a single-node MongoDB"
    },
    "850": {
        "file": "Copy of Midterm Sample Questions.pdf",
        "page": 1,
        "chunk": "instance? Answer: The CAP theorem only applies to distributed systems and a single-node MongoDB is just that - a single node. It is not a distributed system. Therefore, all three principles of the CAP theorem (Consistency, Availability, and Partition Tolerance) are fundamentally satisfied by a single MongoDB node. Question: Describe the differences between horizontal and vertical scaling. Answer: Horizontal scaling is adding more servers or nodes to distribute the load across more servers. Vertical scaling is making the current server/node larger to handle more information. The main difference in practice is that horizontal scaling is essentially limitless. You can add as many servers/nodes to achieve a more efficient load balance. Vertical scaling is more limited because you can only add so many CPUs or GBs of RAM for example. Question: Briefly describe how a key/value store can be used as a feature store. Answer: A feature store is a data system used for machine learning, serving as a centralized hub for storing, processing, and accessing commonly used features. Key-value stores can be used as a feature store because retrieving commonly used features is very efficient in key-value stores. Since the underlying data structure resembles a hash table, finding a particular feature can be done in constant time. Question: When was Redis originally released? Answer: 2009. Question: In Redis, what is the difference between the INC and INCR commands? Answer: INC is not a command in Redis. INCR increases a value by 1. Question: What are the benefits of BSON over JSON in MongoDB? Answer: MongoDB supports a wide variety of data types as its fields. Therefore, BSON is required to quickly serialize and deserialize complex field types such as documents. Using JSON may be slower in such complex cases. Question: Write a Mongo query based on the movies data set"
    },
    "851": {
        "file": "Copy of Midterm Sample Questions.pdf",
        "page": 1,
        "chunk": "JSON in MongoDB? Answer: MongoDB supports a wide variety of data types as its fields. Therefore, BSON is required to quickly serialize and deserialize complex field types such as documents. Using JSON may be slower in such complex cases. Question: Write a Mongo query based on the movies data set that returns the titles of all movies released between 2010 and 2015 from the suspense genre? Answer: db.movies.find({ \"year\": { $gte: 2010, $lte: 2015 }, \"genres\": \"suspense\" }, { \"name\": 1 }) Question: What does the $nin operator mean in a Mongo query? Answer: Not in"
    },
    "852": {
        "file": "Mongo_DB_Shell_Cheat_Sheet_1a0e3aa962.pdf",
        "page": 0,
        "chunk": "MongoDB Shell Cheat Sheet To get started, install the MongoDB Shell (mongosh). Basic Commands These basic help commands are available in the MongoDB Shell. mongosh Open a connection to your local MongoDB instance. All other commands will be run within this mongosh connection. db.help() Show help for database methods. db.<collection>.help() db.users.help() Show help on collection methods. The <collection> can be the name of an existing collection or a non-existing collection. Shows help on methods related to the users collection. show dbs Print a list of all databases on the server. use <db> Switch current database to <db>. The mongo shell variable db is set to the current database. show collections Print a list of all collections for the current database. show users Print a list of users for the current database. show roles Print a list of all roles, both user-defined and built-in, for the current database. show profile Print the five most recent operations that took 1 millisecond or more on databases with profiling enabled."
    },
    "853": {
        "file": "Mongo_DB_Shell_Cheat_Sheet_1a0e3aa962.pdf",
        "page": 1,
        "chunk": "show databases Print a list of all existing databases available to the current user. exit Exit the mongosh session. Create Operations Create or insert operations add new documents to a collection. If the collection does not exist, create operations also create the collection. db.collection.insertOne() db.users.insertOne( { name: \"Chris\"} ) Inserts a single document into a collection. Add a new document with the name of Chris into the users collection db.collection.insertMany() db.users.insertMany( { age: \"24\"}, {age: \"38\"} ) Inserts multiple documents into a collection. Add two new documents with the age of 24 and 38 into the users collection Read Operations Read operations retrieve documents from a collection; i.e. query a collection for documents. db.collection.find() db.users.find() Selects documents in a collection or view and returns a cursor to the selected documents. Returns all users. db.collection.find(<filterobjec t>) db.users.find({place: \"NYC\"}) Find all documents that match the filter object Returns all users with the place NYC. db.collection.find({<field>:1,< field>:1}) db.users.find({status:1,item:1}) Returns all documents that match the query after you explicitly include several fields by setting the <field> to 1 in the projection document. Returns matching documents only from state field, item field and, by default, the _id field."
    },
    "854": {
        "file": "Mongo_DB_Shell_Cheat_Sheet_1a0e3aa962.pdf",
        "page": 2,
        "chunk": "db.collection.find({<field>:1,< field>:0, _id:0}) db.users.find({status:1,item:1,_id:0} ) Returns all documents that match the query and removes the _id field from the results by setting it to 0 in the projection. Returns matching documents only from state field and item field. Does not return the _id field. Update Operations Update operations modify existing documents in a collection. db.collection.updateOne() db.users.updateOne({ age: 25 }, { $set: { age: 32 } }) Updates a single document within the collection based on the filter. Updates all users from the age of 25 to 32. db.collection.updateMany() db.users.updateMany({ age: 27 }, { $inc: { age: 3 } }) Updates a single document within the collection based on the filter. Updates all users with an age of 27 with an increase of 3. db.collection.replaceOne() db.users.replaceOne({ name: Kris }, { name: Chris }) Replaces a single document within the collection based on the filter. Replaces the first user with the name Kris with a document that has the name Chris in its name field. Delete Operations Delete operations remove documents from a collection. db.collection.deleteOne() db.users.deleteOne({ age: 37 }) Removes a single document from a collection. Deletes the first user with the age 37. db.collection.deleteMany() db.users.deleteMany({ age: {$lt:18 }) Removes all documents that match the filter from a collection. Deletes all users with the age less than 18.."
    },
    "855": {
        "file": "Mongo_DB_Shell_Cheat_Sheet_1a0e3aa962.pdf",
        "page": 3,
        "chunk": "Comparison Query Operators Use the following inside an filter object to make complex queries $eq db.users.find({ system: { $eq: \"macOS\" } }) Matches values that are equal to a specified value. Finds all users with the operating system macOS. $gt db.users.deleteMany({ age: { $gt: 99} }) Matches values that are greater than a specified value. Deletes all users with an age greater than 99. $gte db.users.updateMany({ age\": {$gte:21 },{access: \"valid\"}) Matches values that are greater than or equal to a specified value. Updates all access to \"valid\" for all users with an age greater than or equal to 21. $in db.users.find( { place: { $in: [ \"NYC\", \"SF\"] } ) Matches any of the values specified in an array. Find all users with the place field that is either NYC or SF. $lt db.users.deleteMany({ \"age\": {$lt:18 }) Matches values that are less than a specified value. Deletes all users with the age less than 18.. $lte db.users.updateMany({ age: { $lte: 17 }, {access: \"invalid\"}) Matches values that are less than or equal to a specified value. Updates all access to \"invalid\" for all users with an age less than or equal to 17. $ne db.users.find({ \"place\": {$ne: \u2018NYC\"}) Matches all values that are not equal to a specified value. Find all users with the place field set to anything other than NYC."
    },
    "856": {
        "file": "Mongo_DB_Shell_Cheat_Sheet_1a0e3aa962.pdf",
        "page": 4,
        "chunk": "$nin db.users.find( { place: { $nin: [ \"NYC\", \"SF\" ] } ) Matches none of the values specified in an array. Find all users with the place field that does not equal NYC or SF. Field Update Operators Use the following inside an update object to make complex updates $inc db.users.updateOne({ age: 22 }, { $inc: { age: 3} }) Increments the value of the field by the specified amount. Adds 3 to the age of the first user with the age of 22. $min db.scores.insertOne( { _id: 1, highScore: 800, lowScore: 200 } ) db.scores.updateOne( { _id: 1 }, { $min: { lowScore: 150 } } ) Only updates the field if the specified value is less than the existing field value. Creates a scoles collection and sets the value of highScore to 800 and lowScore to 200. $min compares 200 (the current value of lowScore) to the specified value of 150. Because 150 is less than 200, $min will update lowScore to 150. $max db.scores.updateOne( { _id: 1 }, { $max: { highScore: 1000 } } ) Only updates the field if the specified value is greater than the existing field value. $max compares 800 (the current value of highScore) to the specified value of 1000. Because 1000 is more than 800, $max will update highScore to 1000. $rename db.scores.updateOne( { $rename: { 'highScore': 'high'} ) Renames a field. Renames the field \u2018highScores\u2019 to \u2018high\u2019,"
    },
    "857": {
        "file": "Mongo_DB_Shell_Cheat_Sheet_1a0e3aa962.pdf",
        "page": 5,
        "chunk": "$set db.users.updateOne({ $set: { name: \"valid user\" } }) Sets the value of a field in a document. Replaces the value of the name field with the specified value valid user. $unset db.users.updateOne({ $unset: { name: \"\" } }) Removes the specified field from a document. Deletes the specified value valid user from the name field. Read Modifiers Add any of the following to the end of any read operation cursor.sort() db.users.find().sort({ name: 1, age: -1 }) Orders the elements of an array during a $push operation. Sorts all users by name in alphabetical order and then if any names are the same sort by age in reverse order cursor.limit() Specifies the maximum number of documents the cursor will return. cursor.skip() Controls where MongoDB begins returning results. cursor.push() db.users.updateMany({}, { $push: { friends: \"Chris\" } }) Appends a specified value to an array. Add Chris to the friends array for all users"
    },
    "858": {
        "file": "Mongo_DB_Shell_Cheat_Sheet_1a0e3aa962.pdf",
        "page": 6,
        "chunk": "Aggregation Operations The Aggregation Framework provides a specific language that can be used to execute a set of aggregation operations (processing & computation) against data held in MongoDB. db.collection.aggregate() db.users.aggregate([ {$match: { access: \"valid\" } }, {$group: { _id: \"$cust_id\", total:{$sum: \"$amount\" } } }, {$sort: { total: -1 } }]) A method that provides access to the aggregation pipeline. Selects documents in the users collection with accdb.orders.estimatedDocumentCount({})_id field from the sum of the amount field, and sorts the results by the total field in descending order: Aggregation Operations Aggregation pipelines consist of one or more stages that process documents and can return results for groups of documents. count Counts the number of documents in a collection or a view. distinct Displays the distinct values found for a specified key in a collection or a view. mapReduce Run map-reduce aggregation operations over a collection Aggregation Operations Single Purpose Aggregation Methods aggregate documents from a single collection. db.collection.estimatedDocument Count() db.users.estimatedDocumentCount({}) Returns an approximate count of the documents in a collection or a view. Retrieves an approximate count of all the documents in the users collection."
    },
    "859": {
        "file": "Mongo_DB_Shell_Cheat_Sheet_1a0e3aa962.pdf",
        "page": 7,
        "chunk": "db.collection.count() db.users.count({}) Returns a count of the number of documents in a collection or a view. Returns the distinct values for the age field from all documents in the users collection. db.collection.distinct() db.users.distinct(\"age\") Returns an array of documents that have distinct values for the specified field. Returns the distinct values for the age field from all documents in the users collection. Indexing Commands Indexes support the efficient execution of queries in MongoDB. Indexes are special data structures that store a small portion of the data set in an easy-to-traverse form. db.collection.createIndex() db.users.createIndex(\"account creation date\") Builds an index on a collection. Creates the account creation date index in the users collection. db.collection.dropIndex() db.users.dropIndex(\"account creation date\") Removes a specified index on a collection. Removes the account creation date index from the users collection. db.collection.dropIndexes() db.users.dropIndexes() db.users.dropIndex(\"account creation date\", \"account termination date\") Removes all indexes but the _id (no parameters) or a specified set of indexes on a collection. Drop all but the _id index from a collection. Removes the account creation date index and the account termination date index from the users collection."
    },
    "860": {
        "file": "Mongo_DB_Shell_Cheat_Sheet_1a0e3aa962.pdf",
        "page": 8,
        "chunk": "db.collection.getIndexes() db.users.getIndexes() Returns an array of documents that describe the existing indexes on a collection. Returns an array of documents that hold index information for the users collection. db.collection.reIndex() db.users.reIndex() Rebuilds all existing indexes on a collection Drops all indexes on the users collection and recreates them. db.collection.totalIndexSize() db.users.totalIndexSize() Reports the total size used by the indexes on a collection. Provides a wrapper around the totalIndexSize field of the collStats output. Returns the total size of all indexes for the users collection. Replication Commands Replication refers to the process of ensuring that the same data is available on more than one MongoDB Server. rs.add() rs.add( \"mongodbd4.example.net:27017\" ) Adds a member to a replica set. Adds a new secondary member, mongodbd4.example.net:27017, with default vote and priority settings to a new replica set rs.conf() Returns a document that contains the current replica set configuration. rs.status() Returns the replica set status from the point of view of the member where the method is run."
    },
    "861": {
        "file": "Mongo_DB_Shell_Cheat_Sheet_1a0e3aa962.pdf",
        "page": 9,
        "chunk": "rs.stepDown() Instructs the primary of the replica set to become a secondary. After the primary steps down, eligible secondaries will hold an election for primary. rs.remove() Removes the member described by the hostname parameter from the current replica set. rs.reconfig() Reconfigures an existing replica set, overwriting the existing replica set configuration. Sharding Commands Sharding is a method for distributing or partitioning data across multiple computers. This is done by partitioning the data by key ranges and distributing the data among two or more database instances. sh.abortReshardCollection() sh.abortReshardCollection(\"users\") Ends a resharding operation Aborts a running reshard operation on the users collection. sh.addShard() sh.addShard(\"cluster\"/mongodb3.exampl e.net:27327\") Adds a shard to a sharded cluster. Adds the cluster replica set and specifies one member of the replica set. sh.commitReshardCollection () sh.commitReshardCollection(\"records.u sers\") Forces a resharding operation to block writes and complete. Forces the resharding operation on the records.users to block writes and complete."
    },
    "862": {
        "file": "Mongo_DB_Shell_Cheat_Sheet_1a0e3aa962.pdf",
        "page": 10,
        "chunk": "sh.disableBalancing() sh.disableBalancing(\"records.users\") Disable balancing on a single collection in a sharded database. Does not affect balancing of other collections in a sharded cluster. Disables the balancer for the specified sharded collection. sh.enableAutoSplit() Enables auto-splitting for the sharded cluster. sh.disableAutoSplit() Disables auto-splitting for the sharded cluster. sh.enableSharding() sh.enablingSharding(\"records\") Creates a database. Creates the records database. sh.help() Returns help text for the sh methods. sh.moveChunk() sh.moveChunk(\"records.users\", { zipcode: \"10003\" }, \"shardexample\") Migrates a chunk in a sharded cluster. Finds the chunk that contains the documents with the zipcode field set to 10003 and then moves that chunk to the shard named shardexample. sh.reshardCollection() sh.reshardCollection(\"records.users\", { order_id: 1 }) Initiates a resharding operation to change the shard key for a collection, changing the distribution of your data. Reshards the users collection with the new shard key { order_id: 1 } sh.shardCollection() sh.shardCollection(\"records.users\", { zipcode: 1 } ) Enables sharding for a collection. Shards the users collection by the zipcode field."
    },
    "863": {
        "file": "Mongo_DB_Shell_Cheat_Sheet_1a0e3aa962.pdf",
        "page": 11,
        "chunk": "sh.splitAt() sh.splitAt( \"records.users\", { x: 70 } ) Divides an existing chunk into two chunks using a specific value of the shard key as the dividing point. Splits a chunk of the records.users collection at the shard key value x: 70 sh.splitFind() sh.splitFind( \"records.users\", { x:70 } ) Divides an existing chunk that contains a document matching a query into two approximately equal chunks. Splits, at the median point, a chunk that contains the shard key value x: 70. sh.status() Reports on the status of a sharded cluster, as db.printShardingStatus(). sh.waitForPingChange() Internal. Waits for a change in ping state from one of the mongos in the sharded cluster. refineCollectionShardKey db.adminCommand( { shardCollection: \"test.orders\", key: { customer_id: 1 } } ) db.getSiblingDB(\"test\").orders.create Index( { customer_id: 1, order_id: 1 } ) db.adminCommand( { refineCollectionShardKey: \"test.orders\", key: { customer_id: 1, order_id: 1 } } ) Modifies the collection's shard key by adding new field(s) as a suffix to the existing key. Shard the orders collection in the test database. The operation uses the customer_id field as the initial shard key. Create the index to support the new shard key if the index does not already exist. Run refineCollectionShardKey command to add the order_id field as a suffix"
    },
    "864": {
        "file": "Mongo_DB_Shell_Cheat_Sheet_1a0e3aa962.pdf",
        "page": 12,
        "chunk": "convertShardKeyToHashed() use test db.orders.createIndex( { _id: \"hashed\" } ) sh.shardCollection( \"test.orders\", { _id : \"hashed\" } ) { _id: ObjectId(\"5b2be413c06d924ab26ff9ca\"), \"item\" : \"Chocolates\", \"qty\" : 25 } convertShardKeyToHashed( ObjectId(\"5b2be413c06d924ab26ff9ca\") ) Returns the hashed value for the input. Consider a sharded collection that uses a hashed shard key. If the following document exists in the collection, the hashed value of the _id field is used to distribute the document: Determine the hashed value of _id field used to distribute the document across the shards, Database Methods db.runCommand() Run a command against the current database db.adminCommand() Provides a helper to run specified database commands against the admin database."
    },
    "865": {
        "file": "Mongo_DB_Shell_Cheat_Sheet_1a0e3aa962.pdf",
        "page": 13,
        "chunk": "User Management Commands Make updates to users in the MongoDB Shell. db.auth() Authenticates a user to a database. db.changeUserPassword() Updates a user's password. db.createUser() Creates a new user for the database on which the method is run. db.dropUser() db.dropAllUsers() Removes user/all users from the current database. db.getUser() db.getUsers() Returns information for a specified user/all users in the database. db.grantRolesToUser() Grants a role and its privileges to a user. db.removeUser() Removes the specified username from the database. db.revokeRolesFromUser() Removes one or more roles from a user on the current database. db.updateUser() Updates the user's profile on the database on which you run the method. passwordPrompt() Prompts for the password in mongosh."
    },
    "866": {
        "file": "Mongo_DB_Shell_Cheat_Sheet_1a0e3aa962.pdf",
        "page": 14,
        "chunk": "Role Management Commands Make updates to roles in the MongoDB Shell. db.createRole() Authenticates a user to a database. db.dropRole() db.dropAllRoles() Deletes a user-defined role/all user-defined roles associated with a database. db.getRole() db.getRoles() Returns information for the specified role/all the user-defined roles in a database. db.grantPrivilegesToRole() Assigns privileges to a user-defined role. db.revokePrivilegesFromRole() Removes the specified privileges from a user-defined role. db.grantRolesToRole() Specifies roles from which a user-defined role inherits privileges. db.revokeRolesFromRole() Removes inherited roles from a role. db.updateRole() Updates a user-defined role. ff"
    },
    "867": {
        "file": "08 - PyMongo.pdf",
        "page": 0,
        "chunk": "DS 4300 MongoDB + PyMongo Mark Fontenot, PhD Northeastern University"
    },
    "868": {
        "file": "08 - PyMongo.pdf",
        "page": 1,
        "chunk": "PyMongo \u25cfPyMongo is a Python library for interfacing with MongoDB instances 2 from pymongo import MongoClient client = MongoClient( \u2018mongodb://user_name:pw@localhost:27017\u2019 )"
    },
    "869": {
        "file": "08 - PyMongo.pdf",
        "page": 2,
        "chunk": "Getting a Database and Collection 3 from pymongo import MongoClient client = MongoClient( \u2018mongodb://user_name:pw@localhost:27017\u2019 ) db = client[\u2018ds4300\u2019] # or client.ds4300 collection = db[\u2018myCollection\u2019] #or db.myCollection"
    },
    "870": {
        "file": "08 - PyMongo.pdf",
        "page": 3,
        "chunk": "Inserting a Single Document 4 db = client[\u2018ds4300\u2019] collection = db[\u2018myCollection\u2019] post = { \u201cauthor\u201d: \u201cMark\u201d, \u201ctext\u201d: \u201cMongoDB is Cool!\u201d, \u201ctags\u201d: [\u201cmongodb\u201d, \u201cpython\u201d] } post_id = collection.insert_one(post).inserted_id print(post_id)"
    },
    "871": {
        "file": "08 - PyMongo.pdf",
        "page": 4,
        "chunk": "Find all Movies from 2000 5 from bson.json_util import dumps # Find all movies released in 2000 movies_2000 = db.movies.find({\"year\": 2000}) # Print results print(dumps(movies_2000, indent = 2))"
    },
    "872": {
        "file": "08 - PyMongo.pdf",
        "page": 5,
        "chunk": "Jupyter Time - Activate your DS4300 conda or venv python environment - Install pymongo with pip install pymongo - Install Jupyter Lab in you python environment - pip install jupyterlab - Download and unzip > this < zip \ufb01le - contains 2 Jupyter Notebooks - In terminal, navigate to the folder where you unzipped the \ufb01les, and run jupyter lab 6"
    },
    "873": {
        "file": "08 - PyMongo.pdf",
        "page": 6,
        "chunk": "?? 7"
    },
    "874": {
        "file": "Class Notes.pdf",
        "page": 0,
        "chunk": "1/8/2025 - Foundations \u25cf\u200b Searching \u25cb\u200b Most common operation performed by a database system \u25cb\u200b Searching is why we have databases \u25cb\u200b SQL Select is most versatile/complex \u25cb\u200b Baseline for efficiency is Linear Search \u25a0\u200b Start at beginning of a list and proceed element by element until: \u25cf\u200b You find what you are looking for or you get the last element in the list \u25cb\u200b Record - a collection of values for attributes of a single entity instance; row in a table \u25cb\u200b Collection - a set of records of the same entity type; table \u25cb\u200b Search key - a value for an attribute from the entity type \u25a0\u200b Could be >= 1 attribute \u25cf\u200b List of records \u25cb\u200b If each record takes up x bytes of memory, then for n records, we need n*x bytes of memory \u25cb\u200b Contiguous allocated list \u25a0\u200b All n*x bytes are allocated as a single \u201cchunk\u201d of memory \u25cb\u200b Linked list \u25a0\u200b Each record needs x bytes + additional space for 1 or 2 memory addresses \u25cf\u200b That quantity times n is how much space it takes uo \u25a0\u200b Individual records are linked together in a type of chain using memory addresses \u25cb\u200b \u25cf\u200b Pros and Cons \u25cb\u200b Arrays are faster for random access, but slow for inserting anywhere but the end \u25a0\u200b"
    },
    "875": {
        "file": "Class Notes.pdf",
        "page": 1,
        "chunk": "\u25cb\u200b Linked lists are faster for interesting anywhere in the list, but slower for random access \u25a0\u200b Insert \u25cb\u200b Numpy is arrays that are contiguously allocated \u25cf\u200b Observations \u25cb\u200b Arrays \u25a0\u200b Fast for random access \u25a0\u200b Slow for random insertions \u25cb\u200b Linked lists \u25a0\u200b Slow for random access \u25a0\u200b Fast for random insertions \u25cf\u200b Binary Search \u25cb\u200b Input: array of values in sorted order, target value \u25cb\u200b Output: the location (index) of where target is located or some value indicating target was not found \u25cb\u200b Go in the middle, can eliminate half if the item is smaller or larger \u25cb\u200b Recursive in nature but can be dangerous in large data \u25a0\u200b Probably over 30k recursive calls \u25cb\u200b \u25cb\u200b Worst case of most searches needing to be done to find the value or determine it isn\u2019t in the set is log base 2 (n) - log2(n) \u25cb\u200b Example in slide is iterative version \u25cf\u200b Time complexity \u25cb\u200b Linear search \u25a0\u200b Best case: target is found at the first element, only 1 comparison \u25a0\u200b Worst case: target is not in the array; n comparisons \u25a0\u200b Therefore, in the worst case, linear search is O(n) time complexity \u25cb\u200b Binary search \u25a0\u200b Best case: target is found at mid; 1 comparison (inside the loop) \u25a0\u200b Worst case: target is not in the array; log2(n) comparisons \u25a0\u200b Therefore, in the worst case, binary search is O(log2(n)) time complexity \u25a0\u200b Can\u2019t perform binary search on an unsorted array \u25cf\u200b Back to database searching"
    },
    "876": {
        "file": "Class Notes.pdf",
        "page": 2,
        "chunk": "\u25cb\u200b Assume data is stored on disk by column id\u2019s value \u25cb\u200b Searching for a specific id = fast \u25cb\u200b But what if we want to search for a specific specialVal? \u25a0\u200b Only option is linear scan of that column \u25a0\u200b Cannot store data on disk sorted by both id and specialVal (at the same time) \u25cf\u200b Data would have to be duplicated \u2013 inefficient \u25a0\u200b We need an external data structure to support faster searching by specialVal than a linear search \u25cf\u200b What do we have in our arsenal? \u25cb\u200b An array of tuples (specialVal, rowNumber) sorted by specialVal \u25a0\u200b We could use Binary Search to quickly locate a particular specialVal and find its corresponding row in the table \u25a0\u200b But, every insert into the table would be like inserting into a sorted array - slow \u25cb\u200b A linked list of tuples (specialVal, rowNumber) sorted by specialVal \u25a0\u200b Searching for [cont] \u25a0\u200b \u25cf\u200b Something with fast insert and fast search? \u25cb\u200b Binary search tree - a binary tree where every node in the left subtree is less than its parent and every node in the right subtree is greater than its parent \u25a0\u200b Recursive data structure which lends itself to recursive formulas"
    },
    "877": {
        "file": "Class Notes.pdf",
        "page": 3,
        "chunk": "1/16/2025 - Practical outline \u25cf\u200b A.json \u25cb\u200b \u2018Preprocessed_text\u2019: [\u2018bank, \u2018finance\u2019, etc.] \u25cf\u200b Finance articles \u25cb\u200b Jan 2018 etc. up to May \u25cb\u200b Each folder has 50k+ json files \u25cf\u200b Give program path to root folder and then have to recursively descend through the folder \u25cf\u200b BST_NODE \u25cb\u200b Key is going to be a word \u25a0\u200b Values will have the files \u25cf\u200b If bank is in a.json and c.json, key would be bank, value would be a.json, c.json \u25cb\u200b Add-values: goes to values list and appends the new value \u25a0\u200b Self.values \u25a0\u200b Consider if bank appears twice in the file \u25cf\u200b Add it twice, add it only once \u25cf\u200b Prob not valuable information to have it twice \u25cb\u200b Put preprocessed texts into a set to to remove duplicates \u25cb\u200b Use string comparison to determine greater or less than \u25cf\u200b BST_index \u25cb\u200b Abstract index is a type of inheritance defined in abstrac_ index.py \u25a0\u200b Abstract is its own class \u25a0\u200b Can change abstract \u25cb\u200b Abstract method (class that implements) have to provide implementation for abstract method \u25cb\u200b Line 145 insert function \u25a0\u200b Takes a key and a single value \u25cf\u200b Value is name of file currently being parsed \u25cb\u200b Line 28 insert recursive \u25a0\u200b Look up recursive \u25cf\u200b AVL_TREE_NODE \u25cb\u200b Has everything BST has \u25cb\u200b Insert recursive \u25a0\u200b Height is starting at 1 \u25cb\u200b If not root: \u25cb\u200b Node = AVLNode(key) \u25cb\u200b node.add_value(value) \u25a0\u200b Return node \u25cb\u200b Elif key < root.key: \u25a0\u200b Root.left = self.insert_recursive(current.left, key value) \u25cb\u200b Elif key > root.key \u25a0\u200b Root.right = self_insert_recurisve(root.left, key, value) \u25cb\u200b Elif key == root,key"
    },
    "878": {
        "file": "Class Notes.pdf",
        "page": 3,
        "chunk": "\u25a0\u200b Root.right = self_insert_recurisve(root.left, key, value) \u25cb\u200b Elif key == root,key"
    },
    "879": {
        "file": "Class Notes.pdf",
        "page": 4,
        "chunk": "\u25cb\u200b Update height \u25a0\u200b Height_left = (0 if not root.left else root.left.height) \u25a0\u200b Height_right = (0 if not root.right else root.right.height) \u25a0\u200b Root.heigh = 1 +(height_left if (height_left > ehgith_right) else height_right) \u25cb\u200b Equivalent of max function \u25a0\u200b Balance = hieght_left - height_right \u25a0\u200b If balance > 1: \u25cf\u200b If key< root.left.key: \u25cb\u200b Return self._rotate_right(root) \u25cf\u200b Else: \u25cb\u200b Root.left = self._rotate_left(root.left) \u25cb\u200b Return self.rotateright(root) \u25a0\u200b If balance < -1: \u25cf\u200b If key > root.right/key: \u25cb\u200b Return self.rortateleft(root) \u25cf\u200b Else: \u25cb\u200b Root.right self.rotateright(root.right) \u25cb\u200b Return self.rotateleft(root) \u25a0\u200b Return root \u25cf\u200b Rotate left \u25cb\u200b Y = x.right \u25cb\u200b Tw = y.left \u25cb\u200b Y.left = x \u25cb\u200b X.right = tw \u25cb\u200b Ht_x_left = (0 if not x.left else x.left_h \u25cb\u200b Should be using functions and not all if checkers \u25a0\u200b If statement is faster than a function call \u25a0\u200b Make sure its not taking too long \u25cf\u200b Use pickle \u25cb\u200b Library in python that serializing objects to disk so that it can read it in much faster 1/27/25 \u25cf\u200b Benefits of the relational model \u25cb\u200b (mostly) standard data model and query language \u25cb\u200b ACID compliance \u25a0\u200b Atomicity, consistency, isolation, durability \u25cb\u200b Works well will highly structured data \u25cb\u200b Can handle large amounts of data \u25cb\u200b Well understood, lots of tooling, lots of experience \u25cb\u200b A transaction is a unit of work for a database \u25cf\u200b Relational database performance \u25cb\u200b Many ways that a RDBMS increases efficiency:"
    },
    "880": {
        "file": "Class Notes.pdf",
        "page": 5,
        "chunk": "\u25a0\u200b Indexing (focus of class) \u25a0\u200b Directly controlling storage \u25a0\u200b Column oriented storage vs row oriented storage \u25a0\u200b Query optimization \u25a0\u200b caching/prefetching \u25a0\u200b materialized views \u25a0\u200b Precompiled stored procedures \u25a0\u200b Data replication and partitioning \u25cf\u200b Transaction processing \u25cb\u200b Transaction - a sequence of one or more of the CRUD operations performed as a single, logical unit of work \u25a0\u200b Either the entire sequence succeeds (COMMIT) \u25a0\u200b OR the entire sequence fails (ROLLBACK or ABORT) \u25cb\u200b Help ensure \u25a0\u200b Data integrity \u25a0\u200b Error recovery \u25a0\u200b Concurrency control \u25a0\u200b Reliable data storage \u25a0\u200b Simplified error handling \u25cf\u200b Acid properties \u25cb\u200b Characteristics or properties of transactions that ensure the safety of database and the integrity of the database that would be detrimental if don't exist \u25cb\u200b Atomicity \u25a0\u200b Transaction is treated as an atomic unit - it is fully executed or no parts of it are executed \u25cb\u200b Consistency \u25a0\u200b A transaction takes a database from one consistent state to another consistent state \u25a0\u200b Consistent state - all data meets integrity constraints \u25cb\u200b Isolation \u25a0\u200b Two transactions T1 and T2 are being executed at the same time but cannot affect each other \u25a0\u200b If both T1 and T2 are reading the data - no problem \u25a0\u200b Ensured through \u201clocking\u201d \u25a0\u200b If T1 is reading the same data that T2 may be writing, can result in: \u25cf\u200b Dirty read \u25cb\u200b A transaction T1 is able to read a row that has been modified by another transaction T2 that hasn\u2019t yet executed a COMMIT"
    },
    "881": {
        "file": "Class Notes.pdf",
        "page": 6,
        "chunk": "\u25cf\u200b Non-repeatable read \u25cb\u200b Two queries in a single transaction T1 execute a SELECT but get different values because another transaction T2 has changed data and COMMITTED \u25cb\u200b \u25cf\u200b Phantom read \u25cb\u200b When a transaction T1 is running and another transaction T2 adds or deletes rows from the set T1 is using \u25cb\u200b \u25cb\u200b Durability \u25a0\u200b Once a transaction is completed and committed successfully, its changes are permanent \u25a0\u200b Even in the event of a system failure, committed transactions are preserved \u25a0\u200b If data disappears, want to be able to get it back \u25cf\u200b Relational Databases may not be the solution to all problems: \u25cb\u200b Sometimes, schema evolve over time \u25cb\u200b Not all apps may need the full strength of ACID compliance \u25cb\u200b Joins can be expensive \u25cb\u200b A lot of data is semi-structured or unstructured (JSON, XML, etc) \u25cb\u200b Horizontal scaling presents challenges \u25cb\u200b Some apps need something more performant (real time, low latency systems) \u25cf\u200b Scalability up or out \u25cb\u200b Conventional wisdom:"
    },
    "882": {
        "file": "Class Notes.pdf",
        "page": 7,
        "chunk": "\u25a0\u200b Scale vertically (up, with bigger, more powerful systems) until the demands of high-availability make it necessary to scale out with some type of distributed computing model \u25cb\u200b But why? \u25a0\u200b Scaling up is easier - no need to really modify your architecture. But there are practical and financial limits \u25cb\u200b However \u25a0\u200b There are modern systems that make horizontal scaling less problematic \u25cb\u200b \u25cb\u200b More power is not always the answer however it is the easiest thing to do \u25cf\u200b So what Distributed data when scaling out \u25cb\u200b A distributed system is \u201ca collection of independent computers that appear to its users as one computer\u201d \u25cb\u200b Characteristics of distributed system \u25a0\u200b Computers operate concurrently \u25a0\u200b Computers fail independently \u25a0\u200b No shared global clock \u25cf\u200b Distributed storage - 2 directions \u25cb\u200b \u25cf\u200b Distributed data stores \u25cb\u200b Data is stored on > 1 node, typically replicated \u25a0\u200b I.e. each block of data is available on N nodes"
    },
    "883": {
        "file": "Class Notes.pdf",
        "page": 8,
        "chunk": "\u25cb\u200b Distributed databases can be relational or non-relational \u25a0\u200b MySQL and PostgreSQL support replication and sharding \u25a0\u200b CockroachDB - new player on the scene \u25a0\u200b Many NoSQL systems support one or both models \u25cb\u200b But remember: Network partitioning is inevitable \u25a0\u200b Network failures, system failures \u25a0\u200b Overall system needs to be a partition tolerant \u25cf\u200b System can keep running even with network partition \u25cf\u200b The CAP Theorem \u25cb\u200b \u25cb\u200b Can always have two of these but can never have all three \u25cb\u200b Consistency here is not necessarily same consistency as ACID \u25cb\u200b Availability is can i get to it all the time \u25cb\u200b If something breaks up system, can i still read and write from the partitions \u25cb\u200b CAP Theorem states that it is impossible for a distributed data store to simultaneously provide more than two out of the following three guarantees: \u25a0\u200b Consistency: every read receives the most recent write or error thrown \u25cf\u200b Every user of the DB has an identical view of the data at any given instant \u25a0\u200b Availability: every request receives a (non-error) response - but no guarantee that the response contains the most recent write \u25cf\u200b In the event of a failure, the database remains operational \u25a0\u200b Partition tolerance - the system can continue to operate despite arbitrary network issues \u25cf\u200b The database can maintain operations in the event of the network\u2019s failing between two segments of the distributed system"
    },
    "884": {
        "file": "Class Notes.pdf",
        "page": 9,
        "chunk": "\u25cb\u200b \u25cb\u200b Consistency + Availability: System always responds with the latest data and every request gets a response, but may not be able to deal with network issues (network partitions) \u25a0\u200b If you make a request and network has been partitioned, don\u2019t know what you will get back or could get an error \u25cb\u200b Consistency + Partition tolerance: If system responds with data from a distributed store, it is always the latest, else data request is dropped \u25a0\u200b \u201cElse..dropped\u201d is lack of availability \u25a0\u200b If gets partitioned, data may not be consistent and can\u2019t always access it \u25cb\u200b Availability + Partition Tolerance: system always sends responses based on distributed data store, but may not be the absolute latest data \u25a0\u200b \u201cStale data\u201d \u25cf\u200b Cap in reality \u25cb\u200b What it is really saying: \u25a0\u200b If you cannot limit the number of faults, requests can be directed to any server, and you insist on serving every request, then you cannot possibly be consistent \u25cb\u200b But it is interpreted as: \u25a0\u200b You must always give up something: availability, consistency, tolerance to failure 2/3/25 \u25cf\u200b Distributed DBs and ACID - Pessimistic Concurrency \u25cb\u200b ACID transaction"
    },
    "885": {
        "file": "Class Notes.pdf",
        "page": 10,
        "chunk": "\u25cb\u200b If something bad could happen, it will, and attempts to prevent a conflict from happening \u25cb\u200b Focuses on \u201cdata safety\u201d \u25cb\u200b Considered a pessimistic concurrency model because it assumes one transaction has to protect itself from other transactions \u25a0\u200b IOW, it assumes if something can go wrong it will \u25cb\u200b Conflicts are prevented by locking resources until a transaction is complete (there are both read and write locks) \u25cb\u200b Write Lock Analogy \u2192 borrowing a book from a library\u2026If you have it, no one else can \u25cb\u200b Typically when you start a transaction, transaction processing unit can look and see what needs to be locked if anything \u25cf\u200b Optimistic concurrency \u25cb\u200b Transactions don\u2019t obtain locks on data when they read or write \u25cb\u200b Optimistic because it assumes conflicts are unlikely to occur \u25a0\u200b Even if there is a conflict, everything will be ok \u25cb\u200b But how? \u25a0\u200b Add last update timestamp and version number to columns to every table\u2026 read them when changing. THEN, check at the end of transaction to see if any other transaction has caused them to be modified \u25cb\u200b Low conflict systems (backups, analytical dbs, etc) \u25a0\u200b Read heavy systems \u25a0\u200b The conflicts that arise can be handled by rolling back and re-running a transaction that notices a conflict \u25a0\u200b So optimistic concurrency works well - allows for higher concurrency \u25cb\u200b High conflict Systems \u25a0\u200b Rolling back and rerunning transactions that encounter a conflict \u2192 less efficient \u25a0\u200b So, a locking scheme (pessimistic model) might be preferable \u25cb\u200b When lots of people involved, may not want to use this model \u25cf\u200b NOSQL \u25cb\u200b First used to describe a relational database system that did not use SQL \u25a0\u200b 1998 - Carlo Strozzi \u25cb\u200b More common, modern meaning is \u201cNot Only SQL\u201d \u25cb\u200b But, sometimes thought of"
    },
    "886": {
        "file": "Class Notes.pdf",
        "page": 10,
        "chunk": "be preferable \u25cb\u200b When lots of people involved, may not want to use this model \u25cf\u200b NOSQL \u25cb\u200b First used to describe a relational database system that did not use SQL \u25a0\u200b 1998 - Carlo Strozzi \u25cb\u200b More common, modern meaning is \u201cNot Only SQL\u201d \u25cb\u200b But, sometimes thought of as \u201cnon-relational databases\u201d \u25cb\u200b Idea originally developed, in part, as a response to processing unstructured web-based data \u25cf\u200b ACID Alternative for Distribution Systems - BASE \u25cb\u200b Basically Available \u25a0\u200b Guarantees the availability of the data (per CAP), but response can be \u201cfailure\u201d/\u201dunreliable\u201d because the data is an inconsistent or changing state \u25a0\u200b System appears to work most of the time \u25cb\u200b Soft State"
    },
    "887": {
        "file": "Class Notes.pdf",
        "page": 11,
        "chunk": "\u25a0\u200b The state of the system could change over time, even without input. Changes could be the result of eventual consistency \u25cf\u200b Data stores don\u2019t have to be write-consistent \u25cf\u200b Replicas don\u2019t have to be mutually consistent \u25cb\u200b Eventual Consistency \u25a0\u200b The system will eventually become consistent \u25cf\u200b All writes will eventually stop so all nodes/replicas can be updated \u25cb\u200b Many DB systems that support replication of nodes will support the three things of BASE to some degree \u25cf\u200b Categories of NoSQL DBs \u25cb\u200b Document databases \u25a0\u200b Like json \u25cb\u200b Graph databases \u25a0\u200b Things with lots of relationships between the different points?? \u25cb\u200b Key-value databases \u25cb\u200b Columnar databases \u25cb\u200b Vector databases \u25cf\u200b Key Value Stores \u25cb\u200b Or key value databases \u25cb\u200b Feel similar to operating a hash table in python \u25cb\u200b key = value \u25cb\u200b Designed around 3 things \u25a0\u200b Simplicity \u25cf\u200b The data model is extremely simple \u25cf\u200b Comparatively, tables in a RDBMS are very complex \u25cf\u200b Lends itself to simple CRUD ops and API creation \u25a0\u200b Speed \u25cf\u200b Usually developed as in-memory DB \u25cf\u200b Retrieving a value given its key is typically a O(1) op b/c hash tables or similar data structures used under the hood \u25cf\u200b No concept of complex queries or joins\u2026they slow things down \u25a0\u200b Scalability \u25cf\u200b Horizontal scaling is simple - add more nodes \u25cf\u200b Typically concerned with eventual consistency, meaning in a distributed environment, the only guarantee is that all nodes will eventually converge on the same value \u25cf\u200b KV DS Use Cases \u25cb\u200b EDA/Experimentation Results Store \u25a0\u200b Store intermediate results from data preprocessing and EDA \u25a0\u200b Store experiment or testing (A/B) results w/o prod db \u25cb\u200b Feature Store \u25a0\u200b Store frequently accessed feature \u2192 low-latency retrieval for model training and prediction \u25cb\u200b Model Monitoring"
    },
    "888": {
        "file": "Class Notes.pdf",
        "page": 11,
        "chunk": "\u25cb\u200b EDA/Experimentation Results Store \u25a0\u200b Store intermediate results from data preprocessing and EDA \u25a0\u200b Store experiment or testing (A/B) results w/o prod db \u25cb\u200b Feature Store \u25a0\u200b Store frequently accessed feature \u2192 low-latency retrieval for model training and prediction \u25cb\u200b Model Monitoring"
    },
    "889": {
        "file": "Class Notes.pdf",
        "page": 12,
        "chunk": "\u25a0\u200b Store key metrics about performance of model, for example, in real-time inferencing \u25cf\u200b KV SWE Use Cases \u25cb\u200b Caching scenarios \u25cb\u200b Storing Session Information \u25a0\u200b Everything about the current session can be stored via a single PUT or POST and retrieved with a single GET \u2026 very fast \u25cb\u200b User Profiles & Preferences \u25a0\u200b User info could be obtained with a single GET operation \u2026 language, TZ, product or UI preferences \u25cb\u200b Shopping Cart Data \u25a0\u200b Cart data is tied to the user \u25a0\u200b Needs to be available across browser, machines, session \u25cb\u200b Caching Layer \u25a0\u200b In front of a disk-based database \u25cf\u200b Consider efficiency of KV because we can keep everything memory resident \u25cf\u200b Redis DB \u25cb\u200b Remote Directory Server \u25a0\u200b Open source, in-memory database \u25a0\u200b Sometimes called a data structure store \u25a0\u200b Primarily a KV store, but can be used with other models: Graph, Spatial, Full Text Search, Vector, Time Series \u25cb\u200b It is considered an in-memory database system but\u2026. \u25a0\u200b Supports durability of data by \u25cf\u200b Essentially saving snapshots to disk at specific intervals OR \u25cf\u200b Append-only file chick is a journal of changes that can be used for roll-forward if there is af failure \u25cf\u200b Can be very fast .. >100,000 SET ops/second \u25cf\u200b Rich collection of commands \u25cf\u200b Does NOT handle complex data. No secondary indexes. Only supports lookup by Key \u25cf\u200b Redis Data Types \u25cb\u200b Keys: \u25a0\u200b Usually strings but can be binary sequence \u25cb\u200b Values: \u25a0\u200b Strings \u25a0\u200b Lists (linked lists) \u25a0\u200b Sets (unique unsorted string elements) \u25a0\u200b Sorted sets \u25a0\u200b Hashes (string \u2192 string) \u25a0\u200b Geospatial data \u25cf\u200b Redis Databases and Interaction \u25cb\u200b Redis provides 16 databases by default \u25a0\u200b They are numbered 0 to 15"
    },
    "890": {
        "file": "Class Notes.pdf",
        "page": 12,
        "chunk": "Sets (unique unsorted string elements) \u25a0\u200b Sorted sets \u25a0\u200b Hashes (string \u2192 string) \u25a0\u200b Geospatial data \u25cf\u200b Redis Databases and Interaction \u25cb\u200b Redis provides 16 databases by default \u25a0\u200b They are numbered 0 to 15"
    },
    "891": {
        "file": "Class Notes.pdf",
        "page": 13,
        "chunk": "\u25a0\u200b There is no other name associated \u25cb\u200b DIrect interaction with Redis is through a set of commands related to setting and getting k/v pairs (and variations) \u25cb\u200b Many language libraries available as well \u25cf\u200b Foundation Data Type - String \u25cb\u200b Sequence of bytes - text, serialized objects, bin arrays \u25cb\u200b Simplest data type \u25cb\u200b Maps a string to another string \u25cb\u200b Use cases: \u25a0\u200b Caching frequently accessed HTML/CSS/JS fragments \u25a0\u200b Config. Settings, user settings info, token management \u25a0\u200b Counting web page/app screen views or rate limiting \u25cf\u200b Basic command \u25cb\u200b SETNX will only set a key if it doesn\u2019t already exist in the table \u25cb\u200b SET someValue 0 \u25cb\u200b INRC someValue \u25a0\u200b Considered atomic operations, won\u2019t end up have two transactions trying to increment same number at same time \u25cb\u200b INCRBY someValue 10 \u25cb\u200b DECR someValue \u25cb\u200b DECRBY someValue 5 \u25a0\u200b INCR parses the value as an int and increments (or adds to value) \u25cb\u200b \u25cf\u200b Hash Type\u200b \u25cb\u200b Value of KV entry is a collection of field-value pairs \u25cb\u200b Use cases: \u25a0\u200b Can be used to represent basic objects/structures \u25cf\u200b Number of field/value pairs per has is 2^31-1 \u25cf\u200b Practical limit: available system resource (e.g. memory) \u25a0\u200b Session information management \u25a0\u200b User/Event tracking (could include TTL) \u25a0\u200b Active Session Tracking (all sessions under one hash key) \u25cf\u200b Hash Commands \u25cb\u200b"
    },
    "892": {
        "file": "Class Notes.pdf",
        "page": 14,
        "chunk": "\u25cf\u200b As long as for data in table, only searching for primary key, this can become close to what a relational database is like \u25cb\u200b Search only by username \u25cb\u200b But if ever want to search by email, can\u2019t do it in redis \u25cf\u200b List Type\u200b \u25cb\u200b Value of KV Pair is linked lists of string values \u25cb\u200b Use cases: \u25a0\u200b Implementation of stacks and queues \u25a0\u200b Queue management and message passing queues (producer/consumer model) \u25a0\u200b Logging systems (easy to keep in chronological order) \u25a0\u200b Build social media streams/feeds \u25a0\u200b Message history in a chat application \u25a0\u200b Batch processing by queueing up a set of tasks to be executed sequentially at a later time \u25cf\u200b Linked Lists crash course \u25cb\u200b Sequential data structure of linked nodes (instead of contiguously allocated memory) \u25cb\u200b Each node points to the next element of the list (except the last points to nil/null) \u25cb\u200b O(1) to insert new value at front or insert new value at the end \u25cb\u200b \u25cb\u200b push/pop from same side operates like a stack \u25cb\u200b push/pop from alternate sides operates like a queue \u25cf\u200b JSON Type \u25cb\u200b Full support of the JSON standard \u25cb\u200b Uses JSONPath syntax for parsing/navigating a JSON document \u25cb\u200b Internally, stored in binary in a tree-structure \u2192 fast access to sub elements \u25cf\u200b Set Type \u25cb\u200b Unordered collection of unique strings (members) \u25cb\u200b Use cases: \u25a0\u200b Track unique items \u25a0\u200b Primitive relation \u25a0\u200b Access control list for users and permission structures \u25a0\u200b Social network friends list \u25cb\u200b Supports set operations \u25cf\u200b Write a python script that will parallelize the parsing of json files to maximize the throughput that redis can handle MongoDB Lecture \u25cf\u200b Document Database"
    },
    "893": {
        "file": "Class Notes.pdf",
        "page": 14,
        "chunk": "Supports set operations \u25cf\u200b Write a python script that will parallelize the parsing of json files to maximize the throughput that redis can handle MongoDB Lecture \u25cf\u200b Document Database"
    },
    "894": {
        "file": "Class Notes.pdf",
        "page": 15,
        "chunk": "\u25cb\u200b A non-relatinal database that stores data as structured documents usually in json \u25cb\u200b JSON = JavaScript Object Notation \u25a0\u200b A lightweight data-interchange format \u25a0\u200b It is easy for humans to read and write \u25a0\u200b Its easy for machines to parse and generate \u25cb\u200b JSON is built on two structure \u25a0\u200b A collection of name/value pairs. In various languages, this is operationalized as an object, record, struct, dictionary, hash table, keyed list, or associative array \u25a0\u200b An ordered list of values. In most languages, this is operationalized as an array, vector, list, or sequence \u25cb\u200b These are two universal data structures supported by virtually all modern programming languages \u25a0\u200b Thus, json makes a great data interchange \u25cf\u200b Binary Json, BSON \u25cb\u200b BSON \u2192 Binary Json \u25a0\u200b Binary encoded serialization of a JSON-like document structure \u25a0\u200b Supports extended types not part of basic JSON (e.g. data, BinaryDAte, etc.) \u25a0\u200b Lose the human readability aspect \u25a0\u200b Lightweight - keep space overhead to a minimum \u25a0\u200b Traversable - designd to be easily traversed, which is vitially important tot a document DB \u25cf\u200b XML (extensible markup language) \u25cb\u200b Precursor to JSON as data exchange format \u25cb\u200b XML + CSS \u2192 web pages that separated content and formatting \u25cb\u200b Structurally similar to HTML, but tag set is extensible \u25cf\u200b Why Document Databases \u25cb\u200b Document databases address the impedance mismatch problem between object persistence in OO systems and how relational databases structure data \u25a0\u200b OO programming \u2192 inheritance and composition of types \u25a0\u200b How do we save a complex object to a relational database? \u25cf\u200b We basically have to deconstruct it \u25cb\u200b The structure of a document is self-describing \u25cb\u200b They are well-aligned with apps that use JSON/XML as a transport layer Introduction to the Graph Data Model \u25cf\u200b What is a graph database \u25cb\u200b Data"
    },
    "895": {
        "file": "Class Notes.pdf",
        "page": 15,
        "chunk": "save a complex object to a relational database? \u25cf\u200b We basically have to deconstruct it \u25cb\u200b The structure of a document is self-describing \u25cb\u200b They are well-aligned with apps that use JSON/XML as a transport layer Introduction to the Graph Data Model \u25cf\u200b What is a graph database \u25cb\u200b Data model based on the graph data structure \u25cb\u200b Composed of nodes and edges \u25a0\u200b Edges connect nodes \u25a0\u200b Each is uniquely identified \u25a0\u200b Each can contain properties (e.g. name, occupation, etc)"
    },
    "896": {
        "file": "Class Notes.pdf",
        "page": 16,
        "chunk": "\u25a0\u200b Supports queries based on graph-oriented operations \u25cf\u200b Traversals \u25cf\u200b Shortest path \u25cf\u200b Lots of others \u25cf\u200b Where do graphs show up? \u25cb\u200b Social networks \u25a0\u200b Modeling social interactions in fields like psychology and sociology \u25a0\u200b Social media as well \u25cb\u200b The web \u25a0\u200b It is just a big graph of \u201cpages\u201d nodes connected by hyperlinks (edges) \u25cb\u200b Chemical and biological data \u25a0\u200b Systems biology, genetics, etc. \u25a0\u200b Interaction relationships in chemistry \u25cf\u200b What is a graph? \u25cb\u200b Labeled property graph \u25a0\u200b Composed of a set of node (vertex) objects and relationship (edge) objects \u25a0\u200b Labels are used to mark a node as part of a group \u25a0\u200b Properties are attributes (think KV pairs) and can exist on nodes and relationships \u25a0\u200b Nodes with no associated relationships are OK. \u25a0\u200b Edges not connected to nodes are not permitted \u25cf\u200b Example \u25cb\u200b \u25cf\u200b Paths \u25cb\u200b A path is an ordered sequence of nodes connected by edges in which no nodes or edges are repeated"
    },
    "897": {
        "file": "Class Notes.pdf",
        "page": 17,
        "chunk": "\u25a0\u200b \u25cf\u200b Flavors of Graphs \u25cb\u200b Connected (vs Disconnected): there is a path between any two nodes in the graph \u25a0\u200b N choose 2 \u25a0\u200b No representation of how long or direct but can follow lines from one node to any other nodes \u25a0\u200b \u25cb\u200b Weighted (vs unweighted) - edge has a weight property (important for some algorithms) \u25a0\u200b If an unweighted graph, can consider it a weighted graph where they all have even weights \u25a0\u200b \u25cb\u200b Directed ( vs Undirected) - relationships (edges) define a start and end node"
    },
    "898": {
        "file": "Class Notes.pdf",
        "page": 18,
        "chunk": "\u25a0\u200b \u25cb\u200b Acyclic (vs Cyclic) - graph contains no cycles \u25a0\u200b \u25cb\u200b Sparse vs Dense \u25a0\u200b \u25cb\u200b Trees \u25a0\u200b \u25cf\u200b Types of Graph Algorithms - Pathfinding \u25cb\u200b Pathfinding"
    },
    "899": {
        "file": "Class Notes.pdf",
        "page": 19,
        "chunk": "\u25a0\u200b Finding the shortest path between two nodes, if one exists, is probably the most common operation \u25a0\u200b \u201cShortest\u201d means fewest edges or lowest weight \u25a0\u200b Average shortest path can be used to monitor efficiency and resiliency of networks \u25a0\u200b Minimum spanning tree, cycle detection, max/min flow\u2026 are other types of pathfinding \u25cf\u200b BFS vs DFS \u25cb\u200b \u25a0\u200b Moves from light to dark \u25cf\u200b Shortest path \u25cb\u200b \u25cf\u200b Types of Graph Algorithms - Centrality & Community Detection \u25cb\u200b Centrality \u25a0\u200b Determining which nodes are \u201cmore important\u201d in a network compared to other nodes \u25a0\u200b EX: Social Network Influencers?"
    },
    "900": {
        "file": "Class Notes.pdf",
        "page": 20,
        "chunk": "\u25a0\u200b \u25cb\u200b Community Detection \u25a0\u200b Evaluate clustering or partitioning of nodes of a graph and tendency to 7strengthen or break apart \u25cf\u200b Some Famous Graph Algorithms \u25cb\u200b Dijkstra\u2019s Algorithm - single-source shortest path algo for positively weighted graphs \u25cb\u200b A* Algorithm - similar to Dijkstra\u2019s with added feature of using a heuristic to guide traversal \u25cb\u200b PageRank - measures the importance of each node within a graph based on the number of incoming relationships and the importance of the nodes from those incoming relationships \u25cf\u200b Neo4j \u25cb\u200b A graph database system that supports both transactional and analytical processing of graph-based data \u25cb\u200b Relatively new class of no-sql DBs \u25cb\u200b Considered schema optional (one can be imposed) \u25cb\u200b Supports various types of indexing \u25cb\u200b ACID compliant \u25cb\u200b Supports distributed computing \u25cb\u200b Similar: Microsoft CosmoDB, Amazon Neptune \u25cf\u200b Maximum port number is 65535 \u25cf\u200b 0-1023 are ports reserved for root access \u25cf\u200b Docker compose commands must be run in the same folder where the docker yml file is In Class 2/24 \u25cf\u200b Take class notes \u25cf\u200b Generate embeddings \u25cb\u200b Chunk: file name, pdf page, vector \u25cf\u200b Store in a redis-stack \u25cf\u200b On other side with person:"
    },
    "901": {
        "file": "Class Notes.pdf",
        "page": 21,
        "chunk": "\u25cb\u200b Questio embedding \u25a0\u200b Get 10 most similar embeddings \u25cf\u200b Retrieval Augmented Generation (RAG) AWS Introduction \u25cf\u200b Amazon Web Services \u25cb\u200b Leading Cloud Platform with over 200 different services available \u25cb\u200b Globally available via its massive networks of regions and availability zones with their massive data centers \u25a0\u200b Breaks everything into regions and then into availability zones \u25cf\u200b Availability zone think of like a giant data center \u25cb\u200b Based on a pay-as-you-use cost model \u25a0\u200b Theoretically cheaper than renting rackspace/servers in a data center\u2026Theoretically \u25cf\u200b History of AWS \u25cb\u200b Originally launched in 2006 with only 2 servies: S3 and EC2 \u25cb\u200b By 2010, services had expanded to include SimpleDB, Elastic Block Store, Relational Database Service, DynamoDB, CloudWatch, Simple Workflow, CloudFront, Availability Zones, and others \u25cb\u200b CVN = content delivery network \u25cb\u200b Amazon had competitions with big prizes to spur the adoption of AWS in its early days \u25cb\u200b They\u2019ve continuously innovated, always introducing new services for ops, dev, analytics, etc \u25cf\u200b Cloud Models \u25cb\u200b IaaS - Infrastructure as a Service \u25a0\u200b Contains the basic services that are needed to build an IT infrastructure \u25cb\u200b PaaS - Platform as a Service \u25a0\u200b Remove the need for having to manage infrastructure \u25a0\u200b You can get right to deploying your app \u25cb\u200b SaaS - Software as a System \u25a0\u200b Provide full software apps that are run and managed by another party/vendor"
    },
    "902": {
        "file": "Class Notes.pdf",
        "page": 22,
        "chunk": "\u25cb\u200b \u25cf\u200b The Shared Responsibility Model - AWS \u25cb\u200b AWS Responsibilities (Security OF the cloud) \u25a0\u200b Security of physical infrastructure (infra) and network \u25cf\u200b Keep the data centers secure, control access to them \u25cf\u200b Maintain power availability, HVAC, etc. \u25cf\u200b Monitor and maintain physical networking equipment and global infra/connectivity \u25a0\u200b Hypervisor & Host OSs \u25cf\u200b Manage the virtualization layer used in AWS compute services \u25cf\u200b Maintaining underlying host OSs for other services \u25a0\u200b Maintaining managed services \u25cf\u200b Keep infra up to date and functional \u25cf\u200b Maintain server software (patching etc.) \u25cb\u200b Client responsibilities (Security IN the cloud) \u25a0\u200b Control of Data/Content \u25cf\u200b Client controls how its data is classified, encrypted, and shared \u25cf\u200b Implement and enforce appropriate data-handling policies \u25a0\u200b Access management and IAM \u25cf\u200b Properly configure IAM users, roles, and policies \u25cf\u200b Enforce the Principle of Least Privilege \u25a0\u200b Manage self-hosted APps and associated oSs \u25a0\u200b Ensure networks security to its VPC \u25a0\u200b Handle compliance and governance policies and procedures \u25cf\u200b The AWS Global Infrastructure"
    },
    "903": {
        "file": "Class Notes.pdf",
        "page": 23,
        "chunk": "\u25cb\u200b Regions - distinct geographical areas \u25a0\u200b Us-east-1, us-west-1, etc \u25cb\u200b Availability Zones (AZs) \u25a0\u200b Each region has multiple AZs \u25a0\u200b Roughly equivalent to isolated data centers \u25cb\u200b Edge Locations \u25a0\u200b Locations for CDN and other types of caching services \u25a0\u200b Allows content to be closer to end user \u25cb\u200b Currently 36 regions totalling 114 availability zones with 700+ POPs (points of presence) with cloudFront \u25cf\u200b Compute services \u25cb\u200b Compute resources \u25a0\u200b Resources where you create them and have them available \u25a0\u200b All the way to on-demand serverless instances \u25cb\u200b VM-based \u25a0\u200b EC2 and EC2 Spot - Elastic Cloud Compute \u25cb\u200b Container Based \u25a0\u200b ECS - Elastic container service \u25a0\u200b ECR - Elastic container registry \u25a0\u200b EKS - Elastic Kubernetes Service \u25a0\u200b Fargate - Serverless container service \u25cb\u200b Serverless: AWLS Lambda \u25cf\u200b Storage Services \u25cb\u200b Each has its own unique icon \u25cb\u200b Amazon S3 - Simple Storage Service \u25a0\u200b Object storage in buckets; highly scalable; different storage classes \u25cb\u200b Amazon EFS - Elastic File System \u25a0\u200b Simple, serverless, elastic, \u201cset and forget\u201d file system \u25cb\u200b Amazon EBS - Elastic Block Storage \u25a0\u200b High-performance block storage service \u25cb\u200b Amazon File Cache \u25a0\u200b High-speed cache for datasets stored anywhere \u25cb\u200b AWS Backup \u25a0\u200b Fully managed, policy-based service to automate data protection and compliance of apps on AWS \u25cf\u200b Database Services \u25cb\u200b Relational - Amazon RDS, Amazon Aurora \u25cb\u200b Key-value - Amazon DynamoDB \u25cb\u200b In-memory - Amazon MemoryDK, Amazon ElastiCache \u25cb\u200b Document - Amazon DocumentDB (compatible with MongoDB) \u25cb\u200b Graph - Amazon Neptune \u25cf\u200b Analytics Services \u25cb\u200b Amazon Athena - Analyze petabyte scale data wher it lives (S3, for example) \u25cb\u200b Amazon EMR - Elastic MapReduce - Access APache Spark, Hive, Presto etc."
    },
    "904": {
        "file": "Class Notes.pdf",
        "page": 23,
        "chunk": "\u25cf\u200b Analytics Services \u25cb\u200b Amazon Athena - Analyze petabyte scale data wher it lives (S3, for example) \u25cb\u200b Amazon EMR - Elastic MapReduce - Access APache Spark, Hive, Presto etc."
    },
    "905": {
        "file": "Class Notes.pdf",
        "page": 24,
        "chunk": "\u25cb\u200b AWS Glue - Discover, prepare, and integrate all your data \u25cb\u200b Amazon Redshift - Data warehousing service \u25cb\u200b Amazon Kinesis - real-time data streaming \u25cb\u200b Amazon QuickSight - cloud-native BI/reporting tool \u25cf\u200b ML and AI Services \u25cb\u200b Amazon SageMaker \u25a0\u200b Fully-managed ML platform, including Jupyter NBs \u25a0\u200b Build, train, deploy ML models \u25cb\u200b AWS AI Services w/ Pre-trained Models \u25a0\u200b Amazon comprehend - NLP \u25a0\u200b Amazon Rekognition - Image/Video analysis \u25a0\u200b Amazon Textract - text extraction \u25a0\u200b Amazon translate - machine translation \u25cf\u200b Important Services for Data Analytics/Engineering \u25cb\u200b EC2 and Lambda \u25a0\u200b Elastic Cloud Compute \u25a0\u200b Scalable Virtual Computing in the cloud \u25a0\u200b Pay as you go \u25a0\u200b Many instance types available \u25a0\u200b Multiple different operating systems \u25cb\u200b Amazon S3 \u25cb\u200b Amazon RDS and DynamoDB \u25cb\u200b AWS Glue \u25cb\u200b Amazon Athena \u25cb\u200b Amazon EMR \u25cb\u200b Amazon Redshift \u25cf\u200b Features of EC2 \u25cb\u200b Elasticity - easily (and programmatically) scale instances up or down as needed \u25cb\u200b You can use one of the standard AMIs or provide your own AMI if pre-config is needed \u25cb\u200b Easily integrates with many other services such as S3, RDS, etc. \u25cf\u200b EC2 Lifestyle \u25cb\u200b Launch - when starting an instance for the first time with a chosen configuration \u25cb\u200b start/stop - temporarily suspend usage without deleting the instance \u25cb\u200b Terminate - permanently delete the instance \u25cb\u200b Reboot - restart an instance without sling the data on the root volume \u25cf\u200b Where can you store data in EC2 \u25cb\u200b Instance Store: Temporary high-speed storage tied to the instance lifecycle \u25cb\u200b EFS (Elastic File System) Support - shared file storage \u25a0\u200b Like a thumb drive \u25cb\u200b EBS (elastic block storage) - persistent block-level storage \u25cb\u200b S3 - large data set storage or EC2 backups even \u25cf\u200b Common EC2 Use Cases \u25cb\u200b Web Hosting - Run a"
    },
    "906": {
        "file": "Class Notes.pdf",
        "page": 24,
        "chunk": "to the instance lifecycle \u25cb\u200b EFS (Elastic File System) Support - shared file storage \u25a0\u200b Like a thumb drive \u25cb\u200b EBS (elastic block storage) - persistent block-level storage \u25cb\u200b S3 - large data set storage or EC2 backups even \u25cf\u200b Common EC2 Use Cases \u25cb\u200b Web Hosting - Run a website/web server and associated apps"
    },
    "907": {
        "file": "Class Notes.pdf",
        "page": 25,
        "chunk": "\u25cb\u200b Data processing - It\u2019s a VM\u2026 you can do anything to data possible with a programming language \u25cb\u200b Machine Learning - Train models using GPU instances \u25cb\u200b Disaster Recovery - Backup critical workloads or infrastructure in the cloud Lambdas 3/17/25 \u25cf\u200b Lambdas provide serverless computing \u25cf\u200b Automatically run code in response to events \u25cf\u200b Relieves you from having to manage servers - only worry about the code \u25cf\u200b You only pay for execution time, not for idle compute time (different from EC2) EXAM: \u25cf\u200b Mongo query \u25cf\u200b How to rotate/insert a tree \u25cf\u200b Support mapping"
    }
}